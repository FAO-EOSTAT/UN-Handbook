[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UN Handbook on Remote Sensing for Agricultural Statistics",
    "section": "",
    "text": "Intellectual property rights\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively.\nThis book is a practical guide on how to use remote sensing for agricultural statistics. It provides readers with the means of producing high-quality maps of agricutural areas and prediction of crop yields. Given the natural world’s complexity and huge variations in human-nature interactions, only local experts who know their countries and ecosystems can extract full information from big EO data.\nOne group of readers that we are keen to engage with is the national authorities on forest, agriculture, and statistics in developing countries. We aim to foster a collaborative environment where they can use EO data to enhance their national land use and cover estimates, supporting sustainable development policies.\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "UN Handbook on Remote Sensing for Agricultural Statistics",
    "section": "",
    "text": "Disclaimer\nYou are viewing a draft version of the UN Handbook. The final version is planned for November 2025.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "howto.html",
    "href": "howto.html",
    "title": "2  How to use this handbook",
    "section": "",
    "text": "2.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How to use this handbook</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 Background\nEarth Observation (EO) stands at a pivotal juncture in its capacity to transform agricultural statistics. For National Statistical Offices (NSOs) – our primary audience – this handbook arrives at a critical moment. The integration of EO into agricultural statistics is not just a technical opportunity; it is a policy imperative. As countries strive to meet Sustainable Development Goals (SDGs), ensure food security, and respond to climate volatility, the ability to generate timely, reliable, and spatially explicit agricultural data has become essential.\nWhile satellite data availability has expanded dramatically—most notably through the Copernicus Sentinel missions, which now provide near-daily global coverage at 10–20m resolution (ESA, 2023; Drusch et al., 2012)— the operational use of these resources by NSOs remains limited. This persistent gap is striking, given the extensive scientific literature showcasing advanced techniques for land cover and crop type mapping. Only 25–35% of national agricultural monitoring systems in Africa, South Asia, and Southeast Asia have operational EO-based capabilities for crop area and yield estimation—reflecting a notable gap between EO’s technical potential and its institutional adoption in developing economies (Whitcraft, A. K. et al., 2020).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#improving-the-use-of-eo-in-agricultural-statistics",
    "href": "introduction.html#improving-the-use-of-eo-in-agricultural-statistics",
    "title": "3  Introduction",
    "section": "3.2 Improving the use of EO in agricultural statistics",
    "text": "3.2 Improving the use of EO in agricultural statistics\nThe reasons for the disconnect between EO’s potential and its adoption by NSOs are multifaceted. First, data abundance itself introduces complexity. Platforms like the Copernicus Data Space Ecosystem and Digital Earth Africa offer petabyte-scale Analysis-Ready Data (ARD), but turning this into actionable agricultural statistics requires high-performance computing and structured processing chains. While countries such as Poland (ESA, 2023; CloudFerro, 2025) and Brazil (Gomes et al, 2021) have established national HPC ecosystems to process continental-scale EO data cubes, many NSOs lack comparable resources.\nSecond, research-grade EO solutions often falter in operational settings. Deep learning models may achieve over 90% accuracy under experimental conditions (Rubwurm et al, 2024)), but their deployment in national programs is challenged by inconsistent training data, cloud interference, and the need to reconcile statistical rigor with time-sensitive reporting cycles (De Simone et al., 2025). Operational maturity demands more than algorithmic innovation—it requires adaptation, institutional readiness, and governance.\nTo meet these challenges, the global EO community has coalesced around building robust toolchains. ESA’s Sen4Stat platform—built on GDAL and incorporating Orfeo Toolbox components—offers a reproducible pipeline for crop mapping and yield forecasting (Bontemps et al., 2023). Similarly, Brazil’s SITS R package enables scalable time-series classification and is now deployed in Chile’s national land accounts (De Simone et al., 2025). But beyond individual tools, a more integrated coordination framework is emerging.\nAt the center of this effort is GEOGLAM (Group on Earth Observations Global Agricultural Monitoring)—an open international community of practice uniting the world’s leading programs in crop monitoring. GEOGLAM orchestrates a diverse set of platforms and toolboxes for agricultural monitoring such as GEOGLAM Crop Monitor, ESA Sen4Stat, ESA WorldCereal, NASA Harvest GLAM, JRC ASAP, Aircas CropWatch, FAO EOSTAT, FAO GIEWS, FAO WaPOR. These platforms and toolboxes collectively advance the capacity of countries to monitor agricultural production through shared data, methodological standardization, and transparency. Importantly, GEOGLAM is increasingly focused on strengthening the statistical rigor of crop monitoring—aligning more closely with NSO requirements for official area and yield estimation. This handbook complements that ambition, offering tools, case studies, and software that help bridge EO-based crop monitoring with statistical reporting.\nIn parallel, the Food and Agriculture Organization (FAO) has played a critical role in enabling country-level capacity development. The UN Task Team on Earth Observations for Agricultural Statistics, established under the UN Committee of Experts on Big Data and Data Science and the UN Expert Group on Rural, Agricultural and Food Security Statistics, has driven collective progress in EO adoption by NSOs. Through documented contributions to the UN Statistical Commission, the Task Team has promoted South–South cooperation, capacity building, and joint projects across regions. Recently, it has deepened collaboration with the UN Global Platform, particularly through increased engagement with the UN Big Data Regional Hubs. In this context, the Global Hub for Big Data in China has taken a leading role in supporting this handbook initiative as a key deliverable for operationalizing EO within national statistical systems (UNCEBD, 2025).\nAccuracy, in this context, is non-negotiable. As shown by Olofsson et al. (2024), classification errors above 15% can lead to over 20% distortion in area statistics—errors that compromise national reporting and policy formulation. This handbook addresses these issues across the full statistical workflow: from building ARD data cubes to implementing bias-corrected estimators, quantifying uncertainty, and generating statistically valid crop area measures. Real-world deployments, such as Chile’s 30m-resolution crop maps and Indonesia’s use of active learning to cut labeling costs by 40% while maintaining 90% detection accuracy, demonstrate the feasibility of integrating EO into official systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#purpose-of-this-handbook",
    "href": "introduction.html#purpose-of-this-handbook",
    "title": "3  Introduction",
    "section": "3.3 Purpose of this Handbook",
    "text": "3.3 Purpose of this Handbook\nDrawing on lessons from earlier efforts—including the 2017 FAO Handbook on Remote Sensing for Agricultural Statistics co-authored with Jacques Delincé (FAO, 2017)—this volume presents a practical, use-case-driven guide for EO adoption in statistical production. It reflects a significant paradigm shift from theoretical principles to field-tested applications.\nWhat sets this handbook apart is its emphasis on reproducibility and reusability. Each chapter includes executable R and Python scripts that allow NSOs to directly implement the methods discussed. Whether using Sen4Stat’s GDAL workflows, SITS classifiers, NASA Harvest’s ARYA crop yield model (Becker-Reshef et al., 2023), or PRESTO, a transformer-based model for time-series feature extraction (Tseng et al., 2024), the focus is on transparency and adaptability. These are not theoretical tools—they are field-tested systems embedded in national workflows.\nThe Handbook showcases diverse cases contributed by a wide range of national statistical offices and research teams, including crop monitoring with SAR images in Poland (GUS), crop classification in Mexico (INEGI), Senegal (DAPSA and FAO EOSTAT), Zimbabwe (ZIMSTAT and FAO EOSTAT), Chile (INE-Chile), and digital workflows supported by Digital Earth Africa. Yield estimation use cases span countries such as Finland (LUKE), Indonesia (BPS), Poland (GUS), Colombia (DANE and FEDEARROZ), Ukraine (University of Strasburg and Ukraine Statistics) and China (Institute of Remote Sensing and Digital Earth, Zhejiang University, Chinese Academy of Science). These contributions represent collaborative efforts involving institutions such as Université Catholique de Louvain (UCLouvain), University of Strasbourg (UNISTRA), FAO EOSTAT, Statistics Indonesia, IBGE, INEGI, and VITO, among others, alongside international research bodies and space agencies.\nEach contribution has been shaped through close cooperation between NSOs and geospatial experts. Their expertise, spanning the academic, operational, and policy realms, ensures that the methodologies described are both advanced and implementable.\nBeyond showcasing results, the Handbook places special emphasis on methodological components critical for statistical integrity. These include quality assessment of in situ data, evaluation of survey design, validation of classification models through confusion matrices, and estimation of area statistics corrected for classification bias. For instance, recent advances described by De Simone et al. (2025) propose rigorous quality control protocols for training data using self-organizing maps (SOM) and SMOTE-based sample balancing, offering a path to significantly higher classification accuracy.\nInnovation is also explored in chapters addressing frontier topics such as crop yield estimation (UNISTRA), field boundary mapping (e.g., IBGE), and the role of drones and EO in disaster risk reduction. Of relevance is the contribution on the WorldCereal project (Van Tricht et al., 2023), coordinated by VITO, which highlights the potential of self-supervised learning and global pretraining strategies for democratizing access to high-performance EO models in data-scarce regions. These developments reflect continuing validation efforts in operational EO workflows now being adopted in countries like Senegal and Zimbabwe.\nThe Handbook also acknowledges the evolution of toolboxes supporting these efforts. The Sen4Stat system, developed by UCLouvain with ESA support, has become a key asset for NSOs aiming to operationalize EO workflows for agricultural monitoring (Bontemps et al., 2024). Its uptake in Senegal and Zimbabwe illustrates its growing relevance. In a sense, the Handbook captures the evolution from research frontier to operational solutions. It represents the joint aspirations of NSOs, the geospatial science community, and international organizations to reimagine agricultural statistics for the era of big data and planetary monitoring. Throughout, the Handbook emphasizes transparency, trust, and standardization. These are not merely technical ideals but are foundational to the credibility of agricultural statistics in national and global decision-making. By promoting harmonized protocols and open-source tools, this volume seeks to foster a common language between statisticians and EO specialists, thereby enabling more consistent and comparable agricultural data worldwide.\nThe journey from satellite pixels to policy-relevant statistics is complex—but no longer aspirational. With 15 real-world use cases, field-tested methodologies, and ready-to-use software, this handbook aims to help NSOs move from exploration to execution, and from data collection to decision-making.\nWe invite readers to see this Handbook not only as a reference but as a launchpad. The time has come to embed EO firmly into the statistical systems that underpin food security, rural development, and environmental sustainability. Let this volume guide that journey—methodologically rigorous, globally informed, and grounded in practice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#future-of-satellite-mission-and-its-relevance-to-nsos",
    "href": "introduction.html#future-of-satellite-mission-and-its-relevance-to-nsos",
    "title": "3  Introduction",
    "section": "3.4 Future of satellite mission and its relevance to NSOs",
    "text": "3.4 Future of satellite mission and its relevance to NSOs\nAs the EO landscape evolves, so too must national strategies. In 2025, NASA announced the discontinuation of Landsat Next—once intended to continue the legacy of the world’s longest-running EO mission. This unexpected restructuring highlights the fragility of EO continuity and the urgent need for risk mitigation. To this end, the handbook includes a dedicated chapter to help NSOs develop resilient, multi-source EO strategies that reduce dependency on any single mission or platform.\nFortunately, efforts are underway to secure long-term EO data availability. The European Space Agency, through its Sentinel Expansion and Copernicus Next Generation missions, is investing in hyperspectral, L-band radar, and thermal infrared sensors designed to ensure continuity and enhance capacity. These investments, aligned with GEOGLAM’s coordination, provide a sustainable pathway for countries to anchor EO in their statistical systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "3  Introduction",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nThe journey from satellite pixels to policy-relevant statistics is complex—but no longer aspirational. With 15 real-world use cases, field-tested methodologies, and ready-to-use software, this handbook aims to help NSOs move from exploration to execution, and from data collection to decision-making.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "3  Introduction",
    "section": "References",
    "text": "References\nBecker-Reshef, I., Barker, B., Humber, M., Hosseini, M., & Justice, C. (2023). NASA Harvest’s Open Crop Yield Models: ARYA Performance and Global Applications. Frontiers in Sustainable Food Systems. https://doi.org/10.3389/fsufs.2023.1194332\nDe Simone, L., Pizarro, E., Paredes, J., et al. (2025). Quality Control of Training Samples for Agricultural Statistics Using Earth Observation. Statistical Journal of the IAOS, 0(0). https://doi.org/10.1177/18747655251338033\nOlofsson, P., Foody, G. M., Herold, M., Stehman, S. V., Woodcock, C. E., & Wulder, M. A. (2014). Good Practices for Estimating Area and Assessing Accuracy of Land Change. Remote Sensing of Environment, 148, 42–57. https://doi.org/10.1016/j.rse.2014.02.015\nFAO. (2017). Handbook on Remote Sensing for Agricultural Statistics. Food and Agriculture Organization of the United Nations.\nUNCEBD. (2025). Report of the Task Team on Earth Observations for Agricultural Statistics to the UN Statistical Commission. United Nations Committee of Experts on Big Data.\nVan Tricht, K., Degerickx, J., Gilliams, S., et al. (2023). WorldCereal: a dynamic open-source system for global-scale, seasonal, and reproducible crop and irrigation mapping. Earth System Science Data, 15(12), 5491–5515. https://doi.org/10.5194/essd-15-5491-2023\nBontemps, S., Deffense, N., Nørgaard, B., & Defourny, P. (2024). Sen4Stat – Sentinels for Agricultural Statistics, D16.0 Concept Paper.\nESA, 2023. European Space Agency. Copernicus Sentinel Missions Overview. Available at: https://sentinels.copernicus.eu\nDrusch, M., et al., 2012. Sentinel-2: ESA’s Optical High-Resolution Mission for GMES Operational Services. Remote Sensing of Environment, 120, 25–36. https://doi.org/10.1016/j.rse.2011.11.026\nWhitcraft, A. K., Becker-Reshef, I., Killough, B., & Justice, C. O. (2020).Meeting Earth Observation Requirements for Global Agricultural Monitoring. Remote Sensing of Environment, 239, 111901. https://doi.org/10.1016/j.rse.2020.111901\nLesiv, M., et al., 2020. Estimating the Global Distribution of Field Size Using Crowdsourcing. Global Change Biology, 25(1), 174–186. https://doi.org/10.1111/gcb.14492\nCloudFerro. (2025, February 26). CloudFerro to provide NSIS-Cloud services for Poland [Press release]. CloudFerro. Retrieved from https://cloudferro.com/news/cloudferro-to-provide-nsis-cloud/:contentReferenceoaicite:0\nEuropean Space Agency. (2020, November 12). Sentinel data enables new system for agricultural monitoring in Poland. European Space Agency (ESA) – Observing the Earth: Copernicus News. Retrieved from https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel_data_enables_new_system_for_agricultural_monitoring_in_Poland\nGomes, V. C. F., Carlos, F. M., Queiroz, G. R., Ferreira, K. R., & Santos, R. (2021). Accessing and processing Brazilian earth observation data cubes with the Open Data Cube platform. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, V-4-2021, 153–159. https://doi.org/10.5194/isprs-annals-V-4-2021-153-2021:contentReferenceoaicite:2\nRußwurm, M., Wang, S., Kellenberger, B. et al. Meta-learning to address diverse Earth observation problems across resolutions. Commun Earth Environ 5, 37 (2024). https://doi.org/10.1038/s43247-023-01146-0\nTseng, G., Cartuyvels, L., Zvonkov, I., Purohit, S., Rolnick, D., & Kerner, H. (2024). Lightweight, pre-trained transformers for remote sensing timeseries (arXiv preprint arXiv:2304.14178). arXiv. https://arxiv.org/abs/2304.14178\nFoust, J. (2025, May 21). NASA’s budget crisis presents an opportunity for change. SpaceNews. https://spacenews.com/nasas-budget-crisis-presents-an-opportunity-for-change/?utm_source=chatgpt.com",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "Foundations",
    "section": "",
    "text": "Outline\nThis is the introduction to the foundations of the book.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "th_remote_sensing.html",
    "href": "th_remote_sensing.html",
    "title": "4  Remote Sensing images: optical, SAR",
    "section": "",
    "text": "4.1 Outline\nThis chapter provides a general introduction to remote sensing imagery. The authors discuss different types of remote sensing satellites (optical, SAR, hyperspectral) and include examples of satellite constellations (e.g, Landsat, Sentinel-1, CHEOS).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing images: optical, SAR</span>"
    ]
  },
  {
    "objectID": "th_lucc.html",
    "href": "th_lucc.html",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "",
    "text": "5.1 Introduction\nSatellite images are the most comprehensive source of data about our environment; they provide essential information on global challenges. Images provide information for measuring deforestation, crop production, food security, urban footprints, water scarcity, land degradation, among other uses. In recent years, space agencies have adopted open distribution policies. Petabytes of Earth observation data are now available. Experts now have access to repeated acquisitions over the same areas; the resulting time series improve our understanding of ecological patterns and processes[1]. Instead of selecting individual images from specific dates and comparing them, researchers can track changes continuously[2]. To handle big data, scientists are developing new algorithms for image time series (for recent surveys, see [5]). These methods are data-driven and theory-limited. However, numbers do not speak for themselves [6]. Data-driven approaches without solid theories can lead to results which will not increase our knowledge [7].\nConsider how experts use Earth observation data. Their input are images with resolution ranging from 5 to 500 meters, produced by satellites such as Landsat, Sentinels-1/2/3, and CBERS-4. To extract information, experts use methods that assign a label to each pixel (e.g., ‘grasslands’). Labels can represent either land cover or land use. Land cover is the observed biophysical cover of the Earth’s surface; land use concepts describe socio-economic activities\\cite[8]. Thus, forest' is a type of land cover, whilecorn plantation’ is a kind of land use. To support land classification, scientists have proposed ontologies and descriptive schemes [9]. We might thus ask: Are the current classification systems suitable to represent land change when working with big data? If not, which concepts are needed and how should they be applied?\nIn what follows, we present the prevailing consensus on classification systems: FAO’s Land Cover Classification System (LCCS)\\cite[10]. We argue that LCCS does not meet the challenges posed by big data. To support our views, we consider concepts used on image time series analysis; we show these concepys are related to event recognition and are not representable in LCCS. To improve the theory behind big data, we introduce elements of a phenology based approach for land classification.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Land cover and crop classification schemas</span>"
    ]
  },
  {
    "objectID": "th_lucc.html#classification-systems-for-earth-observation-data-current-status",
    "href": "th_lucc.html#classification-systems-for-earth-observation-data-current-status",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.2 Classification systems for Earth observation data: current status",
    "text": "5.2 Classification systems for Earth observation data: current status\nThe act of classification raises philosophical questions dating as far back as Aristotle. We use an a priori conception of reality to classify the world; what we observe has to fit our categories. Words in our language describe elements of the external reality. However, geographical terms such as mountain' andriver’ are imprecise and context-dependent[13]. These ambiguities have motivated research on geospatial semantics [16]. However, building such complete ontologies is hard. Janowicz et al. [17] recognize that “geographical concepts are situated and context-dependent, can be described from different, equally valid, points of view, and ontological commitments are arbitrary to a large extent”. Work on classification systems has shifted. Rather than using a single ontology, the current consensus argues for domain ontologies based on a common foundational ontology. These domain ontologies are means of making concepts of specific disciplines explicit and better communicating them [19].\nThe semantics of Earth observation data are constrained by classification systems. Experts agree on what are the possible descriptions of the objects in the image (e.g., “forest”, “river”, “pasture”). Each pixel of the image is then labeled using visual or automated interpretation. As an example, for countries reporting greenhouse gas inventories, the International Panel of Climate Change (IPCC) restricts the top-level land classes to ‘forest’, ‘cropland’, ‘grassland’, ‘wetlands’, ‘settlements’, and ‘others’. This approach is too simplistic. Sasaki and Putz [20] criticize the IPCC base classes for inducing wrong assessments for ecological and biodiversity conservation. The IPCC classes are an example where pre-conceived rules collide with the diversity of the world’s ecosystems.\nSince land classification provides essential information about our environment, many GIScience researchers have addressed the subject of land use and land cover semantics [23]. They investigated consistency of classification systems [24], semantic similarity between terms used by different systems [25], and disagreements between results [26]. The current consensus favors ontologies aiming at unambiguous definitions of land cover classes, such as the FAO Land Cover Classification System (LCCS) [10]. For this reason, it is important to discuss whether LCCS works well with big EO data.\nFAO has developed the Land Cover Classification System (LCCS) “to provide a consistent framework for the classification and mapping of land cover” [27]. LCCS is a hierarchical system. At its highest level, LCCS has eight major land cover types:\n\nCultivated and managed terrestrial areas.\nNatural and semi-natural terrestrial vegetation.\nCultivated aquatic or regularly flooded areas.\nNatural and semi-natural aquatic or regularly flooded vegetation.\nArtificial surfaces and associated areas.\nBare areas.\nArtificial water bodies, snow, and ice.\nNatural water bodies, snow, and ice.\n\nThe division on eight classes considers three criteria: presence of vegetation, edaphic conditions, and artificiality of cover [27]. Specialization of top-level LCCS classes uses properties such as life form, tree height, and vegetation density, setting pre-defined limits (e.g., “tree height &gt; 10 meters”). These subdivisions are ad hoc and application-dependent, leading to a combinational explosion with dozens or even hundreds of subclasses [10]. Such high expressive power can lead to incompatible LCCS-based class hierarchies [[24]}.\nLCCS is a landmark initiative; it provides a basis for a common understanding of land cover concepts. Many global and regional land mapping products use LCCS, including GLOBCOVER [28] and ESA CCI Land Cover [29]. However, LCCS makes assumptions which limit its use with big data:\n\nLCCS describes land properties based only on land cover types, disregarding land use. For example, LCCS does not distinguish pasture' fromnatural grasslands’; it labels both as herbaceous land cover types.\nThe LCCS hierarchy uses hard boundaries between its subclasses. At each level of the hierarchy, properties of subclasses use fixed values (e.g., “sparse forests have between 10% and 30% of trees”). Real-world class boundaries do not fit into such strict definitions. When doing data analysis with machine learning, boundaries between classes are data-dependent and cannot be set a priori [30].\nClassification in LCCS has no temporal reference. LCCS assumes that subtype properties (e.g., percent of tree cover) are detectable at the moment of classification. These properties do not refer to past or future values. Land use and land cover types whose values require time references (e.g., “forest land cleared in the last decade”) are not representable in LCCS.\n\nFor example, the UNFCCC Reduction of Emissions by Deforestation and Degradation initiative (REDD+) requires representing and measuring forest dynamics [31]. Static and rigid definitions of “forest” used by LCCS cannot represent concepts such as `forest degradation’ [32]. Forest degradation happens when a natural forest loses part of its biodiversity and its tree cover. It is not a stable state but an intermediary situation that can lead to different medium-term outcomes. One can restore a degraded forest; degradation may continue and lead to complete loss of forest cover. Whatever the case, LCCS lacks explicit temporal information to capture forest degradation and thus support initiatives such as REDD+. Therefore, LCCS is thus not fit for many critical applications of EO data.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Land cover and crop classification schemas</span>"
    ]
  },
  {
    "objectID": "th_lucc.html#elements-of-a-phenology-based-classification-schemas",
    "href": "th_lucc.html#elements-of-a-phenology-based-classification-schemas",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.3 Elements of a phenology-based classification schemas",
    "text": "5.3 Elements of a phenology-based classification schemas\nTo represent change in geographical space, GIScience authors distinguish between continuants and occurrents [36]. Continuants refer to entities that “endure through time even while undergoing different sorts of changes” [33]. The Amazon Forest and the city of Brasilia are continuants. Occurrents happen in a well-defined period and may have different stages during this time. Cutting down a forest area, cultivating a crop in a season, and building a road are occurrents. Objects are associated to continuants and events to occurrents.\nAtemporal classification systems such as LCCS refer only to properties of continuants. One can state facts such as “this area has 30% forest cover” using LCCS, but cannot assert that “this area lost 70% of its forest in the last two years”. To convey change, classification systems for big data need to include occurrents. In what follows, we discuss concepts used in the analysis of satellite image time series. These time series are extracted from organized collections of Earth observation data covering a geographical area in regular temporal intervals.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Land cover and crop classification schemas</span>"
    ]
  },
  {
    "objectID": "th_lucc.html#the-key-role-of-time-series",
    "href": "th_lucc.html#the-key-role-of-time-series",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.4 The key role of time series",
    "text": "5.4 The key role of time series\nSince remote sensing satellites revisit the same place, we can calibrate their images so that measures of the same place at different times are comparable (Figure \\(\\ref{fig:sits_a}\\)). These observations can be organized so that each measure from the sensor maps to a three-dimensional array in space-time. From a data analysis perspective, each pixel location \\((x, y)\\) at consecutive times, \\(t_1,...,t_m\\), makes up a satellite image time series (SITS), such as the one in Figure \\(\\ref{fig:sits_b}\\). From these time series, we can extract land-use and land-cover change information. In Figure \\(\\ref{fig:sits_b}\\), after the forest was cut in 2002, the area was used for cattle raising (pasture) for three years, during 2002 to 2008, then turned into cropland.\n\n\n\n\n\n\n\nFigure 5.1: Time series measures (EVI index) of a pixel location \\((x,y)\\) (source:[37]).\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Land change events associated to a pixel location \\((x,y)\\) (source:[37]).\n\n\n\n\nReferences{-}\n\n\n\n\n[1] \nV. J. Pasquarella, C. E. Holden, L. Kaufman, and C. E. Woodcock, “From imagery to ecology: Leveraging time series of all available LANDSAT observations to map and monitor ecosystem state and dynamics,” Remote Sensing in Ecology and Conservation, vol. 2, no. 3, pp. 152–170, 2016, doi: 10.1002/rse2.24.\n\n\n[2] \nC. E. Woodcock, T. R. Loveland, M. Herold, and M. E. Bauer, “Transitioning from change detection to monitoring with remote sensing: A paradigm shift,” Remote Sensing of Environment, vol. 238, p. 111558, 2020, doi: 10.1016/j.rse.2019.111558.\n\n\n[3] \nC. Gomez, J. C. White, and M. A. Wulder, “Optical remotely sensed time series data for land cover classification: A review,” {ISPRS} Journal of Photogrammetry and Remote Sensing, vol. 116, pp. 55–72, 2016, doi: 10.1016/j.isprsjprs.2016.03.008.\n\n\n[4] \nZ. Zhu, “Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 130, pp. 370–384, 2017, doi: 10.1016/j.isprsjprs.2017.06.013.\n\n\n[5] \nL. Zeng, B. D. Wardlow, D. Xiang, S. Hu, and D. Li, “A review of vegetation phenological metrics extraction using time-series, multispectral satellite data,” Remote Sensing of Environment, vol. 237, p. 111511, 2020, doi: 10.1016/j.rse.2019.111511.\n\n\n[6] \ndanah boyd and K. Crawford, “Critical Questions for Big Data,” Information, Communication & Society, vol. 15, no. 5, pp. 662–679, 2012, doi: 10.1080/1369118X.2012.678878.\n\n\n[7] \nR. Kitchin, “Big Data, new epistemologies and paradigm shifts,” Big Data & Society, vol. 1, no. 1, p. 2053951714528481, 2014, doi: 10.1177/2053951714528481.\n\n\n[8] \nA. Comber, “The separation of land cover from land use using data primitives,” Journal of Land Use Science, vol. 3, no. 4, pp. 215–229, 2008, doi: https:/doi.org/10.1080/17474230802465173.\n\n\n[9] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009.\n\n\n[10] \nM. Herold et al., “A joint initiative for harmonization and validation of land cover datasets,” IEEE Transactions on Geoscience and Remote Sensing, vol. 44, no. 7, pp. 1719–1727, 2006, doi: 10.1109/TGRS.2006.871219.\n\n\n[11] \nB. Smith and D. M. Mark, “Geographical categories: An ontological investigation,” International Journal of Geographical Information Science, vol. 15, no. 7, pp. 591–612, 2001, doi: 10.1080/13658810110061199.\n\n\n[12] \nB. Smith and D. M. Mark, “Do Mountains Exist? Towards an Ontology of Landforms,” Environment and Planning B: Planning and Design, vol. 30, no. 3, pp. 411–427, 2003, doi: 10.1068/b12821.\n\n\n[13] \nD. M. Mark and A. G. Turk, “Landscape Categories in Yindjibarndi: Ontology, Environment, and Language,” in Spatial Information Theory. Foundations of Geographic Information Science, 2003, pp. 28–45, doi: 10.1007/978-3-540-39923-0_3.\n\n\n[14] \nB. Smith and D. M. Mark, “Ontology and Geographic Kinds,” 1998, [Online]. Available: https://philarchive.org.\n\n\n[15] \nF. Fonseca, M. Egenhofer, C. Davis, and G. Câmara, “Semantic Granularity in Ontology-Driven Geographic Information Systems,” Annals of Mathematics and Artificial Intelligence, vol. 36, no. 1, pp. 121–151, 2002, doi: 10.1023/A:1015808104769.\n\n\n[16] \nW. Kuhn, “Geospatial Semantics: Why, of What, and How?” Journal on Data Semantics, vol. 3, pp. 1–24, 2005, doi: 10.1007/11496168_1.\n\n\n[17] \nK. Janowicz, S. Scheider, T. Pehle, and G. Hart, “Geospatial semantics and linked spatiotemporal data – Past, present, and future,” Semantic Web, vol. 3, no. 4, pp. 321–332, 2012, doi: 10.3233/SW-2012-0077.\n\n\n[18] \nB. Smith et al., “The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration,” Nature Biotechnology, vol. 25, no. 11, pp. 1251–1255, 2007, doi: 10.1038/nbt1346.\n\n\n[19] \nP. L. Buttigieg, N. Morrison, B. Smith, C. J. Mungall, S. E. Lewis, and the ENVO Consortium, “The environment ontology: Contextualising biological and biomedical entities,” Journal of Biomedical Semantics, vol. 4, no. 1, p. 43, 2013, doi: 10.1186/2041-1480-4-43.\n\n\n[20] \nN. Sasaki and F. E. Putz, “Critical need for new definitions of ‘forest’ and ‘forest degradation’ in global climate change agreements,” Conservation Letters, vol. 2, no. 5, pp. 226–232, 2009, doi: 10.1111/j.1755-263X.2009.00067.x.\n\n\n[21] \nA. Comber, P. Fisher, and R. Wadsworth, “What is Land Cover?” Environment and Planning B: Planning and Design, vol. 32, no. 2, pp. 199–209, 2005, doi: 10.1068/b31135.\n\n\n[22] \nO. Ahlqvist, “Using uncertain conceptual spaces to translate between land cover categories,” International Journal of Geographical Information Science, vol. 19, no. 7, pp. 831–857, 2005, doi: 10.1080/13658810500106729.\n\n\n[23] \nO. Ahlqvist, D. Varanka, S. Fritz, and K. Janowick, Eds., Land Use and Land Cover Semantics: Principles, Best Practices, and Prospects. CRC Press, 2017.\n\n\n[24] \nL. J. M. Jansen, G. Groom, and G. Carrai, “Land-cover harmonisation and semantic similarity: Some methodological issues,” Journal of Land Use Science, vol. 3, no. 2–3, pp. 131–160, 2008, doi: 10.1080/17474230802332076.\n\n\n[25] \nC.-C. Feng and D. M. Flewelling, “Assessment of semantic similarity between land use/land cover classification systems,” Computers, Environment and Urban Systems, vol. 28, no. 3, pp. 229–246, 2004, doi: 10.1016/S0198-9715(03)00020-6.\n\n\n[26] \nS. Fritz et al., “Highlighting continued uncertainty in global land cover maps for the user community,” Environmental Research Letters, vol. 6, no. 4, p. 044005, 2011, doi: 10.1088/1748-9326/6/4/044005.\n\n\n[27] \nA. Di Gregorio, “Land Cover Classification System - Classification concepts Software version 3,” FAO, 2016.\n\n\n[28] \nO. Arino et al., “GlobCover: ESA service for global land cover from MERIS,” in 2007 IEEE International Geoscience and Remote Sensing Symposium, 2007, pp. 2412–2415, doi: 10.1109/IGARSS.2007.4423328.\n\n\n[29] \nW. Li, P. Ciais, N. MacBean, S. Peng, P. Defourny, and S. Bontemps, “Major forest changes and land cover transitions based on plant functional types derived from the ESA CCI Land Cover product,” International Journal of Applied Earth Observation and Geoinformation, vol. 47, pp. 30–39, 2016, doi: 10.1016/j.jag.2015.12.006.\n\n\n[30] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[31] \nE. Corbera and H. Schroeder, “Governing and implementing REDD+,” Environmental Science & Policy, vol. 14, no. 2, pp. 89–99, 2011, doi: 10.1016/j.envsci.2010.11.002.\n\n\n[32] \nF. E. Putz and K. H. Redford, “The Importance of Defining ‘Forest’: Tropical Forest Degradation, Deforestation, Long-term Phase Shifts, and Further Transitions,” Biotropica, vol. 42, no. 1, pp. 10–20, 2010, doi: 10.1111/j.1744-7429.2009.00567.x.\n\n\n[33] \nP. Grenon and B. Smith, “SNAP and SPAN: Towards Dynamic Spatial Ontology,” Spatial Cognition & Computation, vol. 4, no. 1, pp. 69–104, 2004, doi: 10.1207/s15427633scc0401_5.\n\n\n[34] \nA. Galton, “Fields and Objects in Space, Time, and Space-time,” Spatial Cognition & Computation, vol. 4, no. 1, pp. 39–68, 2004, doi: 10.1088/1748-9326/6/4/04400510.1207/s15427633scc0401_4.\n\n\n[35] \nA. Galton, “Experience and History: Processes and their Relation to Events,” Journal of Logic and Computation, vol. 18, no. 3, pp. 323–340, 2008, doi: 10.1088/1748-9326/6/4/04400510.1093/logcom/exm079.\n\n\n[36] \nM. Worboys, “Event-oriented approaches to geographic phenomena,” International Journal of Geographical Information Science, vol. 19, no. 1, pp. 1–28, 2005, doi: 10.1080/13658810412331280167.\n\n\n[37] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Land cover and crop classification schemas</span>"
    ]
  },
  {
    "objectID": "th_quality_control.html",
    "href": "th_quality_control.html",
    "title": "6  Quality control of training sets for agricultural statistics",
    "section": "",
    "text": "6.1 Outline\nSelecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has shown that the number and quality of training samples are crucial factors in obtaining accurate results. This chapter presents pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labelled or possess low discriminatory power. Explains the basics of machine learning and provides examples of designing and using good training sets. Explains k-fold validation, SOM clustering and sample imbalance removal, with examples in R and Python.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quality control of training sets for agricultural statistics</span>"
    ]
  },
  {
    "objectID": "th_machine_learning.html",
    "href": "th_machine_learning.html",
    "title": "7  Machine learning classification of remote sensing images",
    "section": "",
    "text": "7.1 Outline\nThis chapter describes machine learning methods for classifying individual remote sensing images and image time series. The chapter considers three kinds of algorithms: • Machine learning algorithms that do not explicitly consider the spatial and temporal structure of the time series. These methods include random forests, support vector machine and extreme gradient boosting. • Deep learning methods which consider temporal relations between observed values in a time series. This class of models includes 1D convolutional neural networks and temporal attention-based encoders. • Semantic segmentation methods based on U-net paradigms and multidimensional 2D convolution.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Machine learning classification of remote sensing images</span>"
    ]
  },
  {
    "objectID": "th_uncertainty.html",
    "href": "th_uncertainty.html",
    "title": "8  Spatial map uncertainty estimation and active learning in crop classification",
    "section": "",
    "text": "8.1 Outline\nDescribes methods for estimating uncertainty of machine learning classification maps and how to use such estimates to improve classification accuracy. Map uncertainty refers to the degree of doubt or ambiguity in the accuracy of each pixel of the classification results. Several sources of uncertainty can arise during land classification using satellite data, including: a) classification errors; b) ambiguity in classification schema; c) variability in the landscape; and d) limitations of the data. The quality and quantity of input data can influence the accuracy of the classification results. Quantifying uncertainty in land classification is important for ensuring that the results are reliable and valid for decision-making.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Spatial map uncertainty estimation and active learning in crop classification</span>"
    ]
  },
  {
    "objectID": "th_validation.html",
    "href": "th_validation.html",
    "title": "9  Map validation and use of maps for area estimation",
    "section": "",
    "text": "9.1 Outline\nStatistically robust and transparent approaches for assessing accuracy are essential parts of the land classification process. The sits package supports the “good practice” recommendations for designing and implementing an accuracy assessment of a change map and estimating the area based on reference sample data. These recommendations address three components: sampling design, reference data collection, and accuracy estimates.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Map validation and use of maps for area estimation</span>"
    ]
  },
  {
    "objectID": "th_data_sources.html",
    "href": "th_data_sources.html",
    "title": "\n10  EO Big Data Sources\n",
    "section": "",
    "text": "Configurations to run the chapter\n# load package \"tibble\"\nlibrary(tibble)\n# load packages \"sits\" and \"sitsdata\"\nlibrary(sits)\nlibrary(sitsdata)\nlibrary(earthdatalogin)\n# set tempdir if it does not exist \ntempdir_r &lt;- \"~/UN-Handbook/tempdir/R/dc_ardcollections\"\ndir.create(tempdir_r, showWarnings = FALSE)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>EO Big Data Sources</span>"
    ]
  },
  {
    "objectID": "th_data_sources.html#ard-image-collections",
    "href": "th_data_sources.html#ard-image-collections",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.1 ARD Image Collections",
    "text": "10.1 ARD Image Collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD images processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and are provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organised as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images of an ARD collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure 10.1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 10.1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "href": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.2 Cloud platforms providing ARD data",
    "text": "10.2 Cloud platforms providing ARD data\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes.\nThere are a large number of cloud platforms providing open data organized as Analysis-Ready Data, including: Amazon Web Services (AWS), Brazil Data Cube (BDC), Copernicus Data Space Ecosystem (CDSE), Digital Earth Africa (DEAFRICA), Digital Earth Australia (DEAUSTRALIA), Microsoft Planetary Computer (MPC), and Nasa Harmonized Landsat/Sentinel (HLS),\nThis chapter describes how to access these collections and transform ARD images into regular data cubes. A data cube is a set of images organized in tiles of a grid system (e.g., MGRS). Each tile contains single-band images in a unique zone of the coordinate system (e.g, tile 20LMR in MGRS grid) covering the period between start_date and end_date. All tiles share the same set of regular temporal intervals and the same spectral bands and indices. All images have the same spatial resolution."
  },
  {
    "objectID": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "href": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.3 Tiling systems used by ARD collections",
    "text": "10.3 Tiling systems used by ARD collections\nARD image collections are organized in spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone has blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure Figure 10.2 shows the MGRS tiling system for a part of the Northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 10.2: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which breaks the coverage of Landsat satellites into images identified by path and row (see Figure @ref(fig:wrs)). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 10.3: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "href": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.4 Major global or large regional cloud provides",
    "text": "10.4 Major global or large regional cloud provides\nThe following ARD image cloud providers provide global or large regional data:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A level 2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products available include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedian of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are Copernicus DEM-30 and MOD13Q1, MOD10A1 and MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30)."
  },
  {
    "objectID": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.5 Accessing ARD image collections in cloud providers",
    "text": "10.5 Accessing ARD image collections in cloud providers\n\nWe now present the use of the sits R package to obtain information on ARD image collection from cloud providers, using the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information which many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured in a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections in cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be assessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "th_data_sources.html#amazon-web-services",
    "href": "th_data_sources.html#amazon-web-services",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.6 Amazon Web Services",
    "text": "10.6 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports collection SENTINEL-2-L2A which is open data. The bands in 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n\nFigure 10.4: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#microsoft-planetary-computer",
    "href": "th_data_sources.html#microsoft-planetary-computer",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.7 Microsoft Planetary Computer",
    "text": "10.7 Microsoft Planetary Computer\nThe sits package supports access to open data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30 meter resolution) and MOD13Q1-6.1(version 6.1 of the MODIS MOD13Q1 product). Access to the non-open data collection is available for users that have registration in MPC.\n\n10.7.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection."
  },
  {
    "objectID": "th_data_sources.html#r",
    "href": "th_data_sources.html#r",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.8 R",
    "text": "10.8 R\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\nFigure 10.5: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n10.8.1 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from Landsat-4/5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasilia in Brazil."
  },
  {
    "objectID": "th_data_sources.html#r-1",
    "href": "th_data_sources.html#r-1",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.9 R",
    "text": "10.9 R\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\nFigure 10.6: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n10.9.1 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject for variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line-of-sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be re-projected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso Brazil that is an area of the Amazon forest that has been deforested. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear included in relation to the Earth’s longitude."
  },
  {
    "objectID": "th_data_sources.html#r-2",
    "href": "th_data_sources.html#r-2",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.10 R",
    "text": "10.10 R\n\ncube_s1_grd &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\nFigure 10.7: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain corrected) images as described in the next session.\n\n10.10.1 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric distortions and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of the surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and re-project the SAR image from the slant range (radar line-of-sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\ncube_s1_rtc &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\nFigure 10.8: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes mountains. Users are invited to compare this images with the one from the SENTINEL-1-GRD collection and see the significant geometrical distortions of the GRD image compared with the RTC one.\n\n10.10.2 Copernicus DEM 30 meter images in MPC\nThe Copernicus digital elevation model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, designed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30 meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both case, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\n\nFigure 10.9: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "href": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.11 Copernicus Data Space Ecosystem (CDSE)",
    "text": "10.11 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CSDE S3 credentials site. The site will request you to add a new credential. You will receive two keys: an an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n      AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n      AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nAfter including these lines in your .Rprofile, restart R for the changes to take effect. By following these steps, users will have access to the Copernicus Data Space Ecosystem.\n\n10.11.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena river delta in Siberia in summertime.\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\nFigure 10.10: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n10.11.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE are the radiometric terrain corrected (RTC) Sentinel-1 images. in CDSE, this product is referred to as normalized terrain backscater (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ0) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set-up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift valley in Ethiopia.\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\nFigure 10.11: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "th_data_sources.html#digital-earth-africa",
    "href": "th_data_sources.html#digital-earth-africa",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.12 Digital Earth Africa",
    "text": "10.12 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level 2A (SENTINEL-2-L2A), organised as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardised NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California in Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30 meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in grid system WRS-2.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done wither using tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tiles (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n    lon_min = 46.1, lat_min = -15.6,\n    lon_max = 46.6, lat_max = -16.1\n  ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\nFigure 10.12: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data, covering the Serengeti plain in Tanzania.\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\nFigure 10.13: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for year 2020, for an area near the border between Congo and Rwanda.\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\nFigure 10.14: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "th_data_sources.html#harmonized-landsat-sentinel",
    "href": "th_data_sources.html#harmonized-landsat-sentinel",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.13 Harmonized Landsat-Sentinel",
    "text": "10.13 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS tiling system is identical as the one used for Sentinel-2 (MGRS). The tiles dimension is 109.8 km and there is an overlap of 4,900 m on each side.\nTo access NASA HLS, users need to registed at NASA EarthData, and save their login and password in a ~/.netrc plain text file in Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with the earthdatalogin::edl_netrc. This function creates a properly configured .netrc file in the user’s home directory and an environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections which are available in Microsoft Planetary Computer, which are a copy of the NASA collections and are faster to access.\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \nusername = \"&lt;your user name&gt;\", \npassword = \"&lt;your password&gt;\" \n) \n\nAccess to images in NASA HLS is done by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\nFigure 10.15: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#eo-products-from-terrascope",
    "href": "th_data_sources.html#eo-products-from-terrascope",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.14 EO products from TERRASCOPE",
    "text": "10.14 EO products from TERRASCOPE\nTerrascope is online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of Earth observation data and processing services that are accessible free of charge. Currently, sits supports the World Cover 2021 maps, produced by VITO with support form the European Commission and ESA. The following code shows how to access the World Cover 2021 convering tile “22LBL”. The first step is to use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_r\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\nFigure 10.16: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "th_data_sources.html#references",
    "href": "th_data_sources.html#references",
    "title": "\n10  EO Big Data Sources\n",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "th_design_frames.html",
    "href": "th_design_frames.html",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "",
    "text": "11.1 Outline\nThis chapter deals with the requirements of field surveys for statistical quality and for compatibility with EO data. Earth Observation (EO) are widely used in agricultural statistics production. However, the accuracy of EO-based land use classification is limited because of the limitations of using in situ census or survey data as training sets for EO applications. In this work, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "href": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.2 Matching in situ survey data to remote sensing analysis needs",
    "text": "11.2 Matching in situ survey data to remote sensing analysis needs\nData collections done by NSOs can potentially become the main source of training data for EO applications for agricultural statistics. However, while national surveys often adopt GPS technology, satellite imagery remains largely unused by NSOs. To change this status quo, the Global Strategy to Improve Agricultural and Rural Statistics compiled various use cases in its Handbook on Remote Sensing for Agricultural Statistics, highlighting the missing links between EO-driven surveys and most common NSO surveys [1]. Experience shows that sampling design, response design, and quality control of NSO surveys must follow well-documented requirements to obtain statistically sound results when using EO data.\nEO-related quality assurance of the in situ datasets developed in the FAO EOStat and ESA Sent4Stat projects includes two main components: (a) evaluation of survey design and (b) in situ data assessment using EO data. Quality assessment measures the suitability of a statistical survey (i.e. sampling and response design) to leverage satellite imagery in support of agriculture statistics. Many NSOs create their in situ protocols with a focus on aggregation at higher administrative tiers, often overlooking their potential application in EO contexts. Table 1 presents eleven criteria for NSOs to enable the combined use of in situ data for traditional surveys and EO applications.\n\nAssessment framework to qualify the compatibility of an in situ survey design to leverage EO satellite data for agriculture statistics\n\n\n\n\n\n\nCriteria related to the sampling design\n\n\n\n\n\nObservation timing allows identification of crop type in the field (unlike some household surveys, the survey must take place when the crop is visible on the field)\n\n\n\n\n\nMinimum number of samples for marginal crops (including intercrop types) to provide balanced datasets in terms of crop type sample distribution\n\n\n\n\n\nLocal homogeneity of each sample unit to match the corresponding satellite observation footprint\n\n\n\nCriteria related to the response design\n\n\n\n\n\nGeoreferenced ground observation at field or point level to link with satellite geospatial dataset (household geographic coordinates being insufficient)\n\n\n\n\n\nSample unit size at least matching the considered satellite observation footprint (not only the spatial resolution)\n\n\n\n\n\nContextual observation to document sample quality and qualify its representativity\n\n\n\n\n\nRich labelling of each sample beyond crop type to indicate specific growing conditions (e.g., weeds abundance, limited crop cover, water lodging, tree density)\n\n\n\n\n\nHigh precision of crop type nomenclature, including information about infrastructure and agriculture practices (e.g., irrigation, greenhouses, crop under canopy, agroforestry, species dominance for intercropping)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "href": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.3 EO-based quality control of in situ surveys",
    "text": "11.3 EO-based quality control of in situ surveys\nAn integral aspect of the statistical survey process is its quality control procedure. Nationwide surveys require heavy logistics involving hundreds of enumerators dispersed across the country. In many countries, digital encoding devices have integrated GPS receivers and communication support, making near real-time quality checks feasible. In Senegal, the Direction de l’Analyse, de la Prévision et des Statistiques Agricoles (DAPSA) employs near real-time quality control to oversee national data collection, enabling the field campaign to incorporate repetition requests and corrections as needed.\nAchieving the quality required for EO utilization imposes demands on the training of enumerators and presents more challenges for controllers. Besides being useful for aggregated surveys, in situ data must pass EO-based quality control checks. Such protocol is even more critical when combining datasets from distinct surveys requiring strict harmonisation. Experiences of FAO-EOSTAT and Sen2Stat with list frame and area frame statistical survey datasets led us to establish an EO-based quality control protocol. This protocol relies on existing maps and satellite time series processing as independent data quality control sources. Table 2 outlines the criteria applied for EO-based data quality control.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "href": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.4 Technical suitability of in situ data",
    "text": "11.4 Technical suitability of in situ data\nThe first three criteria of the EO-based quality control concern the technical suitability of in situ data for EO applications. Typically, surveyors record GPS coordinates for household localization, crop observation placement, or field area measurement. Requirements for geospatial analysis include ensuring the topological soundness of spatial features, which involves verifying polygon closure, identifying duplicate points, and resolving polygon overlaps (Figure 11.1).\n\n\n\n\n\n\n\nFigure 11.1: Quality control of geospatial features and their coordinates. Examples acquired during the FAO EOStat project in Senegal from left to right: polygon recorded as points sequence instead of a closed polygon, polygon overlap detected and solved, and benchmarking of various protocols and devices (tablet with integrated GPS versus Garmin receiver) to record field boundaries.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "href": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.5 Measuring spatial precision of field plot boundaries",
    "text": "11.5 Measuring spatial precision of field plot boundaries\nThe last three criteria of the EO-based quality control rely on satellite imagery analysis. The spatial precision of field plot boundaries requires visual image interpretation of high spatial resolution imagery that aligns with the survey timeframe. This type of control usually involves overlaying the plotted polygons onto frequent data, such as monthly cloud-free surface reflectance base maps. Figure 11.2 shows a good-quality sample that aligns well with a cultivated field, whereas the second sample area covers multiple fields and some trees. The ideal situation is to use high-resolution images to visually check samples and plot boundaries and then classify the areas with lower spatial resolutions. A possible situation is to use 4.8-meter Planet monthly reflectance maps for sample quality control and 10-meter Sentinel-2 images for classification.\nFrom an EO perspective, assessing sample quality requires time series from satellites such as Sentinel-2 for the growing season. Open-source platforms such as Sen4Stat and Sen2Agri toolbox [2] allow processing of all Sentinel-2 satellite images acquired along the season. One quantitative indicator of sample purity is the NDVI standard deviation (Figure 11.2, right plot) computed from the values of cloud-free satellite observations.\n\n\n\n\n\n\n\nFigure 11.2: Cloud-free Planet monthly base map images (left) and very high-resolution imagery (middle) overlaid with point observation expected to be representative of a circle area (radius of 20 m), as reported by the 2020 wheat rust survey in Ethiopia (Blasch et al., 2022). Plots highlight expected and unexpected NDVI profiles and the associated standard deviation for a homogeneous wheat field derived from the Sentinel-2 time series during the surveyed season..",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "href": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.6 Using NDVI temporal profiles to assess crop phenology",
    "text": "11.6 Using NDVI temporal profiles to assess crop phenology\nThe last EO-based requirement is the most difficult to address. We assume that crops of the same type, grown in the same agro-climatic zone, have similar planting and growing cycles. Analysing NDVI profiles across all crop samples strengthens crop label confidence. Atypical growth patterns (e.g., varied planting/cycle length or lack of growth) indicate potential mislabeling or mislocated samples. These samples need more scrutiny before they can be deemed usable.\nConsider Ethiopia’s diverse crop cycles in Figure 11.3. Temporal profiles for barley, fava beans, and teff show outliers indicating marginal sample quality. The different NDVI profiles for maize samples reveal that a significant portion underwent a double cropping cycle within the observation period. Since the first crop cycle delayed planting, the sample population needs to be divided appropriately using clustering methods. The complexity of the wheat cropping cycle is heightened by sowing dates and varietal differences affecting cycle length. Thus, EO quality control aims to reject unsuitable samples for model calibration and output validation. Subsequent EO-derived results, like crop type maps, area estimates, and yield forecasts, critically depended on this quality control process.\n\nknitr::include_graphics(\"./images//th_design_frames/QA_plot_Ethiopia.png\")\n\n\n\n\n\n\nFigure 11.3: NDVI temporal profiles interpolated from cloud-free Sentinel-2 multispectral images acquired along the observation period. Each colour curve corresponds to a sample for a given crop, while the black curve is the average NDVI value of all samples for this crop. Teff is blue, wheat is red, barley is light green, peas are pink, fava beans are orange, and maise is dark green. The CIMMYT provided these samples in the framework of the ESA Sen4Rust partnership.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_design_frames.html#summary",
    "href": "th_design_frames.html#summary",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.7 Summary",
    "text": "11.7 Summary\nIn this chapter, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments. Following these guidelines will increase the accuracy of EO-based land use classification.\nReferences{-}\n\n\n\n\n[1] \nJ. Delincé et al., Handbook on remote sensing for agricultural statistics. FAO, 2017.\n\n\n[2] \nP. Defourny et al., “Near real-time agriculture monitoring at national scale at parcel resolution: Performance assessment of the Sen2-Agri automated system in various cropping systems around the world,” Remote Sensing of Environment, vol. 221, pp. 551–568, 2019, doi: 10.1016/j.rse.2018.11.007.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Remote Sensing in the Design of Sampling Frames</span>"
    ]
  },
  {
    "objectID": "th_parcel_extraction.html",
    "href": "th_parcel_extraction.html",
    "title": "12  Automatic Extraction of Parcels",
    "section": "",
    "text": "12.1 Outline\nDescribes AI methods for automatic parcel detection. The link to the forthcoming Census of Agriculture 2026 in Brazil should be treated, explaining the functionality of the boundary data set to the efficiency gains expected during the operations of the census.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Automatic Extraction of Parcels</span>"
    ]
  },
  {
    "objectID": "crop_type_mapping.html",
    "href": "crop_type_mapping.html",
    "title": "Use Cases in Crop Type Mapping",
    "section": "",
    "text": "Outline\nDescribe the use cases.",
    "crumbs": [
      "Use Cases in Crop Type Mapping"
    ]
  },
  {
    "objectID": "ct_poland.html",
    "href": "ct_poland.html",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "",
    "text": "13.1 Introduction\nModern agriculture increasingly relies on advanced technologies, including satellite observations, which enable precise and regular monitoring of crop conditions. These methods make it possible to estimate sown areas, assess plant health, and forecast yields on a large scale - often in real time. Satellite data from missions such as Sentinel-2 support decision-making by providing objective and detailed information about the land. In this context, remote sensing becomes a valuable tool not only for farmers but also for institutions responsible for monitoring agricultural production at the national level. Recognizing this potential, the aim of the task was to develop a method for identifying thirty seven types of crops such as winter cereals (barley, rye, triticale, wheat), spring cereals (oat and barley), root crops (potato, sugar beet), buckwheat, maize, winter rapeseed, strawberry, grassland, along with legumes (beans, peas, lupines, lentil), vegetables (carrots, parsley, lettuce, cabbage, cucumber, tomato, etc.) and fruit trees (mainly apples, pears, cherries and plums). The task utilized satellite data, specifically radar imagery from Sentinel-1 and optical imagery from Sentinel-2.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Crop monitoring with SAR images in Poland</span>"
    ]
  },
  {
    "objectID": "ct_poland.html#methods",
    "href": "ct_poland.html#methods",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.2 Methods",
    "text": "13.2 Methods\nThe task was carried out in the following stages:\n\nData acquisition and pre-processing - development of automatic procedures for the collection and pre-processing of radar and optical images,\nMulti-temporal image processing implementation of procedures for processing bi-polarized Sentinel-1 radar images and Sentinel-2 optical images,\nSegmentation of images using the Land Parcel Identification System (LPIS) integrating bothSentinel-1 and Sentinel-2 data,\nTraining the classification system and developing a crop classification algorithm based on optical and radar satellite images in appropriate time series,\nDevelopment of a validation procedure,\nCreating procedures for reporting and creating statistics at all administrative levels of the country (voivodships, districts, communes).\n\nThe classification was carried out separately for each orbit based on the following three steps: 1) construction of the main database; 2) classifier development; and 3) crop map extraction (see Figure 13.1).\n\n\n\n\nFigure 13.1: Outline of classification steps."
  },
  {
    "objectID": "ct_poland.html#data-acquisition-and-pre-processing-satellite-data",
    "href": "ct_poland.html#data-acquisition-and-pre-processing-satellite-data",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.3 Data acquisition and pre-processing Satellite Data",
    "text": "13.3 Data acquisition and pre-processing Satellite Data\nAs part of the crop classification task, satellite data from the Copernicus Sentinel missions were used. Procedures for the automatic downloading of satellite imagery were developed to support this process. Images can be retrieved directly from the Copernicus Data Space Ecosystem website. The downloading procedure requires specifying the time range of image acquisition, the area of interest, and the type of sensor. The product type is automatically matched to the selected sensor to ensure compatibility with the classifier’s requirements. Sentinel-1 radar images are downloaded in the SLC (Single Look Complex) format, while Sentinel-2 optical images are obtained in the S2MSI2A format, corresponding to Level-2A processing.\nProcedures for data pre-processing were also created. For radar images, the procedure carries out orthorectification and filtration processes. In the case of optical images, errors in the cloud mask were diagnosed. For this reason, a procedure was developed that detects these errors in the first stage and corrects them, and then an algorithm was created to fill in the data gaps caused by cloud cover. The proposed procedure works for individual spectral channels and is adapted only to agricultural and forestry areas, excluding urban areas.\nA set of C-band, dual polarization (VH + VV) mode, Single Look Complex (SLC) Sentinel-1 (S-1) products were used. Images for the period just before a growing season (mid-February–end-March) were acquired at a frequency of 12 days. During the growing season (beginning-April–end-September) images were acquired at a frequency of six or 12 days. A total of around 1,050 Sentinel-1 images were used for each year.\nThe Sentinel-2 dataset was acquired for the study area, focusing on agricultural land cover. The data consists of multispectral images containing 13 spectral channels with a spatial resolution of 10-60 m. The images cover multiple dates throughout the growing season, allowing extraction of temporal profiles and vegetation indices. These features were crucial for distinguishing crop types and served as input for machine learning-based classification."
  },
  {
    "objectID": "ct_poland.html#reference-data",
    "href": "ct_poland.html#reference-data",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.4 Reference data",
    "text": "13.4 Reference data\nThe reference data can include administrative data or in situ collections. Nevertheless, the quantity of point per given class is required for the sufficient level of the estimate accuracy. Tests were carried out on how the statistics of individual crops change depending on the size of the sample. It was considered that in the case of using in-situ data, the minimum sample per class should be not less than 50 points within one flight path of the S-1 satellite (for about 4-5 scenes). If agricultural declarations from the Agriculture Restructurisation and Modernisation Agency (ARMA) database are used as input, the sample will be balanced in numbers and no less than 800 points will be taken for training and testing classifications. The large size of the ARMA sample is due to its uncertainty.\nAuxiliary data consisted of records taken from the Land Parcel Identification System database. This database contains vector data about, among other attributes, the use of the terrain (urban settlements, communication areas, forest, agriculture lands, etc.) and crop declarations made by farmers to receive direct payments under Common Agricultural Policy instruments. LPIS vector data formed the basis for image segmentation. Farmers’ declarations were used as a reference dataset both for training and validation of the classifier. Reference data were randomly sampled from around 2,000,000 agricultural parcels larger than 1 ha, for which only one crop had been declared. A total of 1, 000 parcels, per crop type, per orbit, were sampled. Of these, 80% were used for training, and 20% were used for validation. It was possible to extract the number of parcels per class for the following orbits (O): 22, 124, 51, 153. However, this was not possible for the most eastern and most western orbits, which only partially coincide with the country’s land area.\nThe study involves the identification and description of selected crops directly in the field (in situ) based on inspections of agricultural plots. Data was obtained from at least 50 representative plantations and collected in all provinces. Data preparation involves measuring plant height, inter-row width and development stages, taking photos from different perspectives and assessing weed infestation. All observations are carefully documented, and local conditions that may affect the reflection of radar waves are also taken into account, which ensures high quality and reliability of the information collected.\nDevelopment of vector data for object classification (training and control samples) based on information from administrative sources available in Statistics Poland, the Integrated Management and Control System (IACS) and the Database of Topographic Objects (BDOT). In the course of the work, the source data were analysed and the geometry of the spatial division units necessary to present the results were identified. For the presentation of project data, the use of vector data of administrative boundaries obtained from the National Register of Boundaries and the area of territorial division units of the country (PRG) at the level of municipalities was established. These data have been obtained annually from the Head Office of Geodesy and Cartography (GUGiK) since 2002. In addition, geocoding was performed, which ensures the presence of identifiers of space division units in the designed databases."
  },
  {
    "objectID": "ct_poland.html#multi-temporal-image-processing-of-sentinel-1-and-sentinel-2",
    "href": "ct_poland.html#multi-temporal-image-processing-of-sentinel-1-and-sentinel-2",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.5 Multi-temporal image processing of Sentinel-1 and Sentinel-2",
    "text": "13.5 Multi-temporal image processing of Sentinel-1 and Sentinel-2\nWe developed procedures for the use of SAR polarimetric data, including the calculation of the T2 coherence matrix and H/α polarimetric decomposition parameters. After gernerating the A procedure has been created for mosaicking the output layers. Following the implementation of all processing steps, six input layers were generated for each acquisition date and each flight strip, forming the basis for calculating indicators. Subsequently, a series of indicators were created that describe phenological changes in crops using the difference between polarimetric parameters and changes over time in individual parameters.\nThe authors also developed a Normalized Multitemporal, which is calculated on the basis of individual elements of the coherence matrix or elements of the H/α decomposition for two terms. The ratio is calculated for all combinations of available dates, taking the earlier date as the start date and the later date as the final one. In total, about 3800 such parameters were calculated. The second type of developed parameters, which are aimed at describing the growth rate of crops, are parameters a and b of the linear function describing the increase in signal reflection intensity in individual periods. About 7500 such parameters were calculated. The third type of developed parameters related to crop growth are Standardized Ratio of Coherence Matrix Elements. These parameters describe the change in the way the wave is reflected depending on the shape of the crop. These ratio are calculated for individual dates. In total, about 110 parameters of this type were calculated.\nAt country scale, a pixel-based image processing approach would be time consuming, and require enormous computational power. Moreover, the results of such a classification would be affected by the well-known speckle effect. To overcome these problems, we adopted an object-oriented approach, based on the external vector layer of agricultural parcels. The construction of the database of objects used for classification was divided into three steps. In the first step, all images were pre-processed as follows:(1) Noise reduction in SLC S-1 products; (2) Range Doppler terrain correction was applied to project images to zone 34 of the UTM coordinate system; (3) Creation of mosaics (one mosaic per acquisition date), and (4) Image smoothing using an enhanced Lee filter (window size 3 × 3). Then, we built the coherence matrices and applied the H/α decomposition for dual polarization (VV + VH) data. The Coherence matrices were re-scaled using the following substitutions:\n\\[\nT^{'}_{nR} = 1/2*ln(T^{2}_{nR} + T^{2}_{nI})\n\\] \\[\nT^{'}_{nI} = tan^{-1}(T_{nR} / T_{nI})\n\\] where \\(T_{nR}\\) is the real part of the coherence matrix \\(T_{n}\\), and \\(T_{nI}\\) is its imaginary part. Six basic parameters were obtained: three real parts of the coherence matrix – \\(T_{11}\\), \\(T_{12}\\), and \\(T_22\\), and three parameters of the H/α decomposition – entropy (H), alpha (α) and lambda (λ)."
  },
  {
    "objectID": "ct_poland.html#image-segmentation",
    "href": "ct_poland.html#image-segmentation",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.6 Image Segmentation",
    "text": "13.6 Image Segmentation\nThe next step involved image segmentation, with the objective of delineating spatially homogeneous segments such that each segment corresponded to a single crop type (Fig. 2). The segmentation process was performed using eCognition® software. To begin with, LPIS vectors were used to create initial segments that corresponded to parcels, and exclude non-agricultural areas of no interest (e.g. forests, water, built-up areas). As some LPIS parcels contained various crops, an additional division was needed. This was achieved using a time series of λ parameters from the H/α decomposition (eight images acquired in equal time intervals between the end of February and the end of June). The multiresolution segmentation method was used to divide heterogeneous segments. The main assumption of this re-segmentation was that the smallest segment had to be larger than 0.2 ha. The chosen segmentation method enables the integration of multiple image layers, offering consistent and reproducible results. The main parameter of the segmentation is the scale parameter, which was set based on trial and error by an experienced operator. The aim of the process is to create more homogenous objects than are needed, in order to avoid the situation where a heterogeneous object is not divided.\n\n\n\n\nFigure 13.2: The segmentation procedure: the Land Parcel Identification System vector layer (a), segments obtained from the vector layer with non-agricultural parcels excluded (b), multi-temporal λ layers used for re-segmentation (c), the resulting segmentation used for the construction of the main database (d).\n\n\n\nIn the final step, mean values for the six basic parameters for each segment were calculated for all acquisition dates. This data was stored as records in the main database serving as a foundation for subsequent analyses. Since the records are not georeferenced, they allow for faster access and processing. Each entry remains connected to its corresponding segment on the map through a unique identification (ID) number, ensuring consistent referencing between the dataset and spatial layers."
  },
  {
    "objectID": "ct_poland.html#crop-classification-algorithms",
    "href": "ct_poland.html#crop-classification-algorithms",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.7 Crop classification algorithms",
    "text": "13.7 Crop classification algorithms\n\n13.7.1 Development of the phenological database\nAt the beginning, segments that correspond to the reference dataset extracted from farmers’ declarations were selected from the main database. Phenological crop growth descriptors were then calculated using the basic parameters derived from Sentinel-1 images and stored in the main database. The calculation of phenological indices was based on the assumption that, as crops grow, signal power, signal polarization and the scatter mechanism change. Thus the extraction of multi-temporal indices enhances differences among crops and increases classification accuracy. The following phenological indices were proposed and used:\n\nTime series of Normalized Ratio of Coherence Matrix Elements (\\(NRCM_{i,j}\\)). This index aims to enhance differences in the depolarization of the scattered signal from crops. It was calculated as:\n\n\\[\nNRCM_{i,j,k,l}(t) = \\frac{T_{ij} - T_{kl}}{T_{ij} + T_{kl}}\n\\] where: \\(T_{ij}\\) and \\(T_{kl}\\) are elements in the coherence matrix. The indexes \\(1 \\le i \\le j \\le k \\le l \\le n\\) are dimensions of a coherence matrix, and \\(t\\) is time.\n\nNormalised Time Ratios (\\(NRT^{n,m}_{i,j}\\)) enhance the difference in the growing rate of plants in different periods. They were calculated from elements in coherence matrices according to the equation: \\[\nNTR^{n,m}_{i,j} = \\frac{T_{ij}(t_n) - T_{ij}(t_m)}{T_{ij}(t_n) + T_{ij}(t_m)}\n\\] with \\(n \\in {1..M − 1}\\) and \\(m \\in {n + 1..M}\\) where M is the number of images, \\(T_{ij}\\) is an element in the coherence matrix, \\(t_n\\) is an earlier time point and \\(t_m\\) is a later time point.\nSimilar indices were calculated for the parameters of the H/α decomposition. These indices show change over time in the entropy of the scattered signal and the scatter mechanism. \\[\nNTR^{n,m}_{ent} = \\frac{P(t_n) - P(t_m)}{P(t_n) + P(t_m)}\n\\] with \\(n \\in {1..M − 1}\\) and \\(m \\in {n + 1..M}\\) where M is the number of images, \\(P\\) is one of the parameters (H, α, λ) of the H/α decomposition, \\(t_n\\) is an earlier time point and \\(t_m\\) is a later time point.\nLinear functions (LF) between local signal extremes (points where the signal power trend changes considerably) in time series are another way to enhance differences in plant growth rates. These were calculated for elements in the coherence matrices and for the parameters in the H/α decomposition, as follows: \\[\nP(t)=at+b\n\\] for \\(t\\) between \\(min(t_{local})\\) and \\(max(t_{local})\\), where: P is a value of one of the six basic parameters. It can be an element in a coherence matrix or a parameter in the H/α decomposition, \\(min(t_{local})\\) is a time point where the signal value is minimum, and \\(max(t_{local})\\) is a time point where the signal value is maximum.\n\nThe final database contained mean values for all basic parameters and phenological indices in segments that corresponded to the reference dataset extracted from farmers declarations.\n\n13.7.2 Iterative random forest fitting\nDue to the large number of combinations of multi-temporal indices, each segment can be described by thousands of parameters. These parameters are not equally important for the classification, and some may degrade it by introducing noise. Therefore, iterative classifier fitting was applied to select the optimal parameters for classification. Each iteration is as follows:\n\ndevelop 10 random training and testing datasets (80% of objects were considered as training data, 20% were used for testing) based on the reference dataset.\nrun a random forest classification (number of trees 1,000; depth 100).\nanalyse the importance of every parameter.\ncalculate the overall accuracy of the classification for each random training and testing dataset.\ncalculate minimum, maximum and mean overall accuracy for each iteration, and remove 5% of the least-important parameters (as a compromise between the precision of the analysis and the calculation time).\n\nThe results of all iterations were stored in a validation database, which contained error matrices corresponding to input parameters and classifiers. Based on the error matrix, the best input parameters and classifier were selected and used for the final crop map extraction."
  },
  {
    "objectID": "ct_poland.html#validation",
    "href": "ct_poland.html#validation",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.8 Validation",
    "text": "13.8 Validation\nValidation of the classification results is carried out using in-situ data (the part of data that was not used to train the system), data from farmers’ declarations submitted to the Agriculture Restructurisation and Modernisation Agency system and data from ARMA on-the-spot inspections. The matrix of errors and accuracy of underestimation and overestimation of individual crops is calculated. Confusion matrices are produced separately for all classifications using 20% of parcels from reference datasets. Overall accuracy (OA) and kappa are calculated. User’s (UA) and producer’s (PA) accuracies, as well as the F1 score are calculated to assess omission, commission and mean errors within classes, respectively. To test the effect of mosaicking on the classification, we perform a map-to-map comparison of records in the farmer’s declarations database and the per-orbit classification before and after mosaicking."
  },
  {
    "objectID": "ct_poland.html#crop-map-publishing",
    "href": "ct_poland.html#crop-map-publishing",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.9 Crop map publishing",
    "text": "13.9 Crop map publishing\nArea statistics are created based on the resulting classification raster. The areas are added up to all administrative levels of the country (voivodeships, districts, communes). The identification number of individual units allows for the integration of this data with the Statistics Poland reporting system. The results of the classification are published on [geostatistical portal] (https://portal.geo.stat.gov.pl/). The refence paper for this work has been published in a leading journal [1]."
  },
  {
    "objectID": "ct_poland.html#references",
    "href": "ct_poland.html#references",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nE. Woźniak et al., “Multi-temporal phenological indices derived from time series Sentinel-1 images to country-wide crop classification,” International Journal of Applied Earth Observation and Geoinformation, vol. 107, p. 102683, 2022, doi: 10.1016/j.jag.2022.102683."
  },
  {
    "objectID": "ct_mexico.html",
    "href": "ct_mexico.html",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "",
    "text": "14.1 Outline\nThis chapter contributes to the handbook’s goal of enhancing the operational use of Earth Observation (EO) in agricultural statistics by presenting a detailed, step-by-step classification exercise. While the potential of remote sensing is widely recognized, many National Statistical Offices (NSOs) still face challenges in transforming satellite data into reliable statistical products. To address this, we provide a didactic case study focused on crop classification in Mexico, designed to be fully reproducible by the user.\nThe exercise centers on the Yaqui Valley, a prominent agricultural region in Sonora, Mexico. The primary analysis period covers the autumn-winter agricultural season from October 2017 to May 2018. The workflow is built upon an Object-Based Image Analysis (OBIA) approach [1], [2], where machine learning models are trained to classify agricultural parcels rather than individual pixels. This method often yields more accurate and coherent results that better represent real-world field conditions.\nThe methodology leverages a fusion of open-access optical and radar satellite data [3]. We use the Harmonized Landsat and Sentinel-2 (HLS) product [4], [5], which provides analysis-ready, radiometrically consistent surface reflectance data, along with Sentinel-1 SAR imagery [6] to ensure data availability regardless of cloud cover. To facilitate the didactic and reproducible nature of this chapter, all necessary components—the pre-processed satellite imagery, the ground-truth labels used for training, and the complete Python code—are made available for download from a public repository. This allows the reader to focus entirely on understanding and replicating the classification process itself.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_mexico.html#methods",
    "href": "ct_mexico.html#methods",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "\n14.2 Methods",
    "text": "14.2 Methods\nThis section details the complete, step-by-step workflow for the crop classification exercise, from the initial data description to the final machine learning modeling.\n\n14.2.1 Study Area and datasets\nThe methodology is demonstrated in the Yaqui Valley, located in the state of Sonora, Mexico. This area is one of the nation’s most productive agricultural regions, characterized by intensive, irrigated cultivation of crops such as wheat, corn, and vegetables, making it an ideal case study for classification techniques.\nTo ensure the exercise is fully reproducible, all required datasets are provided. The analysis relies on a fusion of publicly available optical and radar imagery covering the agricultural season from October 2017 to May 2018. The datasets include:\n\nOptical Time-Series: A set of monthly, cloud-free composite images derived from the Harmonized Landsat and Sentinel-2 (HLS) collection [5]. Additionally, a single geometric median composite of the entire period is provided, which serves as the primary layer for image segmentation. Each image contains 13 bands, including surface reflectance and a suite of vegetation indices.\nRadar Time-Series: A corresponding monthly time-series of Sentinel-1 Ground Range Detected (GRD) imagery [6]. Each monthly composite includes bands for VV and VH backscatter, as well as the calculated Radar Vegetation Index (RVI).\nGround-Truth Data: A shapefile containing georeferenced sample points used for training and validating the models. This information is derived from administrative records of the Agrifood and Fisheries Information Service (SIAP) of Mexico [7].\n\n14.2.2 Image segmentation for parcel delineation\nThe first step in our analytical workflow is to transition from a pixel-based to an Object-Based Image Analysis (OBIA) approach. This process, known as image segmentation, aggregates pixels into homogeneous regions that are intended to correspond to real-world features. For this agricultural application, the fundamental unit of analysis is shifted from the individual pixel to the image object or segment, which represents a potential agricultural parcel.\nFor this task, the Shepherd Segmentation algorithm from the RSGISLib library was employed [8]. This algorithm was applied to the high-quality, multi-band geometric median composite described previously. The method is a two-stage process: it first uses a k-means clustering to group pixels based on their spectral similarity, and then iteratively merges these small clusters into larger, spatially contiguous segments. Key parameters used in this process were:\n\nNumber of Clusters: The process was initialized with 80 spectral clusters to capture the initial variability.\nInput Bands: All 13 bands of the composite image (six optical and seven vegetation indices) were used, providing a rich spectral basis for clustering.\nMinimum Segment Size: A minimum size of 100 pixels was enforced to eliminate small, noisy polygons and ensure that the final objects are representative of field parcels.\n\nThe final output of this step is a vector shapefile containing the delineated polygons. It is important to acknowledge that as an automated process, the segmentation may not perfectly align with all real-world field boundaries, and some inaccuracies from under- or over-segmentation can occur, particularly in complex landscapes. Nevertheless, these generated objects form the foundational units for all subsequent labeling and feature extraction steps.\n\n14.2.3 Ground-Truth labeling and quality control\nOnce the agricultural parcels were delineated, the next critical step was to assign a known crop type to each segment using the ground-truth data. This was achieved through a spatial join [9], a standard GIS operation that overlays the georeferenced sample points derived from SIAP administrative records onto the polygon layer. This process transfers the crop label from each point to the larger parcel segment that contains it.\nHowever, an initial join can result in ambiguity; a single polygon might intersect with points corresponding to multiple different crop types, or with no points at all. To create a reliable dataset for model training, a strict quality control filter was therefore applied to the joined data. The rule for this filter was to retain only those polygons that were unambiguously labeled—that is, polygons that contained sample points corresponding to one, and only one, unique crop type.\nThis filtering protocol is crucial for improving the purity of the training data. Any polygon that contained no sample points (unlabeled) or was associated with conflicting crop labels (ambiguously labeled) was discarded from the final dataset. The output of this stage is a clean shapefile of high-confidence parcels, each with a single, verified class label, which serves as the definitive ground truth for the subsequent feature extraction and modeling phases.\n\n14.2.4 Time-Series Feature Engineering\nThe final data preparation step involves feature engineering, where a comprehensive set of quantitative attributes is generated for each delineated parcel. This feature set is designed to capture the unique temporal and spectral signatures of each crop type, which the machine learning model will use to perform the classification. The central data structure for this process is the Raster Attribute Table (RAT) associated with the segmentation file, which is progressively populated with new attributes.\nThe process is executed using the zonal statistics [10] capabilities of the rsgislib library. First, the RAT is populated with class proportion information derived from the ground-truth labels raster. This initial step calculates, for each segment, the percentage of its area corresponding to each crop class.\nSubsequently, the RAT is further enriched with a much larger set of features derived from the entire time-series of satellite imagery. For each band of every monthly composite—both the optical HLS and radar Sentinel-1 images—a set of descriptive statistics is computed for each parcel. These statistics include the mean, standard deviation, minimum, and maximum pixel values within the parcel’s boundary.\nThe result of this stage is a highly detailed data table within the RAT, where each row represents a single parcel and the columns contain its complete phenological and structural profile. This profile captures not only the average spectral response of the parcel over time (the mean values) but also its internal heterogeneity or texture (the standard deviation). This final feature table is then exported as a CSV file, creating the analysis-ready dataset for the machine learning workflow.\n\n14.2.5 Machine Learning Modeling\nThe final stage of the methodology focuses on training and selecting the optimal classification model using the comprehensive feature set generated in the previous steps. To streamline this process, this exercise utilizes PyCaret [11], an open-source, low-code Automated Machine Learning (AutoML) library in Python. PyCaret was chosen for its ability to efficiently automate the entire experimental workflow, from data preprocessing to model comparison and selection.\nBefore initiating the AutoML experiment, two critical data preparation steps were performed. First, to address the severe class imbalance in the dataset, any crop class with fewer than 10 representative samples was aggregated into a single ‘Grouped Minorities’ category. This pragmatic approach creates a more stable class distribution for the models to learn from. Second, the dataset was partitioned into a 70% training set and a 30% testing set using stratified sampling. This ensures that the proportional representation of each class is preserved in both subsets, which is crucial for obtaining an unbiased evaluation of model performance.\nThe PyCaret setup() function was then used to configure the experiment, establishing a pipeline of automated preprocessing steps that are applied during model training and evaluation. This included robust feature scaling, automated feature selection to identify the 50 most predictive variables from the high-dimensional dataset, and an additional balancing technique (RandomOverSampler) [13] applied to the training folds within the cross-validation loop. Model performance was evaluated using a 5-fold stratified cross-validation strategy to ensure robust and reliable results.\nFinally, the compare_models() function was executed to systematically train and evaluate a wide range of classification algorithms. The results identified the Extreme Gradient Boosting (XGBoost) [14] model as the top performer, achieving the highest cross-validated accuracy of 89.5%. This winning model, along with its complete preprocessing pipeline, was then saved for generating the final results presented in the following section.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_mexico.html#results",
    "href": "ct_mexico.html#results",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "\n14.3 Results",
    "text": "14.3 Results\nThis section presents the performance of the final classification model, Extreme Gradient Boosting (XGBoost), on the independent test set. The model yielded an overall accuracy of 89.5%.\nA detailed breakdown of the model’s performance is provided in the classification report (Table 14.1). The weighted average F1-score, which accounts for class imbalance, was 0.896. The results show a performance disparity between classes. The model achieved high recall scores for majority classes such as Wheat (94.9%) and Other Veg/Water (98.8%), while recall was significantly lower for minority classes, including the Grouped Minorities category (4.5%).\n\n\nTable 14.1: Per-class classification report for the XGBoost model on the test set, ordered by recall (descending)\n\n\n\nClass Name\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\nOther Veg/Water\n0.978\n0.988\n0.983\n494\n\n\nWheat\n0.941\n0.949\n0.945\n1509\n\n\nChickpea\n0.777\n0.880\n0.826\n234\n\n\nCorn\n0.870\n0.812\n0.840\n271\n\n\nAlfalfa\n0.842\n0.762\n0.800\n21\n\n\nTomato\n0.604\n0.744\n0.667\n43\n\n\nBean\n0.711\n0.696\n0.703\n46\n\n\nPotato\n0.571\n0.462\n0.511\n52\n\n\nChile Pepper\n0.500\n0.455\n0.476\n11\n\n\nSquash\n0.486\n0.429\n0.456\n42\n\n\nAsparagus\n0.750\n0.333\n0.462\n9\n\n\nGrouped Minorities\n0.200\n0.045\n0.074\n22\n\n\nWeighted Avg\n0.895\n0.899\n0.896\n2754\n\n\n\n\n\n\nTo visually assess the model’s behavior, the normalized confusion matrix is presented in Figure 14.1. The strong diagonal, with recall values of 98.8% for Other Veg/Water and 94.9% for Wheat, confirms the model’s high accuracy for the majority classes. However, the off-diagonal values highlight specific areas of confusion. For instance, Potato is frequently misclassified as Corn (36.5% of the time), and a notable portion of Onion samples are also confused with Corn (19.6%). This suggests that these crops may share similar spectral profiles at certain growth stages within the analyzed period. As expected, the model struggles most with the Grouped Minorities class, which has a recall of only 4.5% and is primarily confused with the dominant Wheat and Other Veg/Water classes.\n\n\n\n\n\n\n\nFigure 14.1: Normalized confusion matrix (Recall) of the XGBoost model on the test set. The diagonal shows the percentage of correctly classified parcels for each class\n\n\n\n\nFinally, the trained model can be applied to all delineated parcels in the study area to produce the final crop classification map for the 2017-2018 agricultural season (an example is shown in Figure 14.2).\n\n\n\n\n\n\n\nFigure 14.2: Crop type classification map for the Yaqui Valley study area.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_mexico.html#discussion",
    "href": "ct_mexico.html#discussion",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "\n14.4 Discussion",
    "text": "14.4 Discussion\nThe application of a combined optical-radar, object-based approach with an XGBoost model yielded an overall accuracy of 89.5% on the independent test set. The model’s recall for majority classes, such as Wheat (94.9%) and Other Veg/Water (98.8%), corresponds to their distinct spectral signatures and the large number of available training samples.\nIn contrast, the model’s performance was lower for classes with fewer training samples. The detailed classification report (Table 14.1) shows recall values of 33.3% for Asparagus and 4.5% for the aggregated Grouped Minorities category. This performance gap is directly related to the class imbalance in the training data. An insufficient number of examples can limit a model’s ability to learn a generalizable signature for minority classes, potentially leading to a bias towards better-represented classes.\nThe confusion matrix (Figure 14.1) provides further detail on specific inter-class confusion. For example, a portion of Potato parcels were misclassified as Corn. A possible explanation for this is that these crops may share similar phenological profiles or spectral characteristics during key moments of the analyzed growing season. This represents a classification challenge that is both statistical and agronomic in nature.\nFrom a practical standpoint for National Statistical Offices (NSOs), the open-source workflow presented here offers a methodology for mapping dominant crops, which are often of primary interest for food security monitoring. The results from this exercise also underscore that the quality, quantity, and balance of ground-truth data are determining factors that influence the performance of supervised classification models.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_mexico.html#conclusion-and-future-work",
    "href": "ct_mexico.html#conclusion-and-future-work",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "\n14.5 Conclusion and future work",
    "text": "14.5 Conclusion and future work\nThis chapter presented a reproducible, open-source workflow for crop classification in a major agricultural region of Mexico. The methodology, which combines an object-based approach with an XGBoost model trained on multi-sensor time-series features, yielded an overall accuracy of 89.5% for mapping dominant crops. A key observation from the results is that model performance is influenced by the quality and class balance of the ground-truth data, as shown by the lower recall values for minority crop types.\nThe current results can serve as a baseline for further research. One direction for future work is to explore deep learning architectures capable of learning directly from temporal data, as an alternative to relying on pre-computed statistical features. The preliminary experiments conducted as part of this study indicate potential research directions in this area.\nOne avenue for investigation is the use of 1D Convolutional Neural Networks (1D-CNNs) [15] to automatically extract features from sequential data like vegetation index time-series. A further area for investigation is the fusion of optical and radar data within the deep learning model structure itself. A different approach is to develop dual-input hybrid models [16]. Such an architecture can process temporal data through a 1D-CNN or LSTM branch while simultaneously processing static, season-long features through a parallel MLP branch, allowing the model to learn from both dynamic and stable parcel characteristics. A systematic evaluation of these deep learning approaches could determine their potential to increase classification accuracy for NSO applications.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_mexico.html#references",
    "href": "ct_mexico.html#references",
    "title": "\n14  Crop classification in Mexico\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nT. Blaschke, “Object based image analysis for remote sensing,” {ISPRS} Journal of Photogrammetry and Remote Sensing, vol. 65, no. 1, pp. 2–16, 2010.\n\n\n[2] \nB. Dezso et al., “Object-based image analysis in remote sensing applications using various segmentation techniques,” vol. 37, pp. 103–120, 2012.\n\n\n[3] \nA. Orynbaikyzy, U. Gessner, and C. Conrad, “Crop type classification using a combination of optical and radar remote sensing data: A review,” International Journal of Remote Sensing, vol. 40, no. 3, pp. 1–43, 2019, doi: 10.1080/01431161.2019.1569791.\n\n\n[4] \nJ. Ju et al., “The harmonized landsat and sentinel-2 version 2.0 surface reflectance dataset,” Remote Sensing of Environment, vol. 324, p. 114723, 2025, doi: 10.1016/j.rse.2025.114723.\n\n\n[5] \nJ. Ju et al., Harmonized landsat sentinel-2 (HLS) product user guide. NASA/USGS, 2022.\n\n\n[6] \nM. Shorachi, V. Kumar, and S. C. Steele-Dunne, “Sentinel-1 SAR backscatter response to agricultural drought in the netherlands,” Remote Sensing, vol. 14, no. 10, p. 2435, 2022, doi: 10.3390/rs14102435.\n\n\n[7] \nGeospatial World, “The geospatial technology in the agrifood and fisheries information service (SIAP), méxico,” 2014, [Online]. Available: https://geospatialworld.net/article/the-geospatial-technology-in-the-agrifood-and-fisheries-information-service-siap-mexico/.\n\n\n[8] \nD. Clewley et al., “A python-based open source system for geographic object-based image analysis (GEOBIA) utilizing raster attribute tables,” Remote Sensing, vol. 6, no. 7, pp. 6111–6135, 2014, doi: 10.3390/rs6076111.\n\n\n[9] \nP. A. Longley, M. F. Goodchild, D. J. Maguire, and D. W. Rhind, Geographic information science and systems. John Wiley & Sons, 2015.\n\n\n[10] \nS. Winsemius and J. Braaten, “Zonal statistics,” in Cloud-based remote sensing with google earth engine, Springer, 2024, pp. 463–485.\n\n\n[11] \nM. Ali, “PyCaret: An open source, low-code machine learning library in python.” https://pycaret.readthedocs.io/en/latest/, 2020, [Online]. Available: https://www.pycaret.org.\n\n\n[12] \nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, no. 1, pp. 321–357, 2002.\n\n\n[13] \nA. Crudu and MoldStud Research Team, “Strategies and resources for successfully tackling class imbalance in classification challenges,” MoldStud Articles, 2025, [Online]. Available: https://moldstud.com/articles/p-strategies-and-resources-for-successfully-tackling-class-imbalance-in-classification-challenges.\n\n\n[14] \nT. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–794, doi: 10.1145/2939672.2939785.\n\n\n[15] \nS. Guessoum et al., “The short-term prediction of length of day using 1D convolutional neural networks (1D CNN),” Sensors, vol. 22, no. 23, p. 9517, 2022, [Online]. Available: https://www.mdpi.com/1424-8220/22/23/9517.\n\n\n[16] \nS. Saidi, S. Idbraim, Y. Karmoude, A. Masse, and M. Arbelo, “Deep-learning for change detection using multi-modal fusion of remote sensing images: A review,” Remote Sensing, vol. 16, no. 20, p. 3852, 2024, doi: 10.3390/rs16203852.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Crop classification in Mexico</span>"
    ]
  },
  {
    "objectID": "ct_senegal.html",
    "href": "ct_senegal.html",
    "title": "15  Multi-seasonal crop mapping in Senegal",
    "section": "",
    "text": "15.1 Overview",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multi-seasonal crop mapping in Senegal</span>"
    ]
  },
  {
    "objectID": "ct_zimbabwe.html",
    "href": "ct_zimbabwe.html",
    "title": "16  Crop classification in Zimbabwe",
    "section": "",
    "text": "16.1",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Crop classification in Zimbabwe</span>"
    ]
  },
  {
    "objectID": "ct_chile.html",
    "href": "ct_chile.html",
    "title": "17  Crop classification and land use mapping in Chile",
    "section": "",
    "text": "17.1 Overview\nThis section outlines a pilot methodology for land use and crop classification in Chile, using the Maule Region as a case study. It combines Sentinel-2 time-series imagery, digital elevation models (DEM) and machine learning to produce scalable LULC maps. Ground truth data from agricultural censuses and other sources are used to enhance classification accuracy. The approach aims to support the integration of remote sensing into official agricultural statistics.\nThe study highlights key challenges, including data gaps, cloud cover, and crop heterogeneity. However, it also shows how time-series analysis helps capture crop phenology for better classification. This pilot sets the foundation for a standardized national mapping framework, enabling more timely and spatially detailed agricultural data for decision-making.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Crop classification and land use mapping in Chile</span>"
    ]
  },
  {
    "objectID": "ct_digital_earth_africa.html",
    "href": "ct_digital_earth_africa.html",
    "title": "18  Crop classification using Digital Earth Africa",
    "section": "",
    "text": "18.1 Outline\nDigital Earth Africa offers continental-scale satellite-derived data products and provides guidance on their application in analytical workflows. This chapter demonstrates the use of high quality, cloud-free, geomedian composite images with median absolute deviations (GeoMADs) for crop type mapping. Digital Earth Africa’s GeoMADs are applied to a classification framework in the satellite image time series (SITS) package in the R language.\nThe chapter demonstrates how high-quality, ‘off-the-shelf’ satellite image composites can overcome common challenges in classification workflows, such as the ‘curse of dimensionality’ and computational demand. It also introduces readers and users to Digital Earth Africa products and services, and modes of access, especially through the SITS R package.",
    "crumbs": [
      "Use Cases in Crop Type Mapping",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Crop classification using Digital Earth Africa</span>"
    ]
  },
  {
    "objectID": "cy_finland.html",
    "href": "cy_finland.html",
    "title": "19  Early-season crop yield mapping in Finland",
    "section": "",
    "text": "19.1\n```",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Early-season crop yield mapping in Finland</span>"
    ]
  },
  {
    "objectID": "cy_indonesia.html",
    "href": "cy_indonesia.html",
    "title": "\n20  Mapping crop phenology in Indonesia\n",
    "section": "",
    "text": "20.1 Background\nPaddy data plays a crucial role in shaping national food security policies in Indonesia. The National Long-Term Development Plan 2025–2045 prioritizes the modernization of agricultural data collection, including paddy production [1]. Currently, Statistics Indonesia (BPS) relies on the Area Sampling Frame (ASF) for estimating harvested area and the Crop Cutting Survey for measuring productivity, with final production figures derived from these two sources. However, ASF’s monthly ground-truthing is costly and faces challenges such as remote area access and high surveyor workload.\nSatellite imagery presents a more efficient alternative for estimating crop phenology without ground verification, allowing for accurate and cost-effective harvest area estimation. Machine learning models can classify growth stages using satellite data, and Sentinel-1, a radar-based satellite, is particularly effective for agricultural monitoring in cloud-prone regions like Indonesia [2]. Furthermore, Satellite Imagery Time Series (SITS) data enables deeper pattern recognition and temporal analysis compared to traditional cross-sectional approaches [3].",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Mapping crop phenology in Indonesia</span>"
    ]
  },
  {
    "objectID": "cy_indonesia.html#data",
    "href": "cy_indonesia.html#data",
    "title": "\n20  Mapping crop phenology in Indonesia\n",
    "section": "\n20.2 Data",
    "text": "20.2 Data\nIn this research, Statistics Indonesia-BPS collaborate with the National Research and Innovation Agencies-BRIN for acquiring the Sentinel 1. BRIN give the access of Analysis Ready Data (ARD) data of the Sentinel 1 in 12 days intervals and acquired since 2023 until 2024. The data is further composed into time series data and filtered to the aligned ASF sampling plot, as shown in Figure 20.1.\n\n\n\n\n\n\n\nFigure 20.1: Integration of SITS (Sentinel 1) and the ASF plot.\n\n\n\n\nThe ASF sampling plots is a segment with size 300 x 300 meter and divided into 9 subsegments, each has size 100x100 meter. Each sampling plots is monitorized on last week in everymonth by surveyor. They collect the in-situ paddy phenological phase in centroid of the grid. Figure 20.2 represents the paddy phase monitorized monthly.\n\n\n\n\n\n\n\nFigure 20.2: Paddy Phenological Phase.\n\n\n\n\nHowever, in this study, those classes are modified into several classes detailing the sub-paddy phase compared to the original ASF classes. The following Table 20.1 present the differences of the classes used in the study.\n\n\nTable 20.1: Mapping of ASF Labels and Study Labels\n\n\n\n\n\n\n\n\n\nASF Label\nDescription\nStudy Label\nDescription\n\n\n\n1\nFirst Vegetative\n1\nFirst Vegetative\n\n\n2\nLate Vegetative\n2\nLate Vegetative\n\n\n3\nGeneratif\n3\nGeneratif\n\n\n4\nHarvest\n4\nHarvest\n\n\n\n\n5\nBera, which indicated by consecutive harvest\n\n\n5\nLand Preparation/Tillage\n6\nFallow Land, which indicated by the consecutive tillage\n\n\n\n\n0\nLand Preparation/Tillage\n\n\n6\nNon-Paddy\n7\nNon-Paddy\n\n\n7\nNon-Vegetation\n8\nNon-Vegetation\n\n\n\n\n\n\nIn addition to the Sentinel 1 data, several data are also leveraged for enhancing the quality, such as the elevation for capturing the different pattern of plantation in different elevation level. In details, Table 20.2 present all of the data used in modelling.\n\n\nTable 20.2: Details of variable\n\n\n\nVariable\nDescription\n\n\n\nBulan\nMonth of observation (1–12)\n\n\nelevation\nElevation of the observation point (meters)\n\n\nVH_0\nVH band (Sentinel-1) at time t (current biweekly)\n\n\nVH_1\nVH band at t-1 (previous biweekly)\n\n\nVH_2\nVH band at t-2 (second previous biweekly)\n\n\nVH_3\nVH band at t-3 (third previous biweekly)\n\n\nVH_4\nVH band at t-4 (fourth previous biweekly)\n\n\nVH_5\nVH band at t-5 (fifth previous biweekly)\n\n\nVH_6\nVH band at t-6 (sixth previous biweekly)\n\n\nVH_7\nVH band at t-7 (seventh previous biweekly)\n\n\nVH_8\nVH band at t-8 (eighth previous biweekly)\n\n\nVH_9\nVH band at t-9 (ninth previous biweekly)\n\n\nVH_10\nVH band at t-10 (tenth previous biweekly)\n\n\nVV_0\nVV band (Sentinel-1) at time t (current biweekly)\n\n\nVV_1\nVV band at t-1 (previous biweekly)\n\n\nVV_2\nVV band at t-2 (second previous biweekly)\n\n\nVV_3\nVV band at t-3 (third previous biweekly)\n\n\nVV_4\nVV band at t-4 (fourth previous biweekly)\n\n\nVV_5\nVV band at t-5 (fifth previous biweekly)\n\n\nVV_6\nVV band at t-6 (sixth previous biweekly)\n\n\nVV_7\nVV band at t-7 (seventh previous biweekly)\n\n\nVV_8\nVV band at t-8 (eighth previous biweekly)\n\n\nVV_9\nVV band at t-9 (ninth previous biweekly)\n\n\nVV_10\nVV band at t-10 (tenth previous biweekly)\n\n\nidkab\nRegency identifier code",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Mapping crop phenology in Indonesia</span>"
    ]
  },
  {
    "objectID": "cy_indonesia.html#results",
    "href": "cy_indonesia.html#results",
    "title": "\n20  Mapping crop phenology in Indonesia\n",
    "section": "\n20.3 Results",
    "text": "20.3 Results\nTo produce the model for delivering the paddy phenological classification, several procedures are conducted sequentially as depicted in Figure 20.3.\n\n\n\n\n\n\n\nFigure 20.3: Processing Workflow\n\n\n\n\nHere, the steps and corresponding results are explained on following subsection.\n\n20.3.1 Preprocessing\nNoise Filtering\nAs we know, the Satellite Image Time Series (SITS) data is inherently messy, often affected by noise, atmospheric disturbances, sensor inconsistencies, etc. Those aspect contributes to the mislabeled or ambiguous observations. If not carefully filtered, these issues can reduce the performance and reliability of machine learning models. One effective approach to address this challenge is to use Self-Organizing Maps (SOM) as a filtering procedure.\nSOM can organize complex, high-dimensional time series data into coherent clusters by identifying consistent temporal patterns while exposing anomalies and noisy samples. This enables the construction of a cleaner, higher-quality training dataset. SOM [4] is an unsupervised neural network model that projects multi-dimensional data onto a lower-dimensional (typically two-dimensional) grid, preserving topological relationships and facilitating pattern discovery.\n\nfrom SOMPipeline import SOMPipeline\n\nsom_pipeline = SOMPipeline(som_x=100, som_y=100, sigma=5.0, learning_rate=0.5)\n\nsom_pipeline.load_data(pulau='SUMATERA', kdprov='16', base_path='/data/raw')\nsom_pipeline.train()\n\nIn this study, the SOM pipeline (SOMPipeline) is implemented with several configuration, as follows: - grid of 100 x 100 neurons, - neighborhood function width (sigma) of 5.0, and - learning rate of 0.5.\nAfter training the SOM, the SOM model achieved a small quantization error of 1.35, indicating good fidelity in mapping the high-dimensional SITS data into the SOM grid, with lower quantization error reflecting better representation of the original input patterns.\n\nsom_pipeline.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.4: SOM results (above: BMU Class distribution, bottom: Reverse cumulative affected rows for each dominant neuron class).\n\n\nThe Best Matching Unit (BMU) plot is a diagnostic tool for assessing the quality and structure of Self-Organizing Map (SOM) training. Each point represents a neuron, where the circle size indicates how frequently it was selected as the best match, and the color shows its dominant class. Larger circles suggest neurons that represent more input samples, while smaller ones may indicate less representative or noisy data. As depicted, many neurons likely correspond to noise and should be filtered out to produce cleaner training data.\nTo filter the data, we select neurons with the most homogeneous class composition—indicated by a high dominance of a single class within the neuron. The cumulative density graph helps determine how many data points are affected at each purity level. In this case, we set a 50% threshold for class dominance and remove neurons with a lower value. From the remaining neurons, we retain only the data points that belong to the dominant class of each neuron.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_image_compare(df_all, df_filtered):\n    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n\n    # First plot — All Data\n    heatmap_data_all = df_all.pivot_table(index='idkab', columns='recode', aggfunc='size', fill_value=0)\n    sns.heatmap(heatmap_data_all, annot=True, fmt='d', cmap='viridis', cbar_kws={'label': 'Count'}, ax=axs[0])\n    axs[0].set_title('All Data: Heatmap of Count per idkab')\n    axs[0].set_xlabel('Classes')\n    axs[0].set_ylabel('idkab')\n\n    # Second plot — Filtered Data\n    heatmap_data_filtered = df_filtered.pivot_table(index='idkab', columns='recode', aggfunc='size', fill_value=0)\n    sns.heatmap(heatmap_data_filtered, annot=True, fmt='d', cmap='viridis', cbar_kws={'label': 'Count'}, ax=axs[1])\n    axs[1].set_title('Filtered Data (after SOM): Heatmap of Count per idkab')\n    axs[1].set_xlabel('Classes')\n    axs[1].set_ylabel('idkab')\n\n    plt.tight_layout()\n    plt.show()\n    \ndf_all, df_filtered = som_pipeline.export(dominant_threshold=50)\ndf_filtered.to_csv('data_for_modelling.csv',index=False)\n\nplot_image_compare(df_all, df_filtered)\n\n\n\n\n\n\n\n\nFigure 20.5: Comparison of data count (left: before SOM is performed, right: after SOM is performed)\n\n\n\n\nThe Figure 20.5 compares the data before (left) and after (right) SOM filtering. It shows that the filtering process reduces the dataset, likely by removing noisy data. Class 6 appears to be the most affected, with only around 102 rows remaining after the SOM filtering.\nResampling Approach\nTo address class imbalance in Satellite Image Time Series (SITS), we employed an adaptive resampling strategy based on class size.\nFor minority classes (&lt;1000 samples), we applied Borderline-SMOTE to generate synthetic samples near class boundaries, improving model sensitivity in ambiguous regions. This was repeated until each class reached 1000 samples, ensuring balanced representation. Borderline-SMOTE was chosen over standard SMOTE for its effectiveness in handling class overlap common in SITS-based remote sensing data.\nFor overrepresented classes, Neighbourhood Cleaning Rule (NCL) was applied to remove noisy or ambiguous majority instances, reducing bias and sharpening class boundaries. NCL addresses issues like mislabeled or inconsistent samples caused by mixed pixels, clouds, or seasonal changes in SITS.\nCombined with Borderline-SMOTE, this adaptive resampling creates a balanced, cleaner training set, improving model generalization and robustness in imbalanced, high-dimensional remote sensing data.\n\ndef plot_resampling_comparison(y_train_original, y_resampled):\n    # Force both to categorical with same order\n    classes = sorted(set(y_train_original) | set(y_resampled))\n\n    y_train_original_cat = pd.Categorical(y_train_original, categories=classes, ordered=True)\n    y_resampled_cat = pd.Categorical(y_resampled, categories=classes, ordered=True)\n\n    # Use your palette\n    palette = sns.color_palette(\"Set2\")\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n    # Original y_train\n    sns.countplot(x=y_train_original_cat, ax=axs[0], palette=palette)\n    axs[0].set_title(\"Before Resampling\")\n    axs[0].set_xlabel(\"Class\")\n    axs[0].set_ylabel(\"Count\")\n\n    # Resampled y_train\n    sns.countplot(x=y_resampled_cat, ax=axs[1], palette=palette)\n    axs[1].set_title(\"After Resampling\")\n    axs[1].set_xlabel(\"Class\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nimport numpy as np\nimport pandas as pd\nfrom AdaptiveResampler import AdaptiveResampler\nfrom XGBMultiClassPipeline import XGBMultiClassPipeline\n\ndf_filtered = pd.read_csv('data_for_modelling.csv')\n\ny = df_filtered['recode']\nX = df_filtered[['Bulan', 'elevation', 'VH_10', 'VH_9', 'VH_8',\n       'VH_7', 'VH_6', 'VH_5', 'VH_4', 'VH_3', 'VH_2', 'VH_1', 'VH_0', 'VV_10',\n       'VV_9', 'VV_8', 'VV_7', 'VV_6', 'VV_5', 'VV_4', 'VV_3', 'VV_2', 'VV_1',\n       'VV_0', 'idkab']]\n\n# Define pipeline\npipeline = XGBMultiClassPipeline(num_class=len(np.unique(y)))\n\n# Split data in train / test / validation\nX_train, X_val, y_train, y_val = pipeline.get_train_validation_split(X, y, test_size=0.3)\n\n# Define resampler\nresampler = AdaptiveResampler(minority_thresh=1000, majority_thresh=5000)\n\n# Fit resample\nX_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n\n# Plot results\nplot_resampling_comparison(y_train, y_resampled)\n\n\n\n\n\n\n\n\nFigure 20.6: Comparison of data count (left: before Adaptive Resampling is performed, right: after Adaptive Resampling is performed)\n\n\n\n\nThe Figure 20.6 compares class distributions before and after applying the adaptive resampling strategy. In the original (Before Resampling) distribution, substantial class imbalance is evident—with classes 5 and 7 heavily overrepresented, and classes 0, 6, and 8 severely underrepresented. After resampling, minority classes (especially class 0 and class 8) show significant increases, while class 6 is slightly improved, reflecting the impact of Borderline-SMOTE for classes with fewer than 1000 samples. At the same time, overrepresented classes (5 and 7) are reduced but not fully equalized, indicating that Neighbourhood Cleaning Rule (NCL) has effectively filtered out noisy or borderline instances without distorting the overall class structure.\n\n20.3.2 Model Development\nAfter the preprocessing steps are conducted, the next step is produce the model for classification. In this study, XGBoost is selected as the classification algorithm. XGBoost (Extreme Gradient Boosting) is a scalable, high-performance gradient boosting framework based on decision trees, designed to optimize both speed and predictive accuracy [5]. It builds an ensemble of trees in a sequential manner, where each new tree corrects the errors of the previous ones, and leverages advanced regularization techniques (L1 and L2), shrinkage, and sparsity-aware algorithms to prevent overfitting. Compared to traditional methods such as Random Forest, which builds multiple independent trees in parallel, XGBoost typically achieves superior performance with fewer trees and lower computational cost, especially on high-dimensional, noisy, or imbalanced data.\nHyperparameter Tuning\nTo produce reliable, accurate, and robust model, selecting the hyperparameter is crucial steps. Here, the hyperparameter tuning is performed using Optuna [6], which optimizes key XGBoost parameters (e.g., max_depth, learning_rate, reg_lambda, min_child_weight, and gamma—by sampling) from predefined ranges via the TPE (Tree-structured Parzen Estimator) algorithm [7]. Each trial is evaluated using Stratified K-Fold cross-validation and macro F1-score to ensure bala)nced class performance [8]. Early stopping is applied during model training and across trials (with a 10-trial patience) to prevent overfitting and improve efficiency.\n\nimport optuna\nstudy = optuna.create_study(direction='maximize')\n\n# Attach early stopping callback\nearly_stop = pipeline.EarlyStoppingCallback(patience = 10)\n\n# Run optimization\nstudy.optimize(\n    lambda trial: pipeline.objective(trial, X_resampled, y_resampled),\n    n_trials=50,\n    callbacks=[early_stop]\n)\n\nBased on the trials, the best hyperparameters are shown in Table 20.3:\n\n\nTable 20.3: Table 3. Best Parameter\n\n\n\n\n\n\n\n\nHyperparameter\nValue\nDescription / Effect\n\n\n\nmax_depth\n9\nControls the maximum depth of trees; deeper trees can capture more complex patterns but risk overfitting.\n\n\nlearning_rate\n0.0752\nShrinks the contribution of each tree; lower values require more trees but improve generalization.\n\n\nreg_lambda\n3.472\nL2 regularization term on weights; higher values increase regularization to prevent overfitting.\n\n\nmin_child_weight\n72\nMinimum sum of instance weight (hessian) needed in a child; higher values make the model more conservative.\n\n\ngamma\n0.1028\nMinimum loss reduction required to make a further partition; larger values make the tree pruning more conservative.\n\n\n\n\n\n\n\nimport pickle\nwith open(\"best_params_fin.pkl\", \"wb\") as f:\n    pickle.dump(study.best_params, f)\n\nModelling\nAfter the hyperparameter tuned, the next part is train the model, evaluating the results. The model is trained using the best hyperparameters with early stopping to prevent overfitting and ensure efficient learning. It uses multi-class classification and is optimized to handle class imbalances in the data. The training process evaluates performance at each step and stops automatically when improvements plateau. After training, the model is assessed on the test set using a several validation index. Additionally, to refine predictions, optimal thresholds are applied based on precision-recall performance. Final evaluation includes accuracy, F1-scores, and AUC metrics, offering a balanced view of model performance across all classes..\n\nimport pickle\nwith open(\"best_params_fin.pkl\", \"rb\") as f:\n    best_params = pickle.load(f)\n\npipeline.train(X_resampled, y_resampled, best_params=best_params)\n\nresults = pipeline.evaluate(X_val, y_val, output_file='confusion_matrix')\n\n\n\n\n\n\n\n\nFigure 20.7: Confusion matrix on validation data\n\n\n\n\n\nresults['metrics']\n\n{'val_accuracy': 0.8247489471979268,\n 'val_f1_macro': np.float64(0.7492888552439827),\n 'val_f1_micro': np.float64(0.8247489471979268),\n 'val_auc': np.float64(0.9695775603130876),\n 'val_pr_auc': np.float64(0.8048938208240811)}\nThe model achieved strong overall performance on the validation set, with an accuracy of 82.4% and a matching micro F1-score of 82.4%, indicating high correctness across all classes. The macro F1-score of 74.9% further demonstrates balanced performance between majority and minority classes, suggesting that class imbalance was effectively addressed. The high AUC of 0.9695 reflects excellent ability to distinguish between classes, while the PR-AUC of 0.8048 indicates strong precision-recall trade-offs.\nThe best-performing classes include “Non-Paddy” (class 7) and “Non-Vegetation” (class 8), both with precision and recall exceeding 0.85, demonstrating clear spectral and temporal separability in the Satellite Image Time Series (SITS) data. Additionally, “Harvest” (class 4) and “Bera” (class 5) phases exhibit strong classification performance, likely due to their distinctive phenological signatures. In contrast, the model shows relatively lower recall for “Land Preparation/Tillage” (class 0, recall 0.58) and “Fallow Land” (class 6, recall 0.43), with common misclassifications occurring toward vegetative stages. This is consistent with known challenges in SITS-based crop monitoring, where transitional phases exhibit spectral ambiguity and temporal overlap (e.g., tillage may coincide with early vegetation stages). The confusion matrix further shows that most misclassifications occur between adjacent or phenologically related classes (e.g., “First Vegetative” to “Late Vegetative”), which may be acceptable in practical applications where approximate phase tracking is sufficient. Overall, these results confirm that the combination of adaptive resampling and tuned XGBoost modeling provides a robust framework for multi-class land cover classification from SITS data, with strong performance on dominant classes and reasonable accuracy across minority and transitional phases.\n\nwith open(\"best_pipeline_fin.pkl\", \"wb\") as f:\n    pickle.dump(results, f)\n\n\n20.3.3 C. Bayes Post-Classification\nGiven the natural characteristics of phenologycal phase, sequential rule is emeged to ensure coherency. For instance, the Late Vegetative phase can not be emerged unless the previous month observation is First Vegetative. This implies that the Current Month Phase is influeced by Previous Month Phase. Based on this idea, a Bayes Post-Classification can be formulated for enhancing the quality of modelling with following procedure:\n\\[\n\\begin{aligned}\nP(\\text{Current Class} \\mid \\text{XGB Proba}, \\text{Previous Class})\n&\\propto P(\\text{XGB Proba} \\mid \\text{Current Class}) \\\\\n&\\times P(\\text{Current Class} \\mid \\text{Previous Class})\n\\end{aligned}\n\\]\nLikelihood:\\(P(\\text{XGB Proba} \\mid \\text{Current Class})\\)\nThis represents the probability of observing the XGBoost predicted probabilities given the current class. It is derived directly from the XGBoost classifier’s output (predict_proba).\nPrior:\\(P(\\text{Current Class} \\mid \\text{Previous Class})\\)\nThis is the probability of the current class conditioned on the previous class, estimated from historical data (e.g., frequencies computed over the last 3 years).\nPosterior:\\(P(\\text{Current Class} \\mid \\text{XGB Proba}, \\text{Previous Class})\\)\nThis is the updated belief about the current class combining the likelihood (model evidence) and the prior (historical transitions).\nIn other words,\\[\n  \\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n  \\]\nSummary:\n- The likelihood comes from the XGBoost model’s predicted probabilities.\n- The prior captures temporal dependencies from historical class transitions.\n- The posterior combines both to provide an improved class prediction.\n\ndef add_prev_recode(df, id_col='idsubsegmen', time_col_month='Bulan', time_col_year='Tahun', target_col='recode'):\n    df = df.copy()\n    df.sort_values(by=[id_col, time_col_year, time_col_month], inplace=True)\n\n    # Shift recode per group (idsubsegmen)\n    df['prev_recode'] = (\n        df.groupby(id_col)[target_col]\n        .shift(1)\n    )\n\n    # Shift also Tahun and Bulan per group to compare whether prev record is in previous month-year\n    df['prev_year'] = df.groupby(id_col)[time_col_year].shift(1)\n    df['prev_month'] = df.groupby(id_col)[time_col_month].shift(1)\n\n    # Compute expected previous month and year\n    df['expected_prev_month'] = df[time_col_month] - 1\n    df['expected_prev_year'] = df[time_col_year]\n\n    # If current month is January → previous month should be December of previous year\n    jan_mask = df['expected_prev_month'] == 0\n    df.loc[jan_mask, 'expected_prev_month'] = 12\n    df.loc[jan_mask, 'expected_prev_year'] -= 1\n\n    # Keep only valid previous recode (when prev month and year match expected)\n    valid_prev = (df['prev_month'] == df['expected_prev_month']) & (df['prev_year'] == df['expected_prev_year'])\n    df.loc[~valid_prev, 'prev_recode'] = np.nan\n\n    # Optional: clean temp columns\n    df.drop(columns=['prev_year', 'prev_month', 'expected_prev_month', 'expected_prev_year'], inplace=True)\n\n    return df\n\n\nimport xgboost as xgb\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay, classification_report)\n\nfull_data = pd.read_csv(f'/data/ksa/00_Code_2025/data_16_WO_cluster.csv', delimiter=';')\nfull_data = add_prev_recode(full_data)\ndf_validation = X_val.copy()\n\ndf_validation['Recode'] = y_val\ndf_validation = df_validation.merge(full_data)\n\nwith open(\"results_conditional_probs.pkl\", \"rb\") as f:\n    conditional_probs = pickle.load(f)\n\ndf_recode_given_prev=conditional_probs['16']['prob']['P_cond_prev_transaction']\ndf_recode_given_prev_long = (\n    df_recode_given_prev\n    .reset_index()  # makes idkab and prev_recode columns instead of index\n    .melt(id_vars=[\"idkab\", \"prev_recode\"], \n          var_name=\"recode\", \n          value_name=\"P_given_prev\")\n)\n\ndf_recode_given_prev_long['idkab'] = df_recode_given_prev_long['idkab'].astype(\"int\")\n\ndfm_predict = xgb.DMatrix(df_validation[X_val.columns], label=df_validation[\"recode\"], enable_categorical=True)\ndfm_predict_proba = pipeline.model.predict(dfm_predict)\n\ndf_validation[[f'Class_{i}' for i in range(0,9)]] = dfm_predict_proba\ndf_validation['recode_pred'] = dfm_predict_proba.argmax(axis=1)\n\n\nfrom BayesPostClassificationPipeline import BayesPostClassificationPipeline\n\n# Step 1: Create pipeline\nbpost_pipeline = BayesPostClassificationPipeline(\n    df_prior = df_recode_given_prev_long,\n    id_col = 'idkab',\n    prev_recode_col = 'prev_recode',\n    recode_col = 'recode',\n    prior_col = 'P_given_prev',\n    num_classes = 9\n)\n\n# Step 2: Apply the prior/posterior logic\ndf_val_with_prior_posterior = bpost_pipeline.incorporate_prior_and_posterior(df_validation)\n\n# Step 3: Evaluate the result\ndf_val_with_prior_posterior = bpost_pipeline.evaluate(df_val_with_prior_posterior)\n\n\n\n\n\n\n\n\nFigure 20.8: Confusion matrix on validation data after Bayes Classification performed\n\n\n\n\nThe application of Bayes PostClassification significantly improved both overall model performance and class-level consistency. Before post-processing, the classifier achieved an accuracy of 82% and a macro F1-score of 74.9%. After refinement, the accuracy increased to 90% and the macro F1-score rose to 83.0%, indicating a substantial reduction in misclassifications—particularly in ambiguous or transitional growth phases.\nNotable improvements were observed across several classes. For Class 0 (Land Preparation/Tillage), recall increased from 0.58 to 0.70. For Classes 1 (First Vegetative), 2 (Late Vegetative), and 3 (Generatif), accuracy improved from below 80% to 83%, suggesting that the Bayes filter effectively reduced over-smoothing between neighboring growth stages. The most significant gain was seen in Class 4 (Harvest), where recall jumped from 0.82 to 0.93 and precision improved from 0.79 to 0.90. This greatly reduced false positives being assigned to similar phases such as “Bera” or “Late Vegetative”.\nOverall, the Bayes-based post-processing acted as a temporal prior filter, correcting model uncertainty and aligning predictions more closely with realistic crop growth transitions—an essential enhancement for fine-grained agricultural monitoring.",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Mapping crop phenology in Indonesia</span>"
    ]
  },
  {
    "objectID": "cy_indonesia.html#conclusions",
    "href": "cy_indonesia.html#conclusions",
    "title": "\n20  Mapping crop phenology in Indonesia\n",
    "section": "\n20.4 Conclusions",
    "text": "20.4 Conclusions\nIn this study, we presented a complete workflow for analyzing paddy phenological phases using satellite imagery and machine learning. The process is structured into three main stages: preprocessing, modeling, and refinement. The preprocessing phase includes noise filtering using Self-Organizing Maps (SOM) and adaptive resampling. The modeling phase involves hyperparameter tuning, model development, and performance evaluation. Finally, the refinement phase applies Bayesian post-processing to improve classification accuracy.\nThe results demonstrate that a modernized approach to monitoring paddy phenology—integrating remote sensing and machine learning—can be effectively implemented to enhance accuracy and reliability in capturing crop growth dynamics.",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Mapping crop phenology in Indonesia</span>"
    ]
  },
  {
    "objectID": "cy_indonesia.html#references",
    "href": "cy_indonesia.html#references",
    "title": "\n20  Mapping crop phenology in Indonesia\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nU. Faoziyah, M. F. Rosyaridho, and R. Panggabean, “Unearthing agricultural land use dynamics in indonesia: Between food security and policy interventions,” Land, vol. 13, no. 12, p. 2030, 2024, doi: 10.3390/land13122030.\n\n\n[2] \nH. Muradi et al., “Rice phenology classification model based on sentinel-1 using machine learning method on google earth engine,” Canadian Journal of Remote Sensing, vol. 50, no. 1, 2024, doi: 10.1080/07038992.2024.2293843.\n\n\n[3] \nR. Simoes et al., “Satellite image time series analysis for big earth observation data,” Remote Sensing, vol. 13, no. 13, p. 2428, 2021, doi: 10.3390/rs13132428.\n\n\n[4] \nT. Kohonen, “Essentials of the self-organizing map,” Neural Networks, vol. 37, pp. 52–65, 2013, doi: 10.1016/j.neunet.2012.09.018.\n\n\n[5] \nT. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–794, doi: 10.1145/2939672.2939785.\n\n\n[6] \nT. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A next-generation hyperparameter optimization framework,” Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2623–2631, 2019.\n\n\n[7] \nX. Bouthillier, P. Vincent, and A. Drouin, “Accounting for variance in machine learning benchmarks,” Journal of Machine Learning Research, vol. 22, pp. 1–62, 2021.\n\n\n[8] \nA. Fernández, S. García, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera, “Learning from imbalanced data sets,” Springer, 2018.",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Mapping crop phenology in Indonesia</span>"
    ]
  },
  {
    "objectID": "cy_poland.html",
    "href": "cy_poland.html",
    "title": "21  Yield Forecasting in Poland",
    "section": "",
    "text": "21.1 Overview\nIntegration of Sentinel-3 and MODIS Vegetation Indices with ERA-5 Agro-Meteorological Indicators for Operational Crop Yield Forecasting.",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Yield Forecasting in Poland</span>"
    ]
  },
  {
    "objectID": "cy_colombia.html",
    "href": "cy_colombia.html",
    "title": "22  Rice Phenology in Colombia",
    "section": "",
    "text": "22.1",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Rice Phenology in Colombia</span>"
    ]
  },
  {
    "objectID": "cy_china.html",
    "href": "cy_china.html",
    "title": "23  Crop type classification and crop yield estimation in China",
    "section": "",
    "text": "23.1",
    "crumbs": [
      "Crop yield estimation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Crop type classification and crop yield estimation in China</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html",
    "href": "ad_geoglam.html",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "",
    "text": "24.1 Introduction\nEarth Observation (EO) has become indispensable for generating timely, spatially explicit data on agricultural production. Yet, the final and most consequential step — turning maps into statistics — remains riddled with confusion and poor practices. In particular, there is a widespread tendency to assume that once a crop type or yield map is generated, one can simply count the number of pixels in a given class and multiply by pixel size. This shortcut, though common, is statistically unsound.\nPixel counting assumes that the map is a perfect representation of reality — an assumption rarely, if ever, valid. Classification errors, mixed pixels, spatial autocorrelation, and resolution mismatches all undermine the reliability of raw pixel aggregates. As [1] emphasized, failure to adjust for classification error can lead to area estimates that are biased by 15% or more. Worse, these biases are often invisible to the user, as uncertainty is not quantified or reported. These concerns have been echoed in broader remote sensing literature, which warns against the widespread neglect of error propagation in map-based area statistics [2].\nFor National Statistical Offices (NSOs), whose mandate is to produce statistically defensible, reproducible, and uncertainty-aware agricultural statistics, such practices are unacceptable. EO maps can and should be used — but their outputs must be interpreted and corrected within a proper statistical framework. This chapter introduces three leading paradigms that have emerged as robust and operationally viable approaches for extracting agricultural statistics from EO-derived maps. Each paradigm corresponds to a real-world use case developed by leading institutions and collaborators in this Handbook.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#map-corrected-estimation",
    "href": "ad_geoglam.html#map-corrected-estimation",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.2 Map-Corrected Estimation",
    "text": "24.2 Map-Corrected Estimation\nAnchored by: NASA Harvest, Inbal Becker-Reshef and collaborators\nThis paradigm treats the EO-derived map as a primary product, but one that must be statistically adjusted using validation samples and error matrices. The approach, grounded in the methodology of [1], involves computing bias-adjusted area estimates by integrating confusion matrix values with map pixel counts and deriving confidence intervals based on sampling theory. This is consistent with operational good practice for statistically valid area estimation from remote sensing, as reviewed in [2].\nIt is particularly effective when the map covers large areas, classification accuracy is high-to-moderate, and a well-distributed reference dataset is available. This paradigm has been implemented operationally in conflict settings (e.g. Ukraine), where it enabled rapid area estimation despite difficult field conditions. It emphasizes design-based rigor, quantified uncertainty, and transparency — essential features of any official estimate.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#survey-calibrated-mapping",
    "href": "ad_geoglam.html#survey-calibrated-mapping",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.3 Survey-Calibrated Mapping",
    "text": "24.3 Survey-Calibrated Mapping\nAnchored by: FAO EOSTAT Zimbabwe, Lorenzo De Simone & Sophie Bontemps\nThis paradigm places the statistical survey at the center, using EO maps as auxiliary information to enhance the representativeness, cost-efficiency, and resolution of national estimates. In the Zimbabwe case, a stratified ground survey (over 1,600 Secondary Sampling Units) was used not only for validating the crop type map, but also to build regression models that calibrated the mapped crop proportions. These regression estimators were then applied across the full gridded domain to derive corrected national crop area estimates. This model-assisted approach captures the best of both worlds: it leverages the spatial wall-to-wall nature of EO maps while preserving the statistical integrity of survey-based estimation. Importantly, it allowed FAO and ZIMSTAT to reduce bias and variance in estimates without increasing survey costs — a key feature for NSOs with resource constraints.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#uncertainty-aware-inference-from-imperfect-maps",
    "href": "ad_geoglam.html#uncertainty-aware-inference-from-imperfect-maps",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.4 Uncertainty-Aware Inference from Imperfect Maps",
    "text": "24.4 Uncertainty-Aware Inference from Imperfect Maps\nAnchored by: Sherrie Wang (MIT/Stanford) – Prediction-Powered Inference (PPI)\nThis emerging paradigm tackles the hardest scenario: using imperfect or low-accuracy maps to produce reliable estimates, even when the available ground data are scarce. Sherrie Wang and collaborators introduced a framework known as Prediction-Powered Inference (PPI), which explicitly accounts for the predictive (but uncertain) nature of maps. Instead of assuming that EO outputs are accurate, PPI quantifies their uncertainty and integrates it into the estimation procedure, allowing users to generate statistically valid area and regression estimates with far fewer labeled data points [4].\nIn benchmarking exercises, PPI was shown to dramatically reduce the confidence interval width compared to traditional methods, even when only a small ground-truth sample was available. This paradigm is particularly well-suited for data-scarce or rapidly changing contexts, where EO is the only timely source of information and statistical inference must be done with minimal fieldwork.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#why-these-paradigms-matter",
    "href": "ad_geoglam.html#why-these-paradigms-matter",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.5 Why These Paradigms Matter",
    "text": "24.5 Why These Paradigms Matter\nThese three paradigms represent a progression — from treating maps as authoritative, to integrating them with surveys, to recognizing and modeling their imperfections. Each offers a viable pathway, depending on the country context, data availability, institutional mandate, and risk tolerance. All three converge on a single truth: map outputs must not be taken at face value. Whether through confusion matrix correction, regression calibration, or model-based inference, the path from EO-derived maps to national statistics requires methodological rigor. This chapter provides not only the conceptual foundation, but also real-world demonstrations for each of these paradigms. By replacing pixel-counting shortcuts with robust estimation frameworks, we aim to elevate EO from a promising technology to a pillar of trusted agricultural statistics.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#choosing-the-right-paradigm-comparative-considerations",
    "href": "ad_geoglam.html#choosing-the-right-paradigm-comparative-considerations",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.6 Choosing the Right Paradigm: Comparative Considerations",
    "text": "24.6 Choosing the Right Paradigm: Comparative Considerations\nThe three paradigms presented in this chapter — Map-Corrected Estimation, Survey-Calibrated Mapping, and Uncertainty-Aware Inference from Imperfect Maps — are not mutually exclusive. Rather, they offer a flexible toolbox for NSOs and implementing partners to select the most appropriate approach based on external constraints and technical considerations. The decision to adopt one over another should be grounded in both institutional context and data characteristics.\n\nExternal Enabling Conditions\n\n\n\n\n\n\n\n\nCondition\nMap-Corrected Estimation\nSurvey-Calibrated Mapping\nUncertainty-Aware Inference\n\n\n\n\nExample\nNASA Harvest\nFAO EOSTAT Zimbabwe\nSherrie Wang/MIT PPI)\n\n\nSurvey Infrastructure\nModerate: needs statistically valid sample for validation\nHigh: requires well-structured, stratified survey\nMinimal: can operate with very small ground sample\n\n\nMap Availability\nRequired, high-quality map (e.g. &gt;70% OA)\nRequired, but can tolerate local noise\nRequired, but accuracy can be low or unknown\n\n\nInstitutional Capacity (NSO)\nMedium: needs basic familiarity with confusion matrices\nHigh: needs regression modeling and post-strata expansion\nMedium: needs ability to interpret model-based uncertainty\n\n\nUrgency / Time Sensitivity\nHigh: can deliver rapidly once validation set is in place\nModerate: dependent on survey completion\nHigh: useful in crisis or inaccessible areas\n\n\nBudget Constraints\nLow–moderate cost if map is already produced\nCost-effective when integrated into existing survey design\nLow cost; efficient use of sparse data\n\n\n\n\nIntrinsic Technical Criteria\n\n\n\n\n\n\n\n\nCriterion\nMap-Corrected Estimation\nSurvey-Calibrated Mapping\nUncertainty-Aware Inference\n\n\n\n\nMap Bias Handling\nYes — through confusion matrix weighting\nYes — via regression calibration\nYes — explicitly modeled in PPI\n\n\nVariance Reduction\nModerate — dependent on sample size\nHigh — leverages correlation to boost precision\nVery High — uses map + model jointly\n\n\nError Quantification (CI)\nExplicit — design-based estimation\nExplicit — from model residuals and design\nExplicit — PPI produces valid intervals\n\n\nAssumption Sensitivity\nLow — assumes random sampling\nMedium — assumes linear relationship\nLow–Medium — relies on predictive power but corrects for error\n\n\nScalability\nHigh — can be generalized nationally\nHigh — when survey design is in place\nHigh — especially suited for regional/rapid-scale deployment\n\n\n\nSynthesis: Context-Driven Method Selection\n\nIf the EO map is of high quality, and a good validation sample is available, Map-Corrected Estimation provides a quick, interpretable, and rigorous path — ideal for crop area estimation in well-resourced systems or fast-response settings (e.g. Ukraine, US).\nIf the NSO already operates a stratified agricultural survey, Survey-Calibrated Mapping allows integration of EO as auxiliary data, improving precision and lowering cost per estimate — as successfully demonstrated in Zimbabwe\nIf field data are scarce or the map is noisy, but there is predictive signal in the EO layers, Uncertainty-Aware Inference (PPI) offers a powerful way to still deliver valid statistics — with strong appeal for humanitarian contexts, fragile states, or smallholder-dominant systems.\n\nThese paradigms can also be combined. For example, a regression-calibrated estimator can be supplemented with design-based error correction, or PPI can be used to extend inference when only part of the territory has usable field data.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#bridging-science-statistics-and-policy-through-the-last-mile",
    "href": "ad_geoglam.html#bridging-science-statistics-and-policy-through-the-last-mile",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.7 Bridging Science, Statistics, and Policy through the Last Mile",
    "text": "24.7 Bridging Science, Statistics, and Policy through the Last Mile\nThe issues covered in this chapter occupy a central position in the operationalization of Earth Observation (EO) for agricultural statistics: they address the last mile—the transformation of EO-derived maps into statistically valid estimates that inform national decisions. While preceding steps in the EO workflow focus on building maps, it is in this final step that spatial data are converted into policy-relevant numbers. This is the critical juncture where the potential of EO is either realized—or lost.\nAchieving this transformation requires more than technical capacity; it demands a structured interface between scientific innovation, statistical production, and policy needs. National Statistical Offices (NSOs) need solutions that are not only accurate, but also statistically defensible, reproducible, and aligned with official protocols. At the same time, the scientific community benefits from grounded feedback to refine models and ensure operational applicability.\nA key enabler of this translation has been the coordinated effort of the United Nations Committee of Experts on Big Data and Data Science for Official Statistics, particularly through its Task Team on Earth Observations for Agricultural Statistics. The Task Team has played a catalytic role in consolidating good practices, fostering country-led experimentation, and promoting the institutional adoption of EO methods within national statistical systems. By convening a diverse set of stakeholders—including NSOs, space agencies, academic experts, and international partners—it has created the conditions for methodological innovation to evolve into operational capability.\nThis chapter embodies this convergence. It illustrates how co-designed approaches—developed through collaboration and grounded in real-world constraints—can elevate EO from a research tool to a pillar of official agricultural statistics. Crucially, it underscores that robust area and yield estimation is not a peripheral technicality, but the decisive link connecting EO technologies to food security planning, climate-smart agriculture, and policy accountability. Without this last mile, EO data remain abstract; with it, they become instruments of evidence-based policy.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_geoglam.html#references",
    "href": "ad_geoglam.html#references",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "References",
    "text": "References\n\n\n\n\n[1] P. Olofsson, G. M. Foody, M. Herold, S. V. Stehman, C. E. Woodcock, and M. A. Wulder, “Good practices for estimating area and assessing accuracy of land change,” Remote Sensing of Environment, vol. 148, pp. 42–57, 2014.\n\n\n[2] S. V. Stehman and G. M. Foody, “Key issues in rigorous accuracy assessment of land cover products,” Remote Sensing of Environment, vol. 231, p. 111199, 2019, doi: 10.1016/j.rse.2019.05.018.\n\n\n[3] D. M. Kluger, S. Wang, and D. B. Lobell, “Two shifts for crop mapping: Leveraging aggregate crop statistics to improve satellite-based maps in new regions,” Remote Sensing of Environment, vol. 262, p. 112488, 2021, doi: 10.1016/j.rse.2021.112488.\n\n\n[4] S. Wang, F. Waldner, and D. B. Lobell, “Unlocking large-scale crop field delineation in smallholder farming systems with transfer learning and weak supervision,” Remote Sensing, vol. 14, no. 22, 2022.",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Extraction of crop statistics from crop type and crop yield maps</span>"
    ]
  },
  {
    "objectID": "ad_world_cereal.html",
    "href": "ad_world_cereal.html",
    "title": "25  WorldCereal - A Global Effort for Crop Mapping",
    "section": "",
    "text": "25.1",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>WorldCereal - A Global Effort for Crop Mapping</span>"
    ]
  },
  {
    "objectID": "ad_uav_applications.html",
    "href": "ad_uav_applications.html",
    "title": "26  UAV use in Agricultural Statistics",
    "section": "",
    "text": "26.1",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>UAV use in Agricultural Statistics</span>"
    ]
  },
  {
    "objectID": "ad_disaster_response.html",
    "href": "ad_disaster_response.html",
    "title": "27  Remote Sensing for Agricultural Disaster Response",
    "section": "",
    "text": "27.1",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Remote Sensing for Agricultural Disaster Response</span>"
    ]
  },
  {
    "objectID": "ad_governance.html",
    "href": "ad_governance.html",
    "title": "28  Data Governance for Agricultural Statistics",
    "section": "",
    "text": "28.1 Introduction",
    "crumbs": [
      "Additional Topics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Data Governance for Agricultural Statistics</span>"
    ]
  },
  {
    "objectID": "th_machine_learning.html#data-used-in-this-chapter",
    "href": "th_machine_learning.html#data-used-in-this-chapter",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.1 Data used in this chapter",
    "text": "7.1 Data used in this chapter\nThe following examples show how to train machine learning methods and apply them to classify a single time series. We use the set samples_matogrosso_mod13q1, containing time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (Cerrado, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet). Each time series covers 12 months (23 data points) with six bands (NDVI, EVI, BLUE, RED, NIR, MIR). The samples are arranged along an agricultural year, starting in September and ending in August. The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” [1], and is available in the R package sitsdata."
  },
  {
    "objectID": "th_machine_learning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "href": "th_machine_learning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.2 Common interface to machine learning and deep learning models",
    "text": "7.2 Common interface to machine learning and deep learning models\nThe sits_train() function provides a standard interface to all machine learning models. This function takes two mandatory parameters: the training data (samples) and the ML algorithm (ml_method). After the model is estimated, it can classify individual time series or data cubes with sits_classify(). In what follows, we show how to apply each method to classify a single time series. Then, in Chapter Image classification in data cubes, we discuss how to classify data cubes.\nSince sits is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this Chapter.\nWhen a set of time series organized as tibble is taken as input to the classifier, the result is the same tibble with one additional column (predicted), which contains the information on the labels assigned for each interval. The results can be shown in text format using the function sits_show_prediction() or graphically using plot()."
  },
  {
    "objectID": "th_machine_learning.html#random-forest",
    "href": "th_machine_learning.html#random-forest",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.3 Random Forest",
    "text": "7.3 Random Forest\nRandom Forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, each trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data. The final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be helpful in feature selection and data visualization. For an in-depth discussion of the robustness of Random Forest for satellite image time series classification, please see Pelletier et al [2].\n\n\n\n\nFigure 7.1: Random forest algorithm (source: Venkata Jagannath in Wikipedia).\n\n\n\nsits provides sits_rfor(), which uses the R randomForest package [3]; its main parameter is num_trees, which is the number of trees to grow with a default value of 100. The model can be visualized using plot()."
  },
  {
    "objectID": "th_machine_learning.html#r",
    "href": "th_machine_learning.html#r",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.4 R",
    "text": "7.4 R\n\nset.seed(290356)\n\n# Train the Mato Grosso samples with Random Forest algorithm\nrfor_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_rfor(num_trees = 100))\n\n# Plot the most important variables of the model\nplot(rfor_model)\n\n\n\n\n\nFigure 7.2: Most important variables in Random Forest model.\n\n\n\nThe model plot shows the most important explanatory variables, which are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band on date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes. Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural and natural classes. This corresponds to summertime when the ground is drier after harvesting crops."
  },
  {
    "objectID": "th_machine_learning.html#r-1",
    "href": "th_machine_learning.html#r-1",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.5 R",
    "text": "7.5 R\n\n# Classify using Random Forest model and plot the result\npoint_class &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model  = rfor_model)\n\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.3: Classification of time series using Random Forest.\n\n\n\nThe result shows that the area started as a forest in 2000, was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. This behavior is consistent with expert evaluation of land change process in this region of Amazonia.\nRandom Forest is robust to outliers and can deal with irrelevant inputs [4]. The method tends to overemphasize some variables because its performance tends to stabilize after part of the trees is grown [4]. In cases where abrupt change occurs, such as deforestation mapping, Random Forest (if properly trained) will emphasize the temporal instances and bands that capture such quick change. Before using Random Forest, it is recommended that users balance their training samples as explained in “Reducing imbalances in training samples”"
  },
  {
    "objectID": "th_machine_learning.html#extreme-gradient-boosting",
    "href": "th_machine_learning.html#extreme-gradient-boosting",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.6 Extreme gradient boosting",
    "text": "7.6 Extreme gradient boosting\nXGBoost (eXtreme Gradient Boosting) [5] is an implementation of gradient boosted decision trees designed for speed and performance. It is an ensemble learning method, meaning it combines the predictions from multiple models to produce a final prediction. XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. Each tree builds a new model to correct the errors made by previous models. Using gradient descent, the algorithm iteratively adjusts the predictions of each tree by focusing on instances where previous trees made errors. Models are added sequentially until no further improvements can be made.\nAlthough Random Forest and boosting use trees for classification, there are significant differences. While Random Forest builds multiple decision trees in parallel and merges them together later, XGBoost builds trees one at a time. In XGBoost, each new tree helps to correct errors made by previously trained tree. XGBoost is often preferred for its speed and performance, particularly on large datasets and is well-suited for problems where precision is paramount. Random Forest, on the other hand, is simpler to implement, more interpretable, and can be more robust to overfitting, making it a good choice for general-purpose applications.\n\n\n\n\nFigure 7.4: Flow chart of XGBoost algorithm (source: Guo et al., Applied Sciences, 2020).\n\n\n\nThe boosting method starts from a weak predictor and then improves performance sequentially by fitting a better model at each iteration. It fits a simple classifier to the training data and uses the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although random forest and boosting use trees for classification, there are significant differences. The performance of Random Forest generally increases with the number of trees until it becomes stable. Boosting trees apply finer divisions over previous results to improve performance [4]. Some recent papers show that it outperforms Random Forest for remote sensing image classification [6]. However, this result is not generalizable since the quality of the training dataset controls actual performance.\nIn sits, the XGBoost method is implemented by the sits_xbgoost() function, based on XGBoost R package, and has five hyperparameters that require tuning. The sits_xbgoost() function takes the user choices as input to a cross-validation to determine suitable values for the predictor.\nThe learning rate eta varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value gamma specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The max_depth value controls the maximum depth of the trees. Increasing this value will make the model more complex and likely to overfit (default is 6). The subsample parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The nrounds parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. To follow the convergence of the algorithm, users can turn the verbose parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the Random Forest method.\n\nset.seed(290356)\n\n# Train using  XGBoost\nxgb_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_xgboost(verbose = FALSE))\n\n# Classify using SVM model and plot the result\npoint_class_xgb &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = xgb_model)\n\n# View classification\nplot(point_class_xgb, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.5: Classification of time series using XGBoost."
  },
  {
    "objectID": "th_machine_learning.html#deep-learning-using-multilayer-perceptron",
    "href": "th_machine_learning.html#deep-learning-using-multilayer-perceptron",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.7 Deep learning using multilayer perceptron",
    "text": "7.7 Deep learning using multilayer perceptron\nTo support deep learning methods, sits uses the torch R package, which takes the Facebook torch C++ library as a back-end. Machine learning algorithms that use the R torch package are similar to those developed using PyTorch. The simplest deep learning method is multilayer perceptron (MLP), which are feedforward artificial neural networks. An MLP consists of three kinds of nodes: an input layer, a set of hidden layers, and an output layer. The input layer has the same dimension as the number of features in the dataset. The hidden layers attempt to approximate the best classification function. The output layer decides which class should be assigned to the input. In an MLP, all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most suitable combination of inputs that produces the best output.\nIn sits, MLP models can be built using sits_mlp(). Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the layers parameter, which is a list of integer values. The size of the list is the number of layers, and each element indicates the number of nodes per layer.\nThe choice of the number of layers depends on the inherent separability of the dataset to be classified. For datasets where the classes have different signatures, a shallow model (with three layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. Models with many hidden layers may take a long time to train and may not converge. We suggest to start with three layers and test different options for the number of neurons per layer before increasing the number of layers. In our experience, using three to five layers is a reasonable compromise if the training data has a good quality. Further increase in the number of layers will not improve the model.\nMLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [7], we use the relu activation function.\nThe optimization method (optimizer) represents the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [8]. Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [9]. Many optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [10]. The Adamw optimizer provides a good baseline and reliable performance for general deep learning applications [11]. By default, all deep learning algorithms in sits use Adamw.\nAnother relevant parameter is the list of dropout rates (dropout). Dropout is a technique for randomly dropping units from the neural network during training [12]. By randomly discarding some neurons, dropout reduces overfitting. Since a cascade of neural nets aims to improve learning as more data is acquired, discarding some neurons may seem like a waste of resources. In practice, dropout prevents an early convergence to a local minimum [7]. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.\nThe following example shows how to use sits_mlp(). The default parameters have been chosen based on a modified version of [13], which proposes using multilayer perceptron as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter layers; (b) Using the “relu” activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the “optimizer_adamw” as optimizer (default value); (e) a number of training steps (epochs) of 100; (f) a batch_size of 64, which indicates how many time series are used for input at a given step; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set aside for validation.\nTo simplify the output, the verbose option has been turned off. After the model has been generated, we plot its training history.\n\nset.seed(290356)\n\n# Train using an MLP model\n# This is an example of how to set parameters\n# First-time users should test default options first\nmlp_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_mlp(\n        optimizer        = torch::optim_adamw, \n        layers           = c(512, 512, 512),\n        dropout_rates    = c(0.40, 0.30, 0.20),\n        epochs           = 80,\n        batch_size       = 64,\n        verbose          = FALSE,\n        validation_split = 0.2))\n\n# Show training evolution\nplot(mlp_model)\n\n\n\n\n\nFigure 7.6: Evolution of training accuracy of MLP model.\n\n\n\nThen, we classify a 16-year time series using the multilayer perceptron model.\n\n# Classify using MLP model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(mlp_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.7: Classification of time series using MLP.\n\n\n\nIn theory, multilayer perceptron model can capture more subtle changes than Random Forest and XGBoost. In this specific case, the result is similar to theirs. Although the model mixes the Soy_Corn and Soy_Millet classes, the distinction between their temporal signatures is quite subtle. Also it suggests the need to improve the number of samples. In this example, the MLP model shows an increase in sensitivity compared to previous models. We recommend to compare different configurations since the MLP model is sensitive to changes in its parameters."
  },
  {
    "objectID": "th_machine_learning.html#temporal-convolutional-neural-network-tempcnn",
    "href": "th_machine_learning.html#temporal-convolutional-neural-network-tempcnn",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.8 Temporal Convolutional Neural Network (TempCNN)",
    "text": "7.8 Temporal Convolutional Neural Network (TempCNN)\nConvolutional neural networks (CNN) are deep learning methods that apply convolution filters (sliding windows) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture specifically designed to process sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series to produce another time series as the result of the convolution.\nThe TempCNN architecture for satellite image time series classification is proposed by Pelletier et al. [14]. It has three 1D convolutional layers and a final softmax layer for classification. The authors combine different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalization. In the TempCNN reference paper [14], the authors favourably compare their model with the Recurrent Neural Network proposed by Russwurm and Körner [15]. Figure 7.8 shows the architecture of the TempCNN model. TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN can capture complex temporal patterns in the data and produce accurate predictions.\n\n\n\n\nFigure 7.8: Structure of tempCNN architecture (source: [14]).\n\n\n\nThe function sits_tempcnn() implements the model. The first parameter is the optimizer used in the backpropagation phase for gradient descent. The default is adamw which is considered as a stable and reliable optimization function. The parameter cnn_layers controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter cnn_kernels indicates the size of the convolution kernels; the default is kernels of size 7. Activation for all 1D-CNN layers uses the “relu” function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter cnn_dropout_rates. The validation_split controls the size of the test set relative to the full dataset. We recommend setting aside at least 20% of the samples for validation.\n\nset.seed(290356)\nlibrary(torch)\n\n# Train using tempCNN\ntempcnn_model &lt;- sits_train(\n    samples_matogrosso_mod13q1, \n    sits_tempcnn(\n        optimizer            = torch::optim_adamw,\n        cnn_layers           = c(256, 256, 256),\n        cnn_kernels          = c(7, 7, 7),\n        cnn_dropout_rates    = c(0.2, 0.2, 0.2),\n        epochs               = 80,\n        batch_size           = 64,\n        validation_split     = 0.2,\n        verbose              = TRUE))\n\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\n\nFigure 7.9: Training evolution of TempCNN model.\n\n\n\nUsing the TempCNN model, we classify a 16-year time series.\n\n# Classify using TempCNN model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tempcnn_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.10: Classification of time series using TempCNN\n\n\n\nThe result has important differences from the previous ones. The TempCNN model indicates the Soy_Cotton class as the most likely one in 2004. While this result is possibly wrong, it shows that the time series for 2004 is different from those of Forest and Pasture classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, including forest degradation samples could improve the training data. In our experience, TempCNN models are a reliable way of classifying image time series [16]. Recent work which compares different models also provides evidence that TempCNN models have satisfactory behavior, especially in the case of crop classes [17]."
  },
  {
    "objectID": "th_machine_learning.html#residual-1d-cnn-networks-resnet",
    "href": "th_machine_learning.html#residual-1d-cnn-networks-resnet",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.9 Residual 1D CNN networks (ResNet)",
    "text": "7.9 Residual 1D CNN networks (ResNet)\nA residual 1D CNN network, also known as ResNet, is an extension of the standard 1D CNN architecture, adding residual connections between the layers. Residual connections allow the network to learn residual mappings, which are the difference between the input and output of a layer. By adding these residual connections, the network can learn to bypass specific layers and still capture essential features in the data.\nThe Residual Network (ResNet) for time series classification was proposed by Wang et al. [13], based on the idea of deep residual networks for 2D image recognition [18]. The ResNet architecture comprises 11 layers, with three blocks of three 1D CNN layers each (see Figure 7.11). Each block corresponds to a 1D CNN architecture. The output of each block is combined with a shortcut that links its output to its input, called a skip connection. The purpose of combining the input layer of each block with its output layer (after the convolutions) is to avoid the so-called “vanishing gradient problem”. This issue occurs in deep networks as the neural network’s weights are updated based on the partial derivative of the error function. If the gradient is too small, the weights will not be updated, stopping the training [19]. Skip connections aim to avoid vanishing gradients from occurring, allowing deep networks to be trained.\n\n\n\n\nFigure 7.11: Structure of ResNet architecture (source: [13]).\n\n\n\nIn sits, the Residual Network is implemented using sits_resnet(). The default parameters are those proposed by Wang et al. [13], as implemented by Fawaz et al. [20]. The first parameter is blocks, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128. The parameter kernels controls the size of the kernels of the three layers inside each block. It is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is “relu”, which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is optim_adamw, available in package torchopt.\n\n# Train using ResNet\nresnet_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_resnet(\n                          blocks               = c(64, 128, 128),\n                          kernels              = c(7, 5, 3),\n                          epochs               = 100,\n                          batch_size           = 64,\n                          validation_split     = 0.2,\n                          verbose              = FALSE))\n# Show training evolution\nplot(resnet_model)\n\n\n\n\n\nFigure 7.12: Training evolution of ResNet model.\n\n\n\nUsing the TempCNN model, we classify a 16-year time series.\n\n# Classify using Resnet model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(resnet_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.13: Classification of time series using TempCNN\n\n\n\nIn this case, the result of the ResNet model is quite similar to that of the TempCNN model. In the same way as TempCNN, the ResNet algorithm tends to be better at detecting agricultural or natural forest classes. When it comes to situation where transitions happen during the classification period, as in the case of the transition from forest to pasture in 2004, random forest models tend to be more efficient than TempCNN or ResNet."
  },
  {
    "objectID": "th_machine_learning.html#attention-based-models",
    "href": "th_machine_learning.html#attention-based-models",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.10 Attention-based models",
    "text": "7.10 Attention-based models\nAttention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.\nThe basic idea behind attention-based models is to allow the model to selectively focus on different input parts at different times. This is done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model uses them to compute a weighted sum of the input. The results capture the model’s attention on specific parts of the input.\nAttention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that not all inputs are alike in applications such as language translation. Consider the English sentence “Look at all the lonely people”. A sound translation system needs to relate the words “look” and “people” as the key parts of this sentence to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences [21].\nThe basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms textual input values into numerical vectors and a decoder that processes these vectors to provide suitable answers. The difference is how the values are handled internally. The two main differences between transformer models and other algorithms are positional encoding and self-attention. Positional encoding assigns an index to each input value, ensuring that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention compares every word in a sentence to every other word in the same sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT [22] and GPT-4 Bubeck2023?.\nThe application of attention-based models for satellite image time series analysis is proposed by Garnot et al. [23] and Russwurm and Körner [17]. A self-attention network can learn to focus on specific time steps and image features most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In the case of deforestation, the algorithm tries to identify the dates when the forest is being cut. Attention-based models are a means to identify events that characterize each land class.\nThe first model proposed by Garnot et al. is a full transformer-based model [23]. Considering that image time series classification is easier than natural language processing, Garnot et al. also propose a simplified version of the full transformer model [24]. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of the result.\nIn sits, the full transformer-based model proposed by Garnot et al. [23] is implemented using sits_tae(). The default parameters are those proposed by the authors. The default optimizer is optim_adamw, as also used in the sits_tempcnn() function.\n\n# Train a machine learning model using TAE\ntae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_tae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          validation_split     = 0.2,\n                          verbose              = FALSE))\n\n# Show training evolution\nplot(tae_model)\n\n\n\n\n\nFigure 7.14: Training evolution of Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the TAE model.\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.15: Classification of time series using TAE.\n\n\n\nGarnot and co-authors also proposed the Lightweight Temporal Self-Attention Encoder (LTAE) [24], which the authors claim can achieve high classification accuracy with fewer parameters compared to other neural network models. It is a good choice for applications where computational resources are limited. The sits_lighttae() function implements this algorithm. The most important parameter to be set is the learning rate lr. Values ranging from 0.001 to 0.005 should produce good results. See also the section below on model tuning.\n\n# Train a machine learning model using TAE\nltae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_lighttae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          opt_hparams = list(lr = 0.001),\n                          validation_split     = 0.2))\n\n# Show training evolution\nplot(ltae_model)\n\n\n\n\n\nFigure 7.16: Training evolution of Lightweight Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the LightTAE model.\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(ltae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 7.17: Classification of time series using LightTAE.\n\n\n\nThe behaviour of both sits_tae() and sits_lighttae() is similar to that of sits_tempcnn(). It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated with the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty since the classifiers do not agree on the resulting classes. In general, deep learning temporal-aware models are more sensitive to class variability than Random Forest and extreme gradient boosters."
  },
  {
    "objectID": "th_machine_learning.html#summary",
    "href": "th_machine_learning.html#summary",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "\n7.11 Summary",
    "text": "7.11 Summary\nIn this chapter, we present a basic description of the machine learning algorithms for image time series. These methods are available in the R sits package, but can be used in other environments as well. The basic distinction is between time-sensitive algorithms such as LightTAE and TempCNN and those who do not consider temporal order of values, such as Random Forest. In practice, we suggest that users take Random Forest as their baseline and then use LightTAE or TempCNN to try to improve classification accuracy."
  },
  {
    "objectID": "th_machine_learning.html#references",
    "href": "th_machine_learning.html#references",
    "title": "\n7  Machine learning algorithms for image time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.\n\n\n[2] \nC. Pelletier, S. Valero, J. Inglada, N. Champion, and G. Dedieu, “Assessing the robustness of Random Forests to map land cover with high resolution satellite image time series over large areas,” Remote Sensing of Environment, vol. 187, pp. 156–168, 2016, doi: 10.1016/j.rse.2016.10.010.\n\n\n[3] \nJ. S. Wright et al., “Rainforest-initiated wet season onset over the southern Amazon,” Proceedings of the National Academy of Sciences, 2017, doi: 10.1073/pnas.1621516114.\n\n\n[4] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[5] \nT. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–794, doi: 10.1145/2939672.2939785.\n\n\n[6] \nH. Jafarzadeh, M. Mahdianpari, E. Gill, F. Mohammadimanesh, and S. Homayouni, “Bagging and Boosting Ensemble Classifiers for Classification of Multispectral, Hyperspectral and PolSAR Data: A Comparative Evaluation,” Remote Sensing, vol. 13, no. 21, p. 4405, 2021, doi: 10.3390/rs13214405.\n\n\n[7] \nI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\n\n[8] \nS. Ruder, “An overview of gradient descent optimization algorithms,” CoRR, vol. abs/1609.04747, 2016, [Online]. Available: http://arxiv.org/abs/1609.04747.\n\n\n[9] \nL. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale Machine Learning,” SIAM Review, vol. 60, no. 2, pp. 223–311, 2018, doi: 10.1137/16M1080173.\n\n\n[10] \nR. M. Schmidt, F. Schneider, and P. Hennig, “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers,” in Proceedings of the 38th International Conference on Machine Learning, 2021, pp. 9367–9376, [Online]. Available: https://proceedings.mlr.press/v139/schmidt21a.html.\n\n\n[11] \nD. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization.” arXiv, 2017, doi: 10.48550/arXiv.1412.6980.\n\n\n[12] \nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n\n\n[13] \nZ. Wang, W. Yan, and T. Oates, “Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline,” 2017.\n\n\n[14] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019.\n\n\n[15] \nM. Russwurm and M. Korner, “Multi-temporal land cover classification with sequential recurrent encoders,” ISPRS International Journal of Geo-Information, vol. 7, no. 4, p. 129, 2018.\n\n\n[16] \nR. Simoes et al., “Satellite image time series analysis for big earth observation data,” Remote Sensing, vol. 13, no. 13, p. 2428, 2021, doi: 10.3390/rs13132428.\n\n\n[17] \nM. Rußwurm, C. Pelletier, M. Zollner, S. Lefèvre, and M. Körner, “BreizhCrops: A Time Series Dataset for Crop Type Mapping,” 2020, [Online]. Available: http://arxiv.org/abs/1905.11893.\n\n\n[18] \nK. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778, doi: 10.1109/CVPR.2016.90.\n\n\n[19] \nS. Hochreiter, “The vanishing gradient problem during learning recurrent neural nets and problem solutions,” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 2, pp. 107–116, 1998, doi: 10.1142/S0218488598000094.\n\n\n[20] \nH. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Deep learning for time series classification: A review,” Data Mining and Knowledge Discovery, vol. 33, no. 4, pp. 917–963, 2019.\n\n\n[21] \nA. Vaswani et al., “Attention is All you Need,” in Advances in Neural Information Processing Systems, 2017, vol. 30, [Online]. Available: https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\n\n[22] \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” arXiv, 2019, doi: 10.48550/arXiv.1810.04805.\n\n\n[23] \nV. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12322–12331, doi: 10.1109/CVPR42600.2020.01234.\n\n\n[24] \nV. S. F. Garnot and L. Landrieu, “Lightweight Temporal Self-attention for Classifying Satellite Images Time Series,” in Advanced Analytics and Learning on Temporal Data, 2020, pp. 171–181."
  },
  {
    "objectID": "ct_poland.html#introduction",
    "href": "ct_poland.html#introduction",
    "title": "\n13  Crop monitoring with SAR images in Poland\n",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nModern agriculture increasingly relies on advanced technologies, including satellite observations, which enable precise and regular monitoring of crop conditions. These methods make it possible to estimate sown areas, assess plant health, and forecast yields on a large scale - often in real time. Satellite data from missions such as Sentinel-2 support decision-making by providing objective and detailed information about the land. In this context, remote sensing becomes a valuable tool not only for farmers but also for institutions responsible for monitoring agricultural production at the national level. Recognizing this potential, the aim of the task was to develop a method for identifying thirty seven types of crops such as winter cereals (barley, rye, triticale, wheat), spring cereals (oat and barley), root crops (potato, sugar beet), buckwheat, maize, winter rapeseed, strawberry, grassland, along with legumes (beans, peas, lupines, lentil), vegetables (carrots, parsley, lettuce, cabbage, cucumber, tomato, etc.) and fruit trees (mainly apples, pears, cherries and plums). The task utilized satellite data, specifically radar imagery from Sentinel-1 and optical imagery from Sentinel-2."
  },
  {
    "objectID": "cy_poland.html#introduction",
    "href": "cy_poland.html#introduction",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\nRemote sensing technologies have increasingly become vital tools in modern agriculture for crop health monitoring and yield estimation. Open access to satellite imagery allows for the capture of spatial and temporal changes in crop growth, and in consequence more timely and accurate yield forecasting. The ultimate aim of this study is to provide a methodology to estimate crop yields on a country scale during the growing season. This responds to the need of statistical offices (in this case Statistics Poland) that are obliged to provide statistical information on agricultural production.\nIn this context, this study aims to evaluate with what accuracy yield forecasts can be produced using an automated system running on publicly available data. Therefore, within this study, a novel system for an operational crop yield forecasting at Nomenclature des Unités Territoriales Statistiques level 2 (NUTS-2) and Local Administrative Units (LAU) is proposed.\nThe system exploits Copernicus data sets and climate reanalysis at a global scale, and thus can be applied at any location. The system is dedicated for predicting yields of several crops, namely, winter wheat, winter rapeseed, or maize. Although, the particular software solutions have been developed for yield predictions in Poland, the concept of the model can be successfully used in any country. Obviously, other data bases, reference information or different settings of parameters should be then included."
  },
  {
    "objectID": "cy_poland.html#data",
    "href": "cy_poland.html#data",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "\n21.2 Data",
    "text": "21.2 Data\nThe yield estimation methods applied in this study integrate multiple sources of publicly available data, including satellite-derived vegetation indices, agro-meteorological indicators, and thermal-based crop development metrics. The yield forecasting model is shown in Figure 21.1.\n\n\n\n\nFigure 21.1: Yield Forecasting Model.\n\n\n\n\n21.2.1 Sentinel-3 data\nSentinel-3 Level-2 Near Real Time (NRT) products (i.e., surface reflectance, vegetation indices, land surface temperature) acquired by the Ocean and Land Colour Imager (OLCI) and the Sea and Land Surface Temperature Radiometer (SLSTR) are freely available from the Copernicus Space Data Ecosystem https://dataspace.copernicus.eu/) with a delay up to 3 h after satellite acquisition. The following daily products from the Sentinel-3A and Sentinel-3B satellites for 2018–2020 were used to estimate crop yielding in Poland:\n\nLand Full Resolution (LFR) product derived from OLCI imagery at a 300-m resolution consisting of Global Vegetation Index (OGVI) and Terrestrial Chlorophyll Index (OTCI) indices accompanied with rectified reflectances at 681 nm (RED) and 865 nm (NIR) channels used in this study to calculate the Normalized Difference Vegetation Index (NDVI).\nLand Surface Temperature (LST) from the SLSTR sensor at a 1-km resolution.\n\nThe acquired products were further mosaicked, cloud masked using associated quality flags and reprojected to the Poland CS92 coordinate system (EPSG:2180) using ESA SNAP software.\nThe above mentioned satellite data provide information related to vegetation vigour and biomass (from NDVI) and evapotranspiration (from LST derivatives). The NDVI and LST indicators provide complementary information on the vegetation physical status and access to water in the root zone (through evapotranspiration related to LST).\nTo guarantee universality of the method these particular Sentinel-3 products were selected in line with the overall system design to be based on Copernicus’ ready-to-use products, i.e., not requiring pre-processing of raw data. This ensures that the system will use the data generated by the latest processing methods provided by Copernicus.\n\n21.2.2 MODIS data\nWe used MODIS data for providing long time series of observations more than 20 years. The MOD09Q1, MOD11A2 collection 6 products generated from the Moderate-Resolution Imaging Spectroradiometer (MODIS) imagery are freely available from the Land Processes Distributed Active Archive Center (LP DAAC) of the U.S. Geological Survey. The acquired products covered Poland during the period 2000–2019. The MOD09Q1 product contains 8-day composites of land surface spectral reflectance acquired in 620–670-nm (RED) and 841–876-nm (NIR) channels at a 250-m resolution, which were used to calculate NDVI. Information on the presence of clouds, cloud shadows, and snow was derived from the associated quality flags. The MOD11A2 product provides 8-day composites of daytime and night-time LST and emissivity calculated from the 11.03 μμm and 12.02 μμm channels at a spatial resolution of 1 km. The MODIS NDVI and LST products were mosaicked and reprojected to the Poland CS92 coordinate system (EPSG:2180).\nThese MODIS products were chosen for a backward projection of Sentinel-3 products due to a similar processing stage and spatial resolution. They serve as a long term average for calculation of anomaly-derived indices from Sentinel-3. With this in mind, the data can be at a certain level of generality, as only the average course of the product along the vegetation season is needed. For this reason, it was decided to use 8-day composites, because in the aggregation process the observations with the highest quality are selected taking into account cloud contamination and the sun zenith angle.\n\n21.2.3 Agro-meteorological data\nWe derived agro-meteorological data for the period 2000–2019 at a resolution of 0.25\\(^\\circ\\) by 0.25\\(^\\circ\\) from the ERA-5 reanalysis generated by the European Centre for Medium-Range Weather Forecasts and freely distributed through the Copernicus Climate Data Store. They included hourly data at surface level consisting of: 2-m air temperature, total precipitation, surface incoming solar radiation, and volumetric soil water at 0–7-cm and 7–28-cm depths. These parameters were aggregated into daily means and/or sums using Climate Data Operators (CDO). Additionally, daily minimum and maximum air temperatures were derived to capture thermal variability more precisely.\n\n21.2.4 Crop masks\nA binary crop mask was derived from the Corine Land Cover version 2018 classification, freely distributed by the European Environmental Agency, by extracting 2.1.1 (non-irrigated arable land) and 2.4.2 (complex cultivation patterns) classes as arable land. Further, the binary mask was used to generate fractional arable land products at spatial resolutions matching the Sentinel-3, MODIS, and ERA-5 products. These fractional estimates served as weights to spatially aggregate satellite and agro-meteorological variables for the administrative units. An example of the extent of arable land represented as fractions is visualized for Poland in Figure 21.2.\n\n\n\n\nFigure 21.2: Arable land mask expressed as a fraction of arable land within 0.05 x 0.05 degree grid.\n\n\n\n\n21.2.5 Crop yield statistics\nThe reference data for the crop yield forecasting model consisted of official yield statistics provided by Statistics Poland at NUTS-2 and LAU (local administrative units) levels (Figure 21.3). The NUTS-2 data included winter wheat, winter rapeseed, and maize yields expressed in decitons [dt] for the period 1997–2019. At the LAU level, the length of the time series was shorter, and also inconsistent among administrative units (Figure 21.4).\n\n\n\n\nFigure 21.3: Administrative divisions of Poland for which crop yields were predicted: Nomenclature des Unités Territoriales Statistiques level 2 (NUTS-2)—red lines, and Local Administrative Units (LAU)—gray lines.\n\n\n\n\n\n\n\nFigure 21.4: Time span of official crop yield statistics at the LAU level. Black signifies no data.\n\n\n\nThe yield statistics for each NUTS-2 and LAU region were transformed into temporal yield residuals (Figure 21.5) from the Theil–Sen monotonic trend in annual yields estimates covering the period 1997–2019 [1]. These yield residuals were used as response variables in crop yield forecasting. The final absolute yield forecast is the sum of the monotonic trend and the forecasted yield residual for a particular year (Figure 21.6).\n\n\n\n\nFigure 21.5: Temporal yield residuals from Theil–Sen monotonic trend used as a response variable in crop yield forecasting for NUTS-2. Please mind different limits of Y-axes.\n\n\n\n\n\n\n\nFigure 21.6: Absolute annual crop yields for NUTS-2 units reported by Statistics Poland with a fitted Theil–Sen monotonic trend. Please mind different limits of Y-axes."
  },
  {
    "objectID": "cy_poland.html#methods",
    "href": "cy_poland.html#methods",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "\n21.3 Methods",
    "text": "21.3 Methods\n\n21.3.1 NDVI smoothing\nA 2-iterative spline smoothing technique was applied to filter NDVI values (derived from both MODIS and S3), mostly to minimize the effect of residual cloud cover. This technique assumes that residual cloud cover decreases NDVI values. Therefore, at the beginning the smoothing method fits the spline to the original data. Then, the distance (difference) between the fit and the original values creates weights so that the original values above the spline fit receive high weights and the values below receive weights equal to 0. Then, the final smoothed NDVI is generated by the second spline fit that uses these weights.\n\n21.3.2 Calibration of MODIS data\nMODIS NDVI and LST values were recalibrated to form a homogenous time series with indices derived from Sentinel-3. Calibration was defined as an adjustment (transformation) of MODIS-derived indices to make them homogeneous with data obtained from Sentinel-3. Based on the analysis performed at large agricultural fields, for which the spectrally homogenous satellite signal could have been derived, it was decided to use Random Forest (RF) and K - Nearest Neighbor (kNN) to calibrate NDVI and LST, respectively, using the following explanatory variables describing the MODIS-Sentinel-3 differences:\n\ntime: due to the apparent variability of crop development (and thus the variability of NDVI) along a vegetation season, and also due to the existence of a distinct annual vegetation cycle;\ngrowing degree days: indicating the amount of thermal energy supplied at a given time and the amount of energy needed to reach a given stage of crop development.\n\n21.3.3 Resampling of Explanatory Variables from Calendar Time to Thermal Time\nTo ensure year-to-year comparability of vegetation conditions, the explanatory variables were resampled from calendar time (day of year) to thermal time, which denotes cumulated mean daily air temperatures at 2 m at ground level above a crop-specific threshold. Thus, a thermal time is a good proxy for the crop development stage. Analysis of vegetation indices in respect to thermal time allows derivation of temporal anomalies by referring instantaneous values of an index to a multiannual average calculated for the same thermal time (i.e., the same crop development stage). If the calendar time was used instead of the thermal time, the temporal anomalies could be related to the shift in a vegetation season (e.g., a delay in biomass accumulation), however not to the actual crop conditions that are to be used to forecast crop yields.\nThermal time was calculated for a day \\(d\\) of the year as so-called Growing Degree Days (GDD) from daily maximum (\\(T_max\\)) and minimum (\\(T_min\\)) air temperatures using a formula:\n\n\n\n\nFigure 21.7: Calculation of growing degree days based on air temperature.\n\n\n\nwhere \\(T_b\\) stands for base temperature: 5°C for winter wheat and winter rapeseed, and 10°C for maize. In addition, \\(T_{n,i}\\) is replaced by \\(T_b\\) if \\(T_{n,i}\\) &lt; \\(T_b\\), and \\(T_{x,i}\\) is replaced by 30°C if \\(T_{x,i}\\) &gt; 30 °C. The conditional part of the equation equals 1 if the condition is met, and 0 otherwise, which causes that only positive values (of mean temperature reduced by the base temperature) to be summarized.\nBased on daily GDD values, all yield predictors were resampled for eight GDD values ranging from 150 °C to 1200 °C with a step of 150 °C. Since GDD were calculated at a daily time step, all predictors had to have the same 1-day temporal resolution prior resampling. Therefore, the 8-day MODIS NDVI and LST products were converted to 1-day resolution using the spline function.\n\n21.3.4 Crop yield forecasting\nFor crop yield forecasting proposed, we employ a machine learning technique, the eXtreme Gradient Boosting (XGBoost) algorithm. It is implemented in R environment to predict crop yield residuals from the Theil–Sen monotonic trend using a variety of predictors derived from satellite and agrometeorological data. To train the XGBoost method, we built an extensive input table for each administrative unit (LAU or NUTS-2) consisting of r rows and c columns, where r denotes a number of years for which predictors and reference crop yields were available, and c indicates a number of predictors. The following predictors were calculated for each of eight GDD levels (150 °C, 300 °C, 450 °C, …, 1200 °C):\n\nMinimum, maximum and mean air temperature;\nSurface radiation;\nAccumulated surface radiation since 1 April;\nSoil moisture at 0–7 cm and 7–28 cm levels;\nPrecipitation;\nAccumulated precipitation since 1 April;\n\n\\(NDVI_{GDD}\\);\n\n\\(VCI_{GDD}\\);\n$LST_{GDD};\n$TCI_{GDD};\nAnnual maximum NDVI (which does not correspond to the GDD levels).\n\nIn total, there were 170 predictors (c) but the number of years (r) varied between crop types and administrative units due to the reference data availability. For LAUs, the number of years is presented in Figure 21.4, whereas for NUTS-2, it equaled 17 for winter wheat and 22 for winter rapeseed and maize.\nAll predictors were linearly scaled to the range between zero and one. Then, highly correlated predictors (above 0.75) were removed. Further, the feature selection procedure was applied based on the recursive feature elimination employing the XGBoost method. The optimized XGBoost algorithm was ultimately trained based on selected predictors and crop yield residuals as a dependent variable. The application of the prediction model resulted in the forecasted crop yield residuals. The final absolute yield forecast was then calculated as a sum of this value and the crop yield estimated from the Theil–Sen monotonic trend."
  },
  {
    "objectID": "cy_poland.html#validation",
    "href": "cy_poland.html#validation",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "\n21.4 Validation",
    "text": "21.4 Validation\nValidation of forecasting models involved the comparison of predicted yields and reference official statistics that were not used for the model training. For each administrative unit, crop type, and GDD level, a cross-validation was performed. It followed the leave-one-year-out procedure, which is a special case of the k-fold cross validation where k equals a number of years in a time series. It must be noted that the selection of predictors was repeated at each iteration to avoid the predictor selection procedure to benefit from ‘knowing’ the data from the year that was used for validation.\nThree metrics were used to describe the model performance: Mean Bias Error (MBE, Equation), Root Mean Square Error (RMSE, Equation), and Modeling Efficiency (EF, Equation) as follows:\n\n\n\n\nFigure 21.8: Calculation of growing degree days based on air temperature.\n\n\n\nwhere \\(E_k\\) represents predicted crop yield value; \\(M_k\\) is the reference crop yield value; \\(\\overline{M}\\) is the average value of reference crop yield value; \\(k\\) is step of the time series (i.e. 1 year); and \\(n\\) is the length of the time series.\nThe RMSE and MBE were expressed in relative values (0–100%) denoted as the RRMSE and RMBE, respectively, by dividing these quality metrics by the mean of a reference data (\\(\\overline{M}\\)̅). The MBE, RMSE, and EF were also used to evaluate the accuracy of cross-calibration between MODIS and Sentinel-3 products. However, in this situation the \\(E_k\\) denotes re-calibrated MODIS NDVI/LST and \\(M_k\\) is the original Sentinel-3 NDVI/LST time series.\nAdditionally, the information generated by the model are presented to field experts engaged by Statistics Poland to provide estimates of yields as support for providing final decisions."
  },
  {
    "objectID": "cy_poland.html#presentation-of-results",
    "href": "cy_poland.html#presentation-of-results",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "\n21.5 Presentation of results",
    "text": "21.5 Presentation of results\nWe presented yields for all administrative levels of the country (voivodships, counties, communes). The estimates are derived on the basis of the expected yield. The results of yielding as well as GDD, NDVI are published in Statistics Poland geostatistical portal. Currently, Statistics Poland present estimates based on satellite imagery for 10 crops: winter barley, winter rye, winter triticale, winter wheat, spring cereals, winter rapeseed, maize, sugar beet, pastures, and permanent grasses."
  },
  {
    "objectID": "cy_poland.html#references",
    "href": "cy_poland.html#references",
    "title": "\n21  Yield Forecasting in Poland\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nJ. S. Bojanowski et al., “Integration of Sentinel-3 and MODIS Vegetation Indices with ERA-5 Agro-Meteorological Indicators for Operational Crop Yield Forecasting,” Remote Sensing, vol. 14, no. 5, p. 1238, 2022, doi: 10.3390/rs14051238."
  }
]