[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "UN Handbook on Remote Sensing for Agricultural Statistics",
    "section": "Welcome",
    "text": "Welcome\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively.\nThis book is a practical guide that enables for using remote sensing for agricultural statistics. It provides readers with the means of producing high-quality maps of agricutural areas and prediction of crop yields. Given the natural world’s complexity and huge variations in human-nature interactions, only local experts who know their countries and ecosystems can extract full information from big EO data.\nOne group of readers that we are keen to engage with is the national authorities on forest, agriculture, and statistics in developing countries. We aim to foster a collaborative environment where they can use EO data to enhance their national land use and cover estimates, supporting sustainable development policies."
  },
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "UN Handbook on Remote Sensing for Agricultural Statistics",
    "section": "Intellectual property rights",
    "text": "Intellectual property rights\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0."
  },
  {
    "objectID": "howto.html#section",
    "href": "howto.html#section",
    "title": "1  How to use this handbook",
    "section": "1.1 ",
    "text": "1.1"
  },
  {
    "objectID": "introduction.html#outline",
    "href": "introduction.html#outline",
    "title": "2  Introduction",
    "section": "2.1 Outline",
    "text": "2.1 Outline\nEarth Observation (EO) stands at a pivotal juncture in its capacity to transform agricultural statistics. For National Statistical Offices (NSOs) – our primary audience – this handbook arrives at a critical moment. The integration of EO into agricultural statistics is not just a technical opportunity; it is a policy imperative. As countries strive to meet Sustainable Development Goals (SDGs), ensure food security, and respond to climate volatility, the ability to generate timely, reliable, and spatially explicit agricultural data has become essential.\nWhile satellite data availability has expanded dramatically—most notably through the Copernicus Sentinel missions, which now provide near-daily global coverage at 10–20m resolution (ESA, 2024)—the operational use of these resources by NSOs remains limited. This persistent gap is striking, given the extensive scientific literature showcasing advanced techniques for land cover and crop type mapping. As Nakalembe et al. (2024) observe, “less than 30% of NSOs in developing economies routinely incorporate EO into crop reporting”—a stark mismatch between EO’s potential and its implementation. The reasons for this gap are multifaceted. First, data abundance itself introduces complexity. Platforms like the Copernicus Data Space Ecosystem and Digital Earth Africa offer petabyte-scale Analysis-Ready Data (ARD), but turning this into actionable agricultural statistics requires high-performance computing and structured processing chains. While countries such as Poland and Brazil have established national HPC ecosystems to process continental-scale EO data cubes (Łączyński et al., 2024), many NSOs lack comparable resources.\nSecond, research-grade EO solutions often falter in operational settings. Deep learning models may achieve over 90% accuracy under experimental conditions (Wang et al., 2024), but their deployment in national programs is challenged by inconsistent training data, cloud interference, and the need to reconcile statistical rigor with time-sensitive reporting cycles [deSimone2025]. Operational maturity demands more than algorithmic innovation—it requires adaptation, institutional readiness, and governance. To meet these challenges, the global EO community has coalesced around building robust toolchains. ESA’s Sen4Stat platform—built on GDAL and incorporating Orfeo components—offers a reproducible pipeline for crop mapping and yield forecasting [1]. Similarly, Brazil’s SITS R package enables scalable time-series classification and is now deployed in Chile’s national land accounts [deSimone2025]. But beyond individual tools, a more integrated coordination framework is emerging.\nAt the center of this effort is GEOGLAM (Group on Earth Observations Global Agricultural Monitoring)—a collaborative initiative uniting the world’s leading programs in crop monitoring. GEOGLAM orchestrates a diverse set of efforts including Sen4Stat, NASA Harvest, JRC, CropWatch, EOSTAT, CropMonitor, AMIS, and JECAM. These programs collectively advance the capacity of countries to monitor agricultural production through shared data infrastructures, methodological standardization, and transparency. Importantly, GEOGLAM is increasingly focused on strengthening the statistical rigor of crop monitoring—aligning more closely with NSO requirements for official area and yield estimation. This handbook complements that ambition, offering tools, case studies, and software that help bridge EO-based crop monitoring with statistical reporting. In parallel, the Food and Agriculture Organization (FAO) has played a critical role in enabling country-level capacity development. The UN Task Team on Earth Observations for Agricultural Statistics, established under the UN Committee of Experts on Big Data and Data Science and the UN Expert Group on Rural, Agricultural and Food Security Statistics, has driven collective progress in EO adoption by NSOs. Through documented contributions to the UN Statistical Commission, the Task Team has promoted South–South cooperation, capacity building, and joint projects across regions. Recently, it has deepened collaboration with the UN Global Platform, particularly through increased engagement with the UN Big Data Regional Hubs. In this context, the Global Hub for Big Data in China has taken a leading role in supporting this handbook initiative as a key deliverable for operationalizing EO within national statistical systems (FAO, 2025). Accuracy, in this context, is non-negotiable. As shown by [Olofsson202], classification errors above 15% can lead to over 20% distortion in area statistics—errors that compromise national reporting and policy formulation. This handbook addresses these issues across the full statistical workflow: from building ARD data cubes to implementing bias-corrected estimators, quantifying uncertainty, and generating statistically valid crop area measures. Real-world deployments, such as Chile’s 30m-resolution crop maps and Indonesia’s use of active learning to cut labeling costs by 40% while maintaining 90% detection accuracy, demonstrate the feasibility of integrating EO into official systems.\nWhat sets this handbook apart is its emphasis on reproducibility and reusability. Each chapter includes executable R and Python scripts that allow NSOs to directly implement the methods discussed. Whether using Sen4Stat’s GDAL workflows, SITS classifiers, NASA Harvest’s ARYA crop yield model Becker-Reshef2023?, or PRESTO, a transformer-based model for time-series feature extraction [2], the focus is on transparency and adaptability. These are not theoretical tools—they are field-tested systems embedded in national workflows.\nAs the EO landscape evolves, so too must national strategies. In 2025, NASA announced the discontinuation of Landsat Next—once intended to continue the legacy of the world’s longest-running EO mission. This unexpected restructuring highlights the fragility of EO continuity and the urgent need for risk mitigation. To this end, the handbook includes a dedicated chapter to help NSOs develop resilient, multi-source EO strategies that reduce dependency on any single mission or platform.\nFortunately, efforts are underway to secure long-term EO data availability. The European Space Agency, through its Sentinel Expansion and Copernicus Next Generation missions, is investing in hyperspectral, L-band radar, and thermal infrared sensors designed to ensure continuity and enhance capacity. These investments, aligned with GEOGLAM’s coordination, provide a sustainable pathway for countries to anchor EO in their statistical systems.\nThe journey from satellite pixels to policy-relevant statistics is complex—but no longer aspirational. With 15 real-world use cases, field-tested methodologies, and ready-to-use software, this handbook aims to help NSOs move from exploration to execution, and from data collection to decision-making.\n\n\n\n\n[1] P. Defourny et al., “Near real-time agriculture monitoring at national scale at parcel resolution: Performance assessment of the Sen2-Agri automated system in various cropping systems around the world,” Remote Sensing of Environment, vol. 221, pp. 551–568, 2019, doi: 10.1016/j.rse.2018.11.007.\n\n\n[2] G. Tseng, R. Cartuyvels, I. Zvonkov, M. Purohit, D. Rolnick, and H. Kerner, “Lightweight, Pre-trained Transformers for Remote Sensing Timeseries.” arXiv, 2024, doi: 10.48550/arXiv.2304.14065."
  },
  {
    "objectID": "theory.html#outline",
    "href": "theory.html#outline",
    "title": "Foundations",
    "section": "Outline",
    "text": "Outline\nThis is the introduction to the foundations of the book."
  },
  {
    "objectID": "th_remote_sensing.html#outline",
    "href": "th_remote_sensing.html#outline",
    "title": "3  Remote Sensing images: optical, SAR",
    "section": "3.1 Outline",
    "text": "3.1 Outline\nThis chapter provides a general introduction to remote sensing imagery. The authors discuss different types of remote sensing satellites (optical, SAR, hyperspectral) and include examples of satellite constellations (e.g, Landsat, Sentinel-1, CHEOS)."
  },
  {
    "objectID": "th_lucc.html#outline",
    "href": "th_lucc.html#outline",
    "title": "4  Land cover and crop classification schemas",
    "section": "4.1 Outline",
    "text": "4.1 Outline\nDescribes the problem of designing a good classification schema and provides some examples. It includes a discussion of LCCS/FAO and highlights the limitations of land cover classification based on space first, especially in complex landscapes. Investigate land classification schemes based on events detected by satellite image time series; we show these concepts to be instances of events. For continuous monitoring of land change, event recognition is better than object identification as the prevailing paradigm of LUCC."
  },
  {
    "objectID": "th_quality_control.html#outline",
    "href": "th_quality_control.html#outline",
    "title": "5  Quality control of training sets for agricultural statistics",
    "section": "5.1 Outline",
    "text": "5.1 Outline\nSelecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has shown that the number and quality of training samples are crucial factors in obtaining accurate results. This chapter presents pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labelled or possess low discriminatory power. Explains the basics of machine learning and provides examples of designing and using good training sets. Explains k-fold validation, SOM clustering and sample imbalance removal, with examples in R and Python."
  },
  {
    "objectID": "th_machine_learning.html#outline",
    "href": "th_machine_learning.html#outline",
    "title": "6  Machine learning classification of remote sensing images",
    "section": "6.1 Outline",
    "text": "6.1 Outline\nThis chapter describes machine learning methods for classifying individual remote sensing images and image time series. The chapter considers three kinds of algorithms: • Machine learning algorithms that do not explicitly consider the spatial and temporal structure of the time series. These methods include random forests, support vector machine and extreme gradient boosting. • Deep learning methods which consider temporal relations between observed values in a time series. This class of models includes 1D convolutional neural networks and temporal attention-based encoders. • Semantic segmentation methods based on U-net paradigms and multidimensional 2D convolution."
  },
  {
    "objectID": "th_uncertainty.html#outline",
    "href": "th_uncertainty.html#outline",
    "title": "7  Spatial map uncertainty estimation and active learning in crop classification",
    "section": "7.1 Outline",
    "text": "7.1 Outline\nDescribes methods for estimating uncertainty of machine learning classification maps and how to use such estimates to improve classification accuracy. Map uncertainty refers to the degree of doubt or ambiguity in the accuracy of each pixel of the classification results. Several sources of uncertainty can arise during land classification using satellite data, including: a) classification errors; b) ambiguity in classification schema; c) variability in the landscape; and d) limitations of the data. The quality and quantity of input data can influence the accuracy of the classification results. Quantifying uncertainty in land classification is important for ensuring that the results are reliable and valid for decision-making."
  },
  {
    "objectID": "th_validation.html#outline",
    "href": "th_validation.html#outline",
    "title": "8  Map validation and use of maps for area estimation",
    "section": "8.1 Outline",
    "text": "8.1 Outline\nStatistically robust and transparent approaches for assessing accuracy are essential parts of the land classification process. The sits package supports the “good practice” recommendations for designing and implementing an accuracy assessment of a change map and estimating the area based on reference sample data. These recommendations address three components: sampling design, reference data collection, and accuracy estimates."
  },
  {
    "objectID": "th_data_sources.html#ard-image-collections",
    "href": "th_data_sources.html#ard-image-collections",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.1 ARD Image Collections",
    "text": "9.1 ARD Image Collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD images processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and are provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organised as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images of an ARD collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure 9.1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 9.1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "href": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.2 Cloud platforms providing ARD data",
    "text": "9.2 Cloud platforms providing ARD data\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes.\nThere are a large number of cloud platforms providing open data organized as Analysis-Ready Data, including: Amazon Web Services (AWS), Brazil Data Cube (BDC), Copernicus Data Space Ecosystem (CDSE), Digital Earth Africa (DEAFRICA), Digital Earth Australia (DEAUSTRALIA), Microsoft Planetary Computer (MPC), and Nasa Harmonized Landsat/Sentinel (HLS),\nThis chapter describes how to access these collections and transform ARD images into regular data cubes. A data cube is a set of images organized in tiles of a grid system (e.g., MGRS). Each tile contains single-band images in a unique zone of the coordinate system (e.g, tile 20LMR in MGRS grid) covering the period between start_date and end_date. All tiles share the same set of regular temporal intervals and the same spectral bands and indices. All images have the same spatial resolution."
  },
  {
    "objectID": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "href": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.3 Tiling systems used by ARD collections",
    "text": "9.3 Tiling systems used by ARD collections\nARD image collections are organized in spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone has blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure Figure 9.2 shows the MGRS tiling system for a part of the Northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 9.2: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which breaks the coverage of Landsat satellites into images identified by path and row (see Figure @ref(fig:wrs)). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 9.3: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "href": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.4 Major global or large regional cloud provides",
    "text": "9.4 Major global or large regional cloud provides\nThe following ARD image cloud providers provide global or large regional data:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A level 2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products available include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedian of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are Copernicus DEM-30 and MOD13Q1, MOD10A1 and MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30)."
  },
  {
    "objectID": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.5 Accessing ARD image collections in cloud providers",
    "text": "9.5 Accessing ARD image collections in cloud providers\n\nWe now present the use of the sits R package to obtain information on ARD image collection from cloud providers, using the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information which many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured in a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections in cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be assessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "th_data_sources.html#amazon-web-services",
    "href": "th_data_sources.html#amazon-web-services",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.6 Amazon Web Services",
    "text": "9.6 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports collection SENTINEL-2-L2A which is open data. The bands in 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n\n\n\n\nFigure 9.4: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#microsoft-planetary-computer",
    "href": "th_data_sources.html#microsoft-planetary-computer",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.7 Microsoft Planetary Computer",
    "text": "9.7 Microsoft Planetary Computer\nThe sits package supports access to open data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30 meter resolution) and MOD13Q1-6.1(version 6.1 of the MODIS MOD13Q1 product). Access to the non-open data collection is available for users that have registration in MPC.\n\n9.7.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC = sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\n\n\n\nFigure 9.5: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n9.7.2 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from Landsat-4/5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasilia in Brazil.\n\n\nR\nPython\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi = {\"lon_min\" : -43.5526, \"lat_min\" : -2.9644, \n         \"lon_max\" : -42.5124, \"lat_max\" : -2.1671}\n# Select the cube\ns2_L8_cube_MPC = sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = [\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"],\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\n\n\n\nFigure 9.6: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n9.7.3 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject for variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line-of-sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be re-projected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso Brazil that is an area of the Amazon forest that has been deforested. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear included in relation to the Earth’s longitude.\n\n\nR\nPython\n\n\n\n\ncube_s1_grd &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_grd =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = [\"VV\"],\n  orbit = \"descending\",\n  tiles = [\"21LUJ\",\"21LVJ\"],\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 9.7: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain corrected) images as described in the next session.\n\n9.7.4 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric distortions and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of the surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and re-project the SAR image from the slant range (radar line-of-sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\n\nR\nPython\n\n\n\n\ncube_s1_rtc &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_rtc =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = [\"VV\", \"VH\"],\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 9.8: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes mountains. Users are invited to compare this images with the one from the SENTINEL-1-GRD collection and see the significant geometrical distortions of the GRD image compared with the RTC one.\n\n9.7.5 Copernicus DEM 30 meter images in MPC\nThe Copernicus digital elevation model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, designed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30 meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both case, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\n\nR\nPython\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\n\n\n\n\nFigure 9.9: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "th_data_sources.html#brazil-data-cube",
    "href": "th_data_sources.html#brazil-data-cube",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.8 Brazil Data Cube",
    "text": "9.8 Brazil Data Cube\nThe Brazil Data Cube (BDC) is built by Brazil’s National Institute for Space Research (INPE), to provide regular EO data cubes from CBERS, LANDSAT, SENTINEL-2, and TERRA/MODIS satellites for environmental applications. The collections available in the BDC are: LANDSAT-OLI-16D (Landsat-8 OLI, 30 m resolution, 16-day intervals), SENTINEL-2-16D (Sentinel-2A and 2B MSI images at 10 m resolution, 16-day intervals), CBERS-WFI-16D (CBERS 4 WFI, 64 m resolution, 16-day intervals), CBERS-WFI-8D(CBERS 4 and 4A WFI images, 64m resolution, 8-day intervals), and MOD13Q1-6.1 (MODIS MOD13SQ1 product, collection 6, 250 m resolution, 16-day intervals). For more details, use sits_list_collections(source = \"BDC\").\nThe BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The large grid has tiles of 4224.4 \\(\\times4\\) 224.4 km2 and is used for CBERS-4 AWFI collections at 64 m resolution; each CBERS-4 AWFI tile contains images of 6600 \\(\\times\\) 6600 pixels. The medium grid is used for Landsat-8 OLI collections at 30 m resolution; tiles have an extension of 211.2 \\(\\times\\) 211.2 km2, and each image has 7040 \\(\\times\\) 7040 pixels. The small grid covers 105.6 \\(\\times\\) 105.6 km2 and is used for Sentinel-2 MSI collections at 10 m resolutions; each image has 10560 \\(\\times\\) 10560 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [1].\n\n\n\n\nFigure 9.10: Hierarchical BDC tiling system showing (a) large BDC grid overlayed on Brazilian biomes, (b) one learge tile from the grid used for CBERS-4 AWFI data, (c) four medium tiles from the grid used for LANDSAT data; and (d) sixteen small tiles from the grid used for SENTINEL-2 data. Tiles in (b), (c), and (d) are nested.\n\n\n\nTo access the BDC, users must provide their credentials using environment variables, as shown below. Obtaining a BDC access key is free. Users must register at the BDC site to obtain a key. Please include your BDC access key in your “.Rprofile”.\n\nSys.setenv(BDC_ACCESS_KEY = \"&lt;your_bdc_access_key&gt;\")\n\nIn the example below, the data cube is defined as one tile (“005004”) of CBERS-WFI-16D collection, which holds CBERS AWFI images at 16 days resolution.\n\n\nR\nPython\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = c(\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile = sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = [\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"],\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n\n\n\n\nFigure 9.11: CBERS-4 WFI image in a Cerrado area in Brazil."
  },
  {
    "objectID": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "href": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.9 Copernicus Data Space Ecosystem (CDSE)",
    "text": "9.9 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CSDE S3 credentials site. The site will request you to add a new credential. You will receive two keys: an an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n      AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n      AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nAfter including these lines in your .Rprofile, restart R for the changes to take effect. By following these steps, users will have access to the Copernicus Data Space Ecosystem.\n\n9.9.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena river delta in Siberia in summertime.\n\n\nR\nPython\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = [\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = \"52XDF\"\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 9.12: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n9.9.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE are the radiometric terrain corrected (RTC) Sentinel-1 images. in CDSE, this product is referred to as normalized terrain backscater (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ0) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set-up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift valley in Ethiopia.\n\n\nR\nPython\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = [\"VV\", \"VH\"],\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = \"37NCH\"\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 9.13: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "th_data_sources.html#digital-earth-africa",
    "href": "th_data_sources.html#digital-earth-africa",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.10 Digital Earth Africa",
    "text": "9.10 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level 2A (SENTINEL-2-L2A), organised as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardised NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California in Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30 meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in grid system WRS-2.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done wither using tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tiles (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\n\nR\nPython\n\n\n\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n    lon_min = 46.1, lat_min = -15.6,\n    lon_max = 46.6, lat_max = -16.1\n  ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\ndea_s2_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = {\n    \"lon_min\" : 46.1, \"lat_min\" : -15.6,\n    \"lon_max\" : 46.6, \"lat_max\" : -16.1\n  },\n    bands = [\"B02\", \"B04\", \"B08\"],\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\n\n\n\nFigure 9.14: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data, covering the Serengeti plain in Tanzania.\n\n\nR\nPython\n\n\n\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\ndea_l9_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = {\n        \"lon_min\" : 33.0, \"lat_min\" : -3.60, \n        \"lon_max\" : 33.6, \"lat_max\" : -3.00\n    },\n    bands = [\"B04\", \"B05\", \"B06\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 9.15: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for year 2020, for an area near the border between Congo and Rwanda.\n\n\nR\nPython\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = {\n       \"lon_min\" : 28.69, \"lat_min\" : -2.35, \n       \"lon_max\" : 29.35, \"lat_max\" : -1.56\n    },\n    bands = [\"HH\", \"HV\"],\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\n\n\n\nFigure 9.16: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "th_data_sources.html#digital-earth-australia",
    "href": "th_data_sources.html#digital-earth-australia",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.11 Digital Earth Australia",
    "text": "9.11 Digital Earth Australia\nDigital Earth Australia (DEAUSTRALIA) is an initiative by Geoscience Australia that uses satellite data to monitor and analyze environmental changes and resources across the Australian continent. It provides many datasets that offer detailed information on phenomena such as droughts, agriculture, water availability, floods, coastal erosion, and urban development. The DEAUSTRALIA image collections in sits are:\n\nGA_LS5T_ARD_3: ARD images from Landsat-5 satellite, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, and “CLOUD”.\nGA_LS7E_ARD_3: ARD images from Landsat-7 satellite, with the same bands as Landsat-5.\nGA_LS8C_ARD_3: ARD images from Landsat-8 satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, “PANCHROMATIC”, and “CLOUD”.\nGA_LS9C_ARD_3: ARD images from Landsat-9 satellite, with the same bands as Landsat-8.\nGA_S2AM_ARD_3: ARD images from Sentinel-2A satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “RED-EDGE-1”, “RED-EDGE-2”, “RED-EDGE-3”, “NIR-1”, “NIR-2”, “SWIR-2”, “SWIR-3”, and “CLOUD”.\nGA_S2BM_ARD_3: ARD images from Sentinel-2B satellite, with the same bands as Sentinel-2A.\nGA_LS5T_GM_CYEAR_3: Landsat-5 geomedian images, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR1”, “SWIR2”, “EDEV”, “SDEV”, “BCDEV”.\nGA_LS7E_GM_CYEAR_3: Landsat-7 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS8CLS9C_GM_CYEAR_3: Landsat-8/9 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS_FC_3: Landsat fractional land cover, with bands “BS”, “PV”, “NPV”.\nGA_S2LS_INTERTIDAL_CYEAR_3: Landsat/Sentinel intertidal data, with bands “ELEVATION”, “ELEVATION-UNCERTAINTY”, “EXPOSURE”, “TA-HAT”, “TA-HOT”, “TA-LOT”, “TA-LAT” “TA-OFFSET-HIGH”, “TA-OFFSET-LOW”, “TA-SPREAD”, “QA-NDWI-CORR”and “QA-NDWI-FREQ”.\n\nThe following code retrieves an image from Sentinel-2A.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_55KGR &lt;- sits_mgrs_to_roi(\"55KGR\")\n# retrieve the world cover map for the chosen roi\ns2_56KKV &lt;- sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n# retrieve the world cover map for the chosen tile\ns2_56KKV = sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"],\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n\n\n\n\nFigure 9.17: Sentinel-2A image from the DEAUSTRALIA collection showing MGRS tile 56KKV."
  },
  {
    "objectID": "th_data_sources.html#harmonized-landsat-sentinel",
    "href": "th_data_sources.html#harmonized-landsat-sentinel",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.12 Harmonized Landsat-Sentinel",
    "text": "9.12 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS tiling system is identical as the one used for Sentinel-2 (MGRS). The tiles dimension is 109.8 km and there is an overlap of 4,900 m on each side.\nTo access NASA HLS, users need to registed at NASA EarthData, and save their login and password in a ~/.netrc plain text file in Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with the earthdatalogin::edl_netrc. This function creates a properly configured .netrc file in the user’s home directory and an environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections which are available in Microsoft Planetary Computer, which are a copy of the NASA collections and are faster to access.\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \nusername = \"&lt;your user name&gt;\", \npassword = \"&lt;your password&gt;\" \n) \n\nAccess to images in NASA HLS is done by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n# define a region of interest\nroi = { \"lon_min\" : -45.6422, \"lat_min\" : -24.0335,\n        \"lon_max\" : -45.0840, \"lat_max\" : -23.6178 }\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"],\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\n\n\n\nFigure 9.18: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#eo-products-from-terrascope",
    "href": "th_data_sources.html#eo-products-from-terrascope",
    "title": "\n9  EO Big Data Sources\n",
    "section": "\n9.13 EO products from TERRASCOPE",
    "text": "9.13 EO products from TERRASCOPE\nTerrascope is online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of Earth observation data and processing services that are accessible free of charge. Currently, sits supports the World Cover 2021 maps, produced by VITO with support form the European Commission and ESA. The following code shows how to access the World Cover 2021 convering tile “22LBL”. The first step is to use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_r\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_py\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\n\n\n\nFigure 9.19: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "th_data_sources.html#references",
    "href": "th_data_sources.html#references",
    "title": "\n9  EO Big Data Sources\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nK. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033."
  },
  {
    "objectID": "th_design_frames.html#outline",
    "href": "th_design_frames.html#outline",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.1 Outline",
    "text": "10.1 Outline\nThis chapter deals with the requirements of field surveys for statistical quality and for compatibility with EO data. Earth Observation (EO) are widely used in agricultural statistics production. However, the accuracy of EO-based land use classification is limited because of the limitations of using in situ census or survey data as training sets for EO applications. In this work, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments."
  },
  {
    "objectID": "th_parcel_extraction.html#outline",
    "href": "th_parcel_extraction.html#outline",
    "title": "11  Automatic Extraction of Parcels",
    "section": "11.1 Outline",
    "text": "11.1 Outline\nDescribes AI methods for automatic parcel detection. The link to the forthcoming Census of Agriculture 2026 in Brazil should be treated, explaining the functionality of the boundary data set to the efficiency gains expected during the operations of the census."
  },
  {
    "objectID": "crop_type_mapping.html#outline",
    "href": "crop_type_mapping.html#outline",
    "title": "Use Cases in Crop Type Mapping",
    "section": "Outline",
    "text": "Outline\nDescribe the use cases."
  },
  {
    "objectID": "ct_poland.html#outline",
    "href": "ct_poland.html#outline",
    "title": "12  Crop monitoring with SAR images in Poland",
    "section": "12.1 Outline",
    "text": "12.1 Outline\nCountry-wide crop classification using Sentinel-1 radar imagery. Reference: https://www.sciencedirect.com/science/article/pii/S0303243422000095"
  },
  {
    "objectID": "ct_mexico.html#outline",
    "href": "ct_mexico.html#outline",
    "title": "13  Crop classification in Mexico",
    "section": "13.1 Outline",
    "text": "13.1 Outline"
  },
  {
    "objectID": "ct_senegal.html#overview",
    "href": "ct_senegal.html#overview",
    "title": "14  Multi-seasonal crop mapping in Senegal",
    "section": "14.1 Overview",
    "text": "14.1 Overview"
  },
  {
    "objectID": "ct_zimbabwe.html#section",
    "href": "ct_zimbabwe.html#section",
    "title": "15  Crop classification in Zimbabwe",
    "section": "15.1 ",
    "text": "15.1"
  },
  {
    "objectID": "ct_chile.html#overview",
    "href": "ct_chile.html#overview",
    "title": "16  Crop classification and land use mapping in Chile",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nThis section outlines a pilot methodology for land use and crop classification in Chile, using the Maule Region as a case study. It combines Sentinel-2 time-series imagery, digital elevation models (DEM) and machine learning to produce scalable LULC maps. Ground truth data from agricultural censuses and other sources are used to enhance classification accuracy. The approach aims to support the integration of remote sensing into official agricultural statistics.\nThe study highlights key challenges, including data gaps, cloud cover, and crop heterogeneity. However, it also shows how time-series analysis helps capture crop phenology for better classification. This pilot sets the foundation for a standardized national mapping framework, enabling more timely and spatially detailed agricultural data for decision-making."
  },
  {
    "objectID": "ct_digital_earth_africa.html#outline",
    "href": "ct_digital_earth_africa.html#outline",
    "title": "17  Crop classification using Digital Earth Africa",
    "section": "17.1 Outline",
    "text": "17.1 Outline\nDigital Earth Africa offers continental-scale satellite-derived data products and provides guidance on their application in analytical workflows. This chapter demonstrates the use of high quality, cloud-free, geomedian composite images with median absolute deviations (GeoMADs) for crop type mapping. Digital Earth Africa’s GeoMADs are applied to a classification framework in the satellite image time series (SITS) package in the R language.\nThe chapter demonstrates how high-quality, ‘off-the-shelf’ satellite image composites can overcome common challenges in classification workflows, such as the ‘curse of dimensionality’ and computational demand. It also introduces readers and users to Digital Earth Africa products and services, and modes of access, especially through the SITS R package."
  },
  {
    "objectID": "cy_finland.html#section",
    "href": "cy_finland.html#section",
    "title": "18  Early-season crop yield mapping in Finland",
    "section": "18.1 ",
    "text": "18.1 \n```"
  },
  {
    "objectID": "cy_indonesia.html#outline",
    "href": "cy_indonesia.html#outline",
    "title": "19  Rice Paddy Phenology in Indonesia",
    "section": "19.1 Outline",
    "text": "19.1 Outline\nCombination of multi-year area sampling frame with Sentinel-1 imagery to predict rice phenology and estimate harvest areas.\nRice data plays a vital role in shaping national food security policies, especially in a country like Indonesia, where rice is both a staple and a strategic commodity. At present, the paddy harvested area is estimated through a monthly ground survey known as the Area Sampling Frames (ASF), conducted by Statistics Indonesia. While this method has long served as the official approach, the high cost of data collection and recurring challenges in field implementation underscore the urgent need for more efficient and scalable alternatives. Satellite Imagery Time Series (SITS) data, particularly from the historical archives of Sentinel-1, emerges as a promising solution. By leveraging the power of the XGBoost algorithm — a state-of-the-art machine learning model — it becomes possible to detect phenological stages of paddy growth remotely and with remarkable precision.\nBuilding on this, the study in 2024 introduces an alternative approach implemented in selected 10 provinces across Indonesia based on the harvest and productivity data. The study outlines a carefully designed workflow that enhances model accuracy through a combination of clustering techniques, missing data imputation, noise reduction, and region-specific model calibration. The results show that most cluster regions achieve high accuracy in classifying paddy phenological stages, while the estimated harvested area patterns align closely with the official statistics. These outcomes not only confirm the reliability of the proposed method but also highlight its strong potential to complement or even streamline the existing survey-based system in the future."
  },
  {
    "objectID": "cy_poland.html#overview",
    "href": "cy_poland.html#overview",
    "title": "20  Yield Forecasting in Poland",
    "section": "20.1 Overview",
    "text": "20.1 Overview\nIntegration of Sentinel-3 and MODIS Vegetation Indices with ERA-5 Agro-Meteorological Indicators for Operational Crop Yield Forecasting."
  },
  {
    "objectID": "cy_colombia.html#section",
    "href": "cy_colombia.html#section",
    "title": "21  Rice Phenology in Colombia",
    "section": "21.1 ",
    "text": "21.1"
  },
  {
    "objectID": "cy_china.html#section",
    "href": "cy_china.html#section",
    "title": "22  Crop type classification and crop yield estimation in China",
    "section": "22.1 ",
    "text": "22.1"
  },
  {
    "objectID": "ad_geoglam.html#overview",
    "href": "ad_geoglam.html#overview",
    "title": "23  Extraction of crop statistics from crop type and crop yield maps",
    "section": "23.1 Overview",
    "text": "23.1 Overview\nDescribe concrete experiences carried out by FAO-EOSTAT, Sen4Stat, and NASA Harvest. Ukraine - Crop area and Yield - Production Nasa Harvest Stratifier, sampling, regression estimator"
  },
  {
    "objectID": "ad_world_cereal.html#section",
    "href": "ad_world_cereal.html#section",
    "title": "24  WorldCereal - A Global Effort for Crop Mapping",
    "section": "24.1 ",
    "text": "24.1"
  },
  {
    "objectID": "ad_uav_applications.html#section",
    "href": "ad_uav_applications.html#section",
    "title": "25  UAV use in Agricultural Statistics",
    "section": "25.1 ",
    "text": "25.1"
  },
  {
    "objectID": "ad_disaster_response.html#section",
    "href": "ad_disaster_response.html#section",
    "title": "26  Remote Sensing for Agricultural Disaster Response",
    "section": "26.1 ",
    "text": "26.1"
  },
  {
    "objectID": "ad_governance.html#introduction",
    "href": "ad_governance.html#introduction",
    "title": "27  Data Governance for Agricultural Statistics",
    "section": "27.1 Introduction",
    "text": "27.1 Introduction"
  },
  {
    "objectID": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "href": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.2 Matching in situ survey data to remote sensing analysis needs",
    "text": "10.2 Matching in situ survey data to remote sensing analysis needs\nData collections done by NSOs can potentially become the main source of training data for EO applications for agricultural statistics. However, while national surveys often adopt GPS technology, satellite imagery remains largely unused by NSOs. To change this status quo, the Global Strategy to Improve Agricultural and Rural Statistics compiled various use cases in its Handbook on Remote Sensing for Agricultural Statistics, highlighting the missing links between EO-driven surveys and most common NSO surveys [1]. Experience shows that sampling design, response design, and quality control of NSO surveys must follow well-documented requirements to obtain statistically sound results when using EO data.\nEO-related quality assurance of the in situ datasets developed in the FAO EOStat and ESA Sent4Stat projects includes two main components: (a) evaluation of survey design and (b) in situ data assessment using EO data. Quality assessment measures the suitability of a statistical survey (i.e. sampling and response design) to leverage satellite imagery in support of agriculture statistics. Many NSOs create their in situ protocols with a focus on aggregation at higher administrative tiers, often overlooking their potential application in EO contexts. Table 1 presents eleven criteria for NSOs to enable the combined use of in situ data for traditional surveys and EO applications.\n\nAssessment framework to qualify the compatibility of an in situ survey design to leverage EO satellite data for agriculture statistics\n\n\n\n\n\n\nCriteria related to the sampling design\n\n\n\n\n\nObservation timing allows identification of crop type in the field (unlike some household surveys, the survey must take place when the crop is visible on the field)\n\n\n\n\n\nMinimum number of samples for marginal crops (including intercrop types) to provide balanced datasets in terms of crop type sample distribution\n\n\n\n\n\nLocal homogeneity of each sample unit to match the corresponding satellite observation footprint\n\n\n\nCriteria related to the response design\n\n\n\n\n\nGeoreferenced ground observation at field or point level to link with satellite geospatial dataset (household geographic coordinates being insufficient)\n\n\n\n\n\nSample unit size at least matching the considered satellite observation footprint (not only the spatial resolution)\n\n\n\n\n\nContextual observation to document sample quality and qualify its representativity\n\n\n\n\n\nRich labelling of each sample beyond crop type to indicate specific growing conditions (e.g., weeds abundance, limited crop cover, water lodging, tree density)\n\n\n\n\n\nHigh precision of crop type nomenclature, including information about infrastructure and agriculture practices (e.g., irrigation, greenhouses, crop under canopy, agroforestry, species dominance for intercropping)"
  },
  {
    "objectID": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "href": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.3 EO-based quality control of in situ surveys",
    "text": "10.3 EO-based quality control of in situ surveys\nAn integral aspect of the statistical survey process is its quality control procedure. Nationwide surveys require heavy logistics involving hundreds of enumerators dispersed across the country. In many countries, digital encoding devices have integrated GPS receivers and communication support, making near real-time quality checks feasible. In Senegal, the Direction de l’Analyse, de la Prévision et des Statistiques Agricoles (DAPSA) employs near real-time quality control to oversee national data collection, enabling the field campaign to incorporate repetition requests and corrections as needed.\nAchieving the quality required for EO utilization imposes demands on the training of enumerators and presents more challenges for controllers. Besides being useful for aggregated surveys, in situ data must pass EO-based quality control checks. Such protocol is even more critical when combining datasets from distinct surveys requiring strict harmonisation. Experiences of FAO-EOSTAT and Sen2Stat with list frame and area frame statistical survey datasets led us to establish an EO-based quality control protocol. This protocol relies on existing maps and satellite time series processing as independent data quality control sources. Table 2 outlines the criteria applied for EO-based data quality control."
  },
  {
    "objectID": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "href": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.4 Technical suitability of in situ data",
    "text": "10.4 Technical suitability of in situ data\nThe first three criteria of the EO-based quality control concern the technical suitability of in situ data for EO applications. Typically, surveyors record GPS coordinates for household localization, crop observation placement, or field area measurement. Requirements for geospatial analysis include ensuring the topological soundness of spatial features, which involves verifying polygon closure, identifying duplicate points, and resolving polygon overlaps (Figure 10.1).\n\n\n\n\nFigure 10.1: Quality control of geospatial features and their coordinates. Examples acquired during the FAO EOStat project in Senegal from left to right: polygon recorded as points sequence instead of a closed polygon, polygon overlap detected and solved, and benchmarking of various protocols and devices (tablet with integrated GPS versus Garmin receiver) to record field boundaries."
  },
  {
    "objectID": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "href": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.5 Measuring spatial precision of field plot boundaries",
    "text": "10.5 Measuring spatial precision of field plot boundaries\nThe last three criteria of the EO-based quality control rely on satellite imagery analysis. The spatial precision of field plot boundaries requires visual image interpretation of high spatial resolution imagery that aligns with the survey timeframe. This type of control usually involves overlaying the plotted polygons onto frequent data, such as monthly cloud-free surface reflectance base maps. Figure 10.2 shows a good-quality sample that aligns well with a cultivated field, whereas the second sample area covers multiple fields and some trees. The ideal situation is to use high-resolution images to visually check samples and plot boundaries and then classify the areas with lower spatial resolutions. A possible situation is to use 4.8-meter Planet monthly reflectance maps for sample quality control and 10-meter Sentinel-2 images for classification.\nFrom an EO perspective, assessing sample quality requires time series from satellites such as Sentinel-2 for the growing season. Open-source platforms such as Sen4Stat and Sen2Agri toolbox [2] allow processing of all Sentinel-2 satellite images acquired along the season. One quantitative indicator of sample purity is the NDVI standard deviation (Figure 10.2, right plot) computed from the values of cloud-free satellite observations.\n\n\n\n\nFigure 10.2: Cloud-free Planet monthly base map images (left) and very high-resolution imagery (middle) overlaid with point observation expected to be representative of a circle area (radius of 20 m), as reported by the 2020 wheat rust survey in Ethiopia (Blasch et al., 2022). Plots highlight expected and unexpected NDVI profiles and the associated standard deviation for a homogeneous wheat field derived from the Sentinel-2 time series during the surveyed season.."
  },
  {
    "objectID": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "href": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.6 Using NDVI temporal profiles to assess crop phenology",
    "text": "10.6 Using NDVI temporal profiles to assess crop phenology\nThe last EO-based requirement is the most difficult to address. We assume that crops of the same type, grown in the same agro-climatic zone, have similar planting and growing cycles. Analysing NDVI profiles across all crop samples strengthens crop label confidence. Atypical growth patterns (e.g., varied planting/cycle length or lack of growth) indicate potential mislabeling or mislocated samples. These samples need more scrutiny before they can be deemed usable.\nConsider Ethiopia’s diverse crop cycles in Figure 10.3. Temporal profiles for barley, fava beans, and teff show outliers indicating marginal sample quality. The different NDVI profiles for maize samples reveal that a significant portion underwent a double cropping cycle within the observation period. Since the first crop cycle delayed planting, the sample population needs to be divided appropriately using clustering methods. The complexity of the wheat cropping cycle is heightened by sowing dates and varietal differences affecting cycle length. Thus, EO quality control aims to reject unsuitable samples for model calibration and output validation. Subsequent EO-derived results, like crop type maps, area estimates, and yield forecasts, critically depended on this quality control process.\n\n\n\n\nFigure 10.3: NDVI temporal profiles interpolated from cloud-free Sentinel-2 multispectral images acquired along the observation period. Each colour curve corresponds to a sample for a given crop, while the black curve is the average NDVI value of all samples for this crop. Teff is blue, wheat is red, barley is light green, peas are pink, fava beans are orange, and maise is dark green. The CIMMYT provided these samples in the framework of the ESA Sen4Rust partnership."
  },
  {
    "objectID": "th_design_frames.html#summary",
    "href": "th_design_frames.html#summary",
    "title": "\n10  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n10.7 Summary",
    "text": "10.7 Summary\nIn this chapter, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments. Following these guidelines will increase the accuracy of EO-based land use classification.\nReferences{-}\n\n\n\n\n[1] \nJ. Delincé et al., Handbook on remote sensing for agricultural statistics. FAO, 2017.\n\n\n[2] \nP. Defourny et al., “Near real-time agriculture monitoring at national scale at parcel resolution: Performance assessment of the Sen2-Agri automated system in various cropping systems around the world,” Remote Sensing of Environment, vol. 221, pp. 551–568, 2019, doi: 10.1016/j.rse.2018.11.007."
  }
]