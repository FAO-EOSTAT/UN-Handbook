[
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "UN Handbook on Remote Sensing for Agricultural Statistics",
    "section": "Intellectual property rights",
    "text": "Intellectual property rights\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0.\n\nDisclaimer\nYou are viewing a draft version of the UN Handbook. The final version is planned for November 2025."
  },
  {
    "objectID": "howto.html#section",
    "href": "howto.html#section",
    "title": "2  How to use this handbook",
    "section": "2.1 ",
    "text": "2.1"
  },
  {
    "objectID": "introduction.html#background",
    "href": "introduction.html#background",
    "title": "3  Introduction",
    "section": "3.1 Background",
    "text": "3.1 Background\nEarth Observation (EO) stands at a pivotal juncture in its capacity to transform agricultural statistics. For National Statistical Offices (NSOs) – our primary audience – this handbook arrives at a critical moment. The integration of EO into agricultural statistics is not just a technical opportunity; it is a policy imperative. As countries strive to meet Sustainable Development Goals (SDGs), ensure food security, and respond to climate volatility, the ability to generate timely, reliable, and spatially explicit agricultural data has become essential.\nWhile satellite data availability has expanded dramatically—most notably through the Copernicus Sentinel missions, which now provide near-daily global coverage at 10–20m resolution (ESA, 2023; Drusch et al., 2012)— the operational use of these resources by NSOs remains limited. This persistent gap is striking, given the extensive scientific literature showcasing advanced techniques for land cover and crop type mapping. Only 25–35% of national agricultural monitoring systems in Africa, South Asia, and Southeast Asia have operational EO-based capabilities for crop area and yield estimation—reflecting a notable gap between EO’s technical potential and its institutional adoption in developing economies (Whitcraft, A. K. et al., 2020)."
  },
  {
    "objectID": "introduction.html#improving-the-use-of-eo-in-agricultural-statistics",
    "href": "introduction.html#improving-the-use-of-eo-in-agricultural-statistics",
    "title": "3  Introduction",
    "section": "3.2 Improving the use of EO in agricultural statistics",
    "text": "3.2 Improving the use of EO in agricultural statistics\nThe reasons for the disconnect between EO’s potential and its adoption by NSOs are multifaceted. First, data abundance itself introduces complexity. Platforms like the Copernicus Data Space Ecosystem and Digital Earth Africa offer petabyte-scale Analysis-Ready Data (ARD), but turning this into actionable agricultural statistics requires high-performance computing and structured processing chains. While countries such as Poland (ESA, 2023; CloudFerro, 2025) and Brazil (Gomes et al, 2021) have established national HPC ecosystems to process continental-scale EO data cubes, many NSOs lack comparable resources.\nSecond, research-grade EO solutions often falter in operational settings. Deep learning models may achieve over 90% accuracy under experimental conditions (Rubwurm et al, 2024)), but their deployment in national programs is challenged by inconsistent training data, cloud interference, and the need to reconcile statistical rigor with time-sensitive reporting cycles (De Simone et al., 2025). Operational maturity demands more than algorithmic innovation—it requires adaptation, institutional readiness, and governance.\nTo meet these challenges, the global EO community has coalesced around building robust toolchains. ESA’s Sen4Stat platform—built on GDAL and incorporating Orfeo Toolbox components—offers a reproducible pipeline for crop mapping and yield forecasting (Bontemps et al., 2023). Similarly, Brazil’s SITS R package enables scalable time-series classification and is now deployed in Chile’s national land accounts (De Simone et al., 2025). But beyond individual tools, a more integrated coordination framework is emerging.\nAt the center of this effort is GEOGLAM (Group on Earth Observations Global Agricultural Monitoring)—an open international community of practice uniting the world’s leading programs in crop monitoring. GEOGLAM orchestrates a diverse set of platforms and toolboxes for agricultural monitoring such as GEOGLAM Crop Monitor, ESA Sen4Stat, ESA WorldCereal, NASA Harvest GLAM, JRC ASAP, Aircas CropWatch, FAO EOSTAT, FAO GIEWS, FAO WaPOR. These platforms and toolboxes collectively advance the capacity of countries to monitor agricultural production through shared data, methodological standardization, and transparency. Importantly, GEOGLAM is increasingly focused on strengthening the statistical rigor of crop monitoring—aligning more closely with NSO requirements for official area and yield estimation. This handbook complements that ambition, offering tools, case studies, and software that help bridge EO-based crop monitoring with statistical reporting.\nIn parallel, the Food and Agriculture Organization (FAO) has played a critical role in enabling country-level capacity development. The UN Task Team on Earth Observations for Agricultural Statistics, established under the UN Committee of Experts on Big Data and Data Science and the UN Expert Group on Rural, Agricultural and Food Security Statistics, has driven collective progress in EO adoption by NSOs. Through documented contributions to the UN Statistical Commission, the Task Team has promoted South–South cooperation, capacity building, and joint projects across regions. Recently, it has deepened collaboration with the UN Global Platform, particularly through increased engagement with the UN Big Data Regional Hubs. In this context, the Global Hub for Big Data in China has taken a leading role in supporting this handbook initiative as a key deliverable for operationalizing EO within national statistical systems (UNCEBD, 2025).\nAccuracy, in this context, is non-negotiable. As shown by Olofsson et al. (2024), classification errors above 15% can lead to over 20% distortion in area statistics—errors that compromise national reporting and policy formulation. This handbook addresses these issues across the full statistical workflow: from building ARD data cubes to implementing bias-corrected estimators, quantifying uncertainty, and generating statistically valid crop area measures. Real-world deployments, such as Chile’s 30m-resolution crop maps and Indonesia’s use of active learning to cut labeling costs by 40% while maintaining 90% detection accuracy, demonstrate the feasibility of integrating EO into official systems."
  },
  {
    "objectID": "introduction.html#purpose-of-this-handbook",
    "href": "introduction.html#purpose-of-this-handbook",
    "title": "3  Introduction",
    "section": "3.3 Purpose of this Handbook",
    "text": "3.3 Purpose of this Handbook\nDrawing on lessons from earlier efforts—including the 2017 FAO Handbook on Remote Sensing for Agricultural Statistics co-authored with Jacques Delincé (FAO, 2017)—this volume presents a practical, use-case-driven guide for EO adoption in statistical production. It reflects a significant paradigm shift from theoretical principles to field-tested applications.\nWhat sets this handbook apart is its emphasis on reproducibility and reusability. Each chapter includes executable R and Python scripts that allow NSOs to directly implement the methods discussed. Whether using Sen4Stat’s GDAL workflows, SITS classifiers, NASA Harvest’s ARYA crop yield model (Becker-Reshef et al., 2023), or PRESTO, a transformer-based model for time-series feature extraction (Tseng et al., 2024), the focus is on transparency and adaptability. These are not theoretical tools—they are field-tested systems embedded in national workflows.\nThe Handbook showcases diverse cases contributed by a wide range of national statistical offices and research teams, including crop monitoring with SAR images in Poland (GUS), crop classification in Mexico (INEGI), Senegal (DAPSA and FAO EOSTAT), Zimbabwe (ZIMSTAT and FAO EOSTAT), Chile (INE-Chile), and digital workflows supported by Digital Earth Africa. Yield estimation use cases span countries such as Finland (LUKE), Indonesia (BPS), Poland (GUS), Colombia (DANE and FEDEARROZ), Ukraine (University of Strasburg and Ukraine Statistics) and China (Institute of Remote Sensing and Digital Earth, Zhejiang University, Chinese Academy of Science). These contributions represent collaborative efforts involving institutions such as Université Catholique de Louvain (UCLouvain), University of Strasbourg (UNISTRA), FAO EOSTAT, Statistics Indonesia, IBGE, INEGI, and VITO, among others, alongside international research bodies and space agencies.\nEach contribution has been shaped through close cooperation between NSOs and geospatial experts. Their expertise, spanning the academic, operational, and policy realms, ensures that the methodologies described are both advanced and implementable.\nBeyond showcasing results, the Handbook places special emphasis on methodological components critical for statistical integrity. These include quality assessment of in situ data, evaluation of survey design, validation of classification models through confusion matrices, and estimation of area statistics corrected for classification bias. For instance, recent advances described by De Simone et al. (2025) propose rigorous quality control protocols for training data using self-organizing maps (SOM) and SMOTE-based sample balancing, offering a path to significantly higher classification accuracy.\nInnovation is also explored in chapters addressing frontier topics such as crop yield estimation (UNISTRA), field boundary mapping (e.g., IBGE), and the role of drones and EO in disaster risk reduction. Of relevance is the contribution on the WorldCereal project (Van Tricht et al., 2023), coordinated by VITO, which highlights the potential of self-supervised learning and global pretraining strategies for democratizing access to high-performance EO models in data-scarce regions. These developments reflect continuing validation efforts in operational EO workflows now being adopted in countries like Senegal and Zimbabwe.\nThe Handbook also acknowledges the evolution of toolboxes supporting these efforts. The Sen4Stat system, developed by UCLouvain with ESA support, has become a key asset for NSOs aiming to operationalize EO workflows for agricultural monitoring (Bontemps et al., 2024). Its uptake in Senegal and Zimbabwe illustrates its growing relevance. In a sense, the Handbook captures the evolution from research frontier to operational solutions. It represents the joint aspirations of NSOs, the geospatial science community, and international organizations to reimagine agricultural statistics for the era of big data and planetary monitoring. Throughout, the Handbook emphasizes transparency, trust, and standardization. These are not merely technical ideals but are foundational to the credibility of agricultural statistics in national and global decision-making. By promoting harmonized protocols and open-source tools, this volume seeks to foster a common language between statisticians and EO specialists, thereby enabling more consistent and comparable agricultural data worldwide.\nThe journey from satellite pixels to policy-relevant statistics is complex—but no longer aspirational. With 15 real-world use cases, field-tested methodologies, and ready-to-use software, this handbook aims to help NSOs move from exploration to execution, and from data collection to decision-making.\nWe invite readers to see this Handbook not only as a reference but as a launchpad. The time has come to embed EO firmly into the statistical systems that underpin food security, rural development, and environmental sustainability. Let this volume guide that journey—methodologically rigorous, globally informed, and grounded in practice."
  },
  {
    "objectID": "introduction.html#future-of-satellite-mission-and-its-relevance-to-nsos",
    "href": "introduction.html#future-of-satellite-mission-and-its-relevance-to-nsos",
    "title": "3  Introduction",
    "section": "3.4 Future of satellite mission and its relevance to NSOs",
    "text": "3.4 Future of satellite mission and its relevance to NSOs\nAs the EO landscape evolves, so too must national strategies. In 2025, NASA announced the discontinuation of Landsat Next—once intended to continue the legacy of the world’s longest-running EO mission. This unexpected restructuring highlights the fragility of EO continuity and the urgent need for risk mitigation. To this end, the handbook includes a dedicated chapter to help NSOs develop resilient, multi-source EO strategies that reduce dependency on any single mission or platform.\nFortunately, efforts are underway to secure long-term EO data availability. The European Space Agency, through its Sentinel Expansion and Copernicus Next Generation missions, is investing in hyperspectral, L-band radar, and thermal infrared sensors designed to ensure continuity and enhance capacity. These investments, aligned with GEOGLAM’s coordination, provide a sustainable pathway for countries to anchor EO in their statistical systems."
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "3  Introduction",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nThe journey from satellite pixels to policy-relevant statistics is complex—but no longer aspirational. With 15 real-world use cases, field-tested methodologies, and ready-to-use software, this handbook aims to help NSOs move from exploration to execution, and from data collection to decision-making."
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "3  Introduction",
    "section": "References",
    "text": "References\nBecker-Reshef, I., Barker, B., Humber, M., Hosseini, M., & Justice, C. (2023). NASA Harvest’s Open Crop Yield Models: ARYA Performance and Global Applications. Frontiers in Sustainable Food Systems. https://doi.org/10.3389/fsufs.2023.1194332\nDe Simone, L., Pizarro, E., Paredes, J., et al. (2025). Quality Control of Training Samples for Agricultural Statistics Using Earth Observation. Statistical Journal of the IAOS, 0(0). https://doi.org/10.1177/18747655251338033\nOlofsson, P., Foody, G. M., Herold, M., Stehman, S. V., Woodcock, C. E., & Wulder, M. A. (2014). Good Practices for Estimating Area and Assessing Accuracy of Land Change. Remote Sensing of Environment, 148, 42–57. https://doi.org/10.1016/j.rse.2014.02.015\nFAO. (2017). Handbook on Remote Sensing for Agricultural Statistics. Food and Agriculture Organization of the United Nations.\nUNCEBD. (2025). Report of the Task Team on Earth Observations for Agricultural Statistics to the UN Statistical Commission. United Nations Committee of Experts on Big Data.\nVan Tricht, K., Degerickx, J., Gilliams, S., et al. (2023). WorldCereal: a dynamic open-source system for global-scale, seasonal, and reproducible crop and irrigation mapping. Earth System Science Data, 15(12), 5491–5515. https://doi.org/10.5194/essd-15-5491-2023\nBontemps, S., Deffense, N., Nørgaard, B., & Defourny, P. (2024). Sen4Stat – Sentinels for Agricultural Statistics, D16.0 Concept Paper.\nESA, 2023. European Space Agency. Copernicus Sentinel Missions Overview. Available at: https://sentinels.copernicus.eu\nDrusch, M., et al., 2012. Sentinel-2: ESA’s Optical High-Resolution Mission for GMES Operational Services. Remote Sensing of Environment, 120, 25–36. https://doi.org/10.1016/j.rse.2011.11.026\nWhitcraft, A. K., Becker-Reshef, I., Killough, B., & Justice, C. O. (2020).Meeting Earth Observation Requirements for Global Agricultural Monitoring. Remote Sensing of Environment, 239, 111901. https://doi.org/10.1016/j.rse.2020.111901\nLesiv, M., et al., 2020. Estimating the Global Distribution of Field Size Using Crowdsourcing. Global Change Biology, 25(1), 174–186. https://doi.org/10.1111/gcb.14492\nCloudFerro. (2025, February 26). CloudFerro to provide NSIS-Cloud services for Poland [Press release]. CloudFerro. Retrieved from https://cloudferro.com/news/cloudferro-to-provide-nsis-cloud/:contentReferenceoaicite:0\nEuropean Space Agency. (2020, November 12). Sentinel data enables new system for agricultural monitoring in Poland. European Space Agency (ESA) – Observing the Earth: Copernicus News. Retrieved from https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel_data_enables_new_system_for_agricultural_monitoring_in_Poland\nGomes, V. C. F., Carlos, F. M., Queiroz, G. R., Ferreira, K. R., & Santos, R. (2021). Accessing and processing Brazilian earth observation data cubes with the Open Data Cube platform. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, V-4-2021, 153–159. https://doi.org/10.5194/isprs-annals-V-4-2021-153-2021:contentReferenceoaicite:2\nRußwurm, M., Wang, S., Kellenberger, B. et al. Meta-learning to address diverse Earth observation problems across resolutions. Commun Earth Environ 5, 37 (2024). https://doi.org/10.1038/s43247-023-01146-0\nTseng, G., Cartuyvels, L., Zvonkov, I., Purohit, S., Rolnick, D., & Kerner, H. (2024). Lightweight, pre-trained transformers for remote sensing timeseries (arXiv preprint arXiv:2304.14178). arXiv. https://arxiv.org/abs/2304.14178\nFoust, J. (2025, May 21). NASA’s budget crisis presents an opportunity for change. SpaceNews. https://spacenews.com/nasas-budget-crisis-presents-an-opportunity-for-change/?utm_source=chatgpt.com"
  },
  {
    "objectID": "theory.html#outline",
    "href": "theory.html#outline",
    "title": "Foundations",
    "section": "Outline",
    "text": "Outline\nThis is the introduction to the foundations of the book."
  },
  {
    "objectID": "th_remote_sensing.html#outline",
    "href": "th_remote_sensing.html#outline",
    "title": "4  Remote Sensing images: optical, SAR",
    "section": "4.1 Outline",
    "text": "4.1 Outline\nThis chapter provides a general introduction to remote sensing imagery. The authors discuss different types of remote sensing satellites (optical, SAR, hyperspectral) and include examples of satellite constellations (e.g, Landsat, Sentinel-1, CHEOS)."
  },
  {
    "objectID": "th_lucc.html#introduction",
    "href": "th_lucc.html#introduction",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nSatellite images are the most comprehensive source of data about our environment; they provide essential information on global challenges. Images provide information for measuring deforestation, crop production, food security, urban footprints, water scarcity, land degradation, among other uses. In recent years, space agencies have adopted open distribution policies. Petabytes of Earth observation data are now available. Experts now have access to repeated acquisitions over the same areas; the resulting time series improve our understanding of ecological patterns and processes[1]. Instead of selecting individual images from specific dates and comparing them, researchers can track changes continuously[2]. To handle big data, scientists are developing new algorithms for image time series (for recent surveys, see [5]). These methods are data-driven and theory-limited. However, numbers do not speak for themselves [6]. Data-driven approaches without solid theories can lead to results which will not increase our knowledge [7].\nConsider how experts use Earth observation data. Their input are images with resolution ranging from 5 to 500 meters, produced by satellites such as Landsat, Sentinels-1/2/3, and CBERS-4. To extract information, experts use methods that assign a label to each pixel (e.g., ‘grasslands’). Labels can represent either land cover or land use. Land cover is the observed biophysical cover of the Earth’s surface; land use concepts describe socio-economic activities\\cite[8]. Thus, forest' is a type of land cover, whilecorn plantation’ is a kind of land use. To support land classification, scientists have proposed ontologies and descriptive schemes [9]. We might thus ask: Are the current classification systems suitable to represent land change when working with big data? If not, which concepts are needed and how should they be applied?\nIn what follows, we present the prevailing consensus on classification systems: FAO’s Land Cover Classification System (LCCS)\\cite[10]. We argue that LCCS does not meet the challenges posed by big data. To support our views, we consider concepts used on image time series analysis; we show these concepys are related to event recognition and are not representable in LCCS. To improve the theory behind big data, we introduce elements of a phenology based approach for land classification."
  },
  {
    "objectID": "th_lucc.html#classification-systems-for-earth-observation-data-current-status",
    "href": "th_lucc.html#classification-systems-for-earth-observation-data-current-status",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.2 Classification systems for Earth observation data: current status",
    "text": "5.2 Classification systems for Earth observation data: current status\nThe act of classification raises philosophical questions dating as far back as Aristotle. We use an a priori conception of reality to classify the world; what we observe has to fit our categories. Words in our language describe elements of the external reality. However, geographical terms such as mountain' andriver’ are imprecise and context-dependent[13]. These ambiguities have motivated research on geospatial semantics [16]. However, building such complete ontologies is hard. Janowicz et al. [17] recognize that “geographical concepts are situated and context-dependent, can be described from different, equally valid, points of view, and ontological commitments are arbitrary to a large extent”. Work on classification systems has shifted. Rather than using a single ontology, the current consensus argues for domain ontologies based on a common foundational ontology. These domain ontologies are means of making concepts of specific disciplines explicit and better communicating them [19].\nThe semantics of Earth observation data are constrained by classification systems. Experts agree on what are the possible descriptions of the objects in the image (e.g., “forest”, “river”, “pasture”). Each pixel of the image is then labeled using visual or automated interpretation. As an example, for countries reporting greenhouse gas inventories, the International Panel of Climate Change (IPCC) restricts the top-level land classes to ‘forest’, ‘cropland’, ‘grassland’, ‘wetlands’, ‘settlements’, and ‘others’. This approach is too simplistic. Sasaki and Putz [20] criticize the IPCC base classes for inducing wrong assessments for ecological and biodiversity conservation. The IPCC classes are an example where pre-conceived rules collide with the diversity of the world’s ecosystems.\nSince land classification provides essential information about our environment, many GIScience researchers have addressed the subject of land use and land cover semantics [23]. They investigated consistency of classification systems [24], semantic similarity between terms used by different systems [25], and disagreements between results [26]. The current consensus favors ontologies aiming at unambiguous definitions of land cover classes, such as the FAO Land Cover Classification System (LCCS) [10]. For this reason, it is important to discuss whether LCCS works well with big EO data.\nFAO has developed the Land Cover Classification System (LCCS) “to provide a consistent framework for the classification and mapping of land cover” [27]. LCCS is a hierarchical system. At its highest level, LCCS has eight major land cover types:\n\nCultivated and managed terrestrial areas.\nNatural and semi-natural terrestrial vegetation.\nCultivated aquatic or regularly flooded areas.\nNatural and semi-natural aquatic or regularly flooded vegetation.\nArtificial surfaces and associated areas.\nBare areas.\nArtificial water bodies, snow, and ice.\nNatural water bodies, snow, and ice.\n\nThe division on eight classes considers three criteria: presence of vegetation, edaphic conditions, and artificiality of cover [27]. Specialization of top-level LCCS classes uses properties such as life form, tree height, and vegetation density, setting pre-defined limits (e.g., “tree height &gt; 10 meters”). These subdivisions are ad hoc and application-dependent, leading to a combinational explosion with dozens or even hundreds of subclasses [10]. Such high expressive power can lead to incompatible LCCS-based class hierarchies [[24]}.\nLCCS is a landmark initiative; it provides a basis for a common understanding of land cover concepts. Many global and regional land mapping products use LCCS, including GLOBCOVER [28] and ESA CCI Land Cover [29]. However, LCCS makes assumptions which limit its use with big data:\n\nLCCS describes land properties based only on land cover types, disregarding land use. For example, LCCS does not distinguish pasture' fromnatural grasslands’; it labels both as herbaceous land cover types.\nThe LCCS hierarchy uses hard boundaries between its subclasses. At each level of the hierarchy, properties of subclasses use fixed values (e.g., “sparse forests have between 10% and 30% of trees”). Real-world class boundaries do not fit into such strict definitions. When doing data analysis with machine learning, boundaries between classes are data-dependent and cannot be set a priori [30].\nClassification in LCCS has no temporal reference. LCCS assumes that subtype properties (e.g., percent of tree cover) are detectable at the moment of classification. These properties do not refer to past or future values. Land use and land cover types whose values require time references (e.g., “forest land cleared in the last decade”) are not representable in LCCS.\n\nFor example, the UNFCCC Reduction of Emissions by Deforestation and Degradation initiative (REDD+) requires representing and measuring forest dynamics [31]. Static and rigid definitions of “forest” used by LCCS cannot represent concepts such as `forest degradation’ [32]. Forest degradation happens when a natural forest loses part of its biodiversity and its tree cover. It is not a stable state but an intermediary situation that can lead to different medium-term outcomes. One can restore a degraded forest; degradation may continue and lead to complete loss of forest cover. Whatever the case, LCCS lacks explicit temporal information to capture forest degradation and thus support initiatives such as REDD+. Therefore, LCCS is thus not fit for many critical applications of EO data."
  },
  {
    "objectID": "th_lucc.html#elements-of-a-phenology-based-classification-schemas",
    "href": "th_lucc.html#elements-of-a-phenology-based-classification-schemas",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.3 Elements of a phenology-based classification schemas",
    "text": "5.3 Elements of a phenology-based classification schemas\nTo represent change in geographical space, GIScience authors distinguish between continuants and occurrents [36]. Continuants refer to entities that “endure through time even while undergoing different sorts of changes” [33]. The Amazon Forest and the city of Brasilia are continuants. Occurrents happen in a well-defined period and may have different stages during this time. Cutting down a forest area, cultivating a crop in a season, and building a road are occurrents. Objects are associated to continuants and events to occurrents.\nAtemporal classification systems such as LCCS refer only to properties of continuants. One can state facts such as “this area has 30% forest cover” using LCCS, but cannot assert that “this area lost 70% of its forest in the last two years”. To convey change, classification systems for big data need to include occurrents. In what follows, we discuss concepts used in the analysis of satellite image time series. These time series are extracted from organized collections of Earth observation data covering a geographical area in regular temporal intervals."
  },
  {
    "objectID": "th_lucc.html#the-key-role-of-time-series",
    "href": "th_lucc.html#the-key-role-of-time-series",
    "title": "\n5  Land cover and crop classification schemas\n",
    "section": "\n5.4 The key role of time series",
    "text": "5.4 The key role of time series\nSince remote sensing satellites revisit the same place, we can calibrate their images so that measures of the same place at different times are comparable (Figure \\(\\ref{fig:sits_a}\\)). These observations can be organized so that each measure from the sensor maps to a three-dimensional array in space-time. From a data analysis perspective, each pixel location \\((x, y)\\) at consecutive times, \\(t_1,...,t_m\\), makes up a satellite image time series (SITS), such as the one in Figure \\(\\ref{fig:sits_b}\\). From these time series, we can extract land-use and land-cover change information. In Figure \\(\\ref{fig:sits_b}\\), after the forest was cut in 2002, the area was used for cattle raising (pasture) for three years, during 2002 to 2008, then turned into cropland.\n\n\n\n\nFigure 5.1: Time series measures (EVI index) of a pixel location \\((x,y)\\) (source:[37]).\n\n\n\n\n\n\n\nFigure 5.2: Land change events associated to a pixel location \\((x,y)\\) (source:[37]).\n\n\n\nReferences{-}\n\n\n\n\n[1] \nV. J. Pasquarella, C. E. Holden, L. Kaufman, and C. E. Woodcock, “From imagery to ecology: Leveraging time series of all available LANDSAT observations to map and monitor ecosystem state and dynamics,” Remote Sensing in Ecology and Conservation, vol. 2, no. 3, pp. 152–170, 2016, doi: 10.1002/rse2.24.\n\n\n[2] \nC. E. Woodcock, T. R. Loveland, M. Herold, and M. E. Bauer, “Transitioning from change detection to monitoring with remote sensing: A paradigm shift,” Remote Sensing of Environment, vol. 238, p. 111558, 2020, doi: 10.1016/j.rse.2019.111558.\n\n\n[3] \nC. Gomez, J. C. White, and M. A. Wulder, “Optical remotely sensed time series data for land cover classification: A review,” {ISPRS} Journal of Photogrammetry and Remote Sensing, vol. 116, pp. 55–72, 2016, doi: 10.1016/j.isprsjprs.2016.03.008.\n\n\n[4] \nZ. Zhu, “Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 130, pp. 370–384, 2017, doi: 10.1016/j.isprsjprs.2017.06.013.\n\n\n[5] \nL. Zeng, B. D. Wardlow, D. Xiang, S. Hu, and D. Li, “A review of vegetation phenological metrics extraction using time-series, multispectral satellite data,” Remote Sensing of Environment, vol. 237, p. 111511, 2020, doi: 10.1016/j.rse.2019.111511.\n\n\n[6] \ndanah boyd and K. Crawford, “Critical Questions for Big Data,” Information, Communication & Society, vol. 15, no. 5, pp. 662–679, 2012, doi: 10.1080/1369118X.2012.678878.\n\n\n[7] \nR. Kitchin, “Big Data, new epistemologies and paradigm shifts,” Big Data & Society, vol. 1, no. 1, p. 2053951714528481, 2014, doi: 10.1177/2053951714528481.\n\n\n[8] \nA. Comber, “The separation of land cover from land use using data primitives,” Journal of Land Use Science, vol. 3, no. 4, pp. 215–229, 2008, doi: https:/doi.org/10.1080/17474230802465173.\n\n\n[9] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009.\n\n\n[10] \nM. Herold et al., “A joint initiative for harmonization and validation of land cover datasets,” IEEE Transactions on Geoscience and Remote Sensing, vol. 44, no. 7, pp. 1719–1727, 2006, doi: 10.1109/TGRS.2006.871219.\n\n\n[11] \nB. Smith and D. M. Mark, “Geographical categories: An ontological investigation,” International Journal of Geographical Information Science, vol. 15, no. 7, pp. 591–612, 2001, doi: 10.1080/13658810110061199.\n\n\n[12] \nB. Smith and D. M. Mark, “Do Mountains Exist? Towards an Ontology of Landforms,” Environment and Planning B: Planning and Design, vol. 30, no. 3, pp. 411–427, 2003, doi: 10.1068/b12821.\n\n\n[13] \nD. M. Mark and A. G. Turk, “Landscape Categories in Yindjibarndi: Ontology, Environment, and Language,” in Spatial Information Theory. Foundations of Geographic Information Science, 2003, pp. 28–45, doi: 10.1007/978-3-540-39923-0_3.\n\n\n[14] \nB. Smith and D. M. Mark, “Ontology and Geographic Kinds,” 1998, [Online]. Available: https://philarchive.org.\n\n\n[15] \nF. Fonseca, M. Egenhofer, C. Davis, and G. Câmara, “Semantic Granularity in Ontology-Driven Geographic Information Systems,” Annals of Mathematics and Artificial Intelligence, vol. 36, no. 1, pp. 121–151, 2002, doi: 10.1023/A:1015808104769.\n\n\n[16] \nW. Kuhn, “Geospatial Semantics: Why, of What, and How?” Journal on Data Semantics, vol. 3, pp. 1–24, 2005, doi: 10.1007/11496168_1.\n\n\n[17] \nK. Janowicz, S. Scheider, T. Pehle, and G. Hart, “Geospatial semantics and linked spatiotemporal data – Past, present, and future,” Semantic Web, vol. 3, no. 4, pp. 321–332, 2012, doi: 10.3233/SW-2012-0077.\n\n\n[18] \nB. Smith et al., “The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration,” Nature Biotechnology, vol. 25, no. 11, pp. 1251–1255, 2007, doi: 10.1038/nbt1346.\n\n\n[19] \nP. L. Buttigieg, N. Morrison, B. Smith, C. J. Mungall, S. E. Lewis, and the ENVO Consortium, “The environment ontology: Contextualising biological and biomedical entities,” Journal of Biomedical Semantics, vol. 4, no. 1, p. 43, 2013, doi: 10.1186/2041-1480-4-43.\n\n\n[20] \nN. Sasaki and F. E. Putz, “Critical need for new definitions of ‘forest’ and ‘forest degradation’ in global climate change agreements,” Conservation Letters, vol. 2, no. 5, pp. 226–232, 2009, doi: 10.1111/j.1755-263X.2009.00067.x.\n\n\n[21] \nA. Comber, P. Fisher, and R. Wadsworth, “What is Land Cover?” Environment and Planning B: Planning and Design, vol. 32, no. 2, pp. 199–209, 2005, doi: 10.1068/b31135.\n\n\n[22] \nO. Ahlqvist, “Using uncertain conceptual spaces to translate between land cover categories,” International Journal of Geographical Information Science, vol. 19, no. 7, pp. 831–857, 2005, doi: 10.1080/13658810500106729.\n\n\n[23] \nO. Ahlqvist, D. Varanka, S. Fritz, and K. Janowick, Eds., Land Use and Land Cover Semantics: Principles, Best Practices, and Prospects. CRC Press, 2017.\n\n\n[24] \nL. J. M. Jansen, G. Groom, and G. Carrai, “Land-cover harmonisation and semantic similarity: Some methodological issues,” Journal of Land Use Science, vol. 3, no. 2–3, pp. 131–160, 2008, doi: 10.1080/17474230802332076.\n\n\n[25] \nC.-C. Feng and D. M. Flewelling, “Assessment of semantic similarity between land use/land cover classification systems,” Computers, Environment and Urban Systems, vol. 28, no. 3, pp. 229–246, 2004, doi: 10.1016/S0198-9715(03)00020-6.\n\n\n[26] \nS. Fritz et al., “Highlighting continued uncertainty in global land cover maps for the user community,” Environmental Research Letters, vol. 6, no. 4, p. 044005, 2011, doi: 10.1088/1748-9326/6/4/044005.\n\n\n[27] \nA. Di Gregorio, “Land Cover Classification System - Classification concepts Software version 3,” FAO, 2016.\n\n\n[28] \nO. Arino et al., “GlobCover: ESA service for global land cover from MERIS,” in 2007 IEEE International Geoscience and Remote Sensing Symposium, 2007, pp. 2412–2415, doi: 10.1109/IGARSS.2007.4423328.\n\n\n[29] \nW. Li, P. Ciais, N. MacBean, S. Peng, P. Defourny, and S. Bontemps, “Major forest changes and land cover transitions based on plant functional types derived from the ESA CCI Land Cover product,” International Journal of Applied Earth Observation and Geoinformation, vol. 47, pp. 30–39, 2016, doi: 10.1016/j.jag.2015.12.006.\n\n\n[30] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[31] \nE. Corbera and H. Schroeder, “Governing and implementing REDD+,” Environmental Science & Policy, vol. 14, no. 2, pp. 89–99, 2011, doi: 10.1016/j.envsci.2010.11.002.\n\n\n[32] \nF. E. Putz and K. H. Redford, “The Importance of Defining ‘Forest’: Tropical Forest Degradation, Deforestation, Long-term Phase Shifts, and Further Transitions,” Biotropica, vol. 42, no. 1, pp. 10–20, 2010, doi: 10.1111/j.1744-7429.2009.00567.x.\n\n\n[33] \nP. Grenon and B. Smith, “SNAP and SPAN: Towards Dynamic Spatial Ontology,” Spatial Cognition & Computation, vol. 4, no. 1, pp. 69–104, 2004, doi: 10.1207/s15427633scc0401_5.\n\n\n[34] \nA. Galton, “Fields and Objects in Space, Time, and Space-time,” Spatial Cognition & Computation, vol. 4, no. 1, pp. 39–68, 2004, doi: 10.1088/1748-9326/6/4/04400510.1207/s15427633scc0401_4.\n\n\n[35] \nA. Galton, “Experience and History: Processes and their Relation to Events,” Journal of Logic and Computation, vol. 18, no. 3, pp. 323–340, 2008, doi: 10.1088/1748-9326/6/4/04400510.1093/logcom/exm079.\n\n\n[36] \nM. Worboys, “Event-oriented approaches to geographic phenomena,” International Journal of Geographical Information Science, vol. 19, no. 1, pp. 1–28, 2005, doi: 10.1080/13658810412331280167.\n\n\n[37] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007."
  },
  {
    "objectID": "th_quality_control.html#outline",
    "href": "th_quality_control.html#outline",
    "title": "6  Quality control of training sets for agricultural statistics",
    "section": "6.1 Outline",
    "text": "6.1 Outline\nSelecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has shown that the number and quality of training samples are crucial factors in obtaining accurate results. This chapter presents pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labelled or possess low discriminatory power. Explains the basics of machine learning and provides examples of designing and using good training sets. Explains k-fold validation, SOM clustering and sample imbalance removal, with examples in R and Python."
  },
  {
    "objectID": "th_machine_learning.html#outline",
    "href": "th_machine_learning.html#outline",
    "title": "7  Machine learning classification of remote sensing images",
    "section": "7.1 Outline",
    "text": "7.1 Outline\nThis chapter describes machine learning methods for classifying individual remote sensing images and image time series. The chapter considers three kinds of algorithms: • Machine learning algorithms that do not explicitly consider the spatial and temporal structure of the time series. These methods include random forests, support vector machine and extreme gradient boosting. • Deep learning methods which consider temporal relations between observed values in a time series. This class of models includes 1D convolutional neural networks and temporal attention-based encoders. • Semantic segmentation methods based on U-net paradigms and multidimensional 2D convolution."
  },
  {
    "objectID": "th_uncertainty.html#outline",
    "href": "th_uncertainty.html#outline",
    "title": "8  Spatial map uncertainty estimation and active learning in crop classification",
    "section": "8.1 Outline",
    "text": "8.1 Outline\nDescribes methods for estimating uncertainty of machine learning classification maps and how to use such estimates to improve classification accuracy. Map uncertainty refers to the degree of doubt or ambiguity in the accuracy of each pixel of the classification results. Several sources of uncertainty can arise during land classification using satellite data, including: a) classification errors; b) ambiguity in classification schema; c) variability in the landscape; and d) limitations of the data. The quality and quantity of input data can influence the accuracy of the classification results. Quantifying uncertainty in land classification is important for ensuring that the results are reliable and valid for decision-making."
  },
  {
    "objectID": "th_validation.html#outline",
    "href": "th_validation.html#outline",
    "title": "9  Map validation and use of maps for area estimation",
    "section": "9.1 Outline",
    "text": "9.1 Outline\nStatistically robust and transparent approaches for assessing accuracy are essential parts of the land classification process. The sits package supports the “good practice” recommendations for designing and implementing an accuracy assessment of a change map and estimating the area based on reference sample data. These recommendations address three components: sampling design, reference data collection, and accuracy estimates."
  },
  {
    "objectID": "th_data_sources.html#ard-image-collections",
    "href": "th_data_sources.html#ard-image-collections",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.1 ARD Image Collections",
    "text": "10.1 ARD Image Collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD images processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and are provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organised as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images of an ARD collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure 10.1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 10.1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "href": "th_data_sources.html#cloud-platforms-providing-ard-data",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.2 Cloud platforms providing ARD data",
    "text": "10.2 Cloud platforms providing ARD data\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes.\nThere are a large number of cloud platforms providing open data organized as Analysis-Ready Data, including: Amazon Web Services (AWS), Brazil Data Cube (BDC), Copernicus Data Space Ecosystem (CDSE), Digital Earth Africa (DEAFRICA), Digital Earth Australia (DEAUSTRALIA), Microsoft Planetary Computer (MPC), and Nasa Harmonized Landsat/Sentinel (HLS),\nThis chapter describes how to access these collections and transform ARD images into regular data cubes. A data cube is a set of images organized in tiles of a grid system (e.g., MGRS). Each tile contains single-band images in a unique zone of the coordinate system (e.g, tile 20LMR in MGRS grid) covering the period between start_date and end_date. All tiles share the same set of regular temporal intervals and the same spectral bands and indices. All images have the same spatial resolution."
  },
  {
    "objectID": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "href": "th_data_sources.html#tiling-systems-used-by-ard-collections",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.3 Tiling systems used by ARD collections",
    "text": "10.3 Tiling systems used by ARD collections\nARD image collections are organized in spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone has blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure Figure 10.2 shows the MGRS tiling system for a part of the Northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 10.2: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which breaks the coverage of Landsat satellites into images identified by path and row (see Figure @ref(fig:wrs)). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 10.3: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "href": "th_data_sources.html#major-global-or-large-regional-cloud-provides",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.4 Major global or large regional cloud provides",
    "text": "10.4 Major global or large regional cloud provides\nThe following ARD image cloud providers provide global or large regional data:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A level 2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products available include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedian of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are Copernicus DEM-30 and MOD13Q1, MOD10A1 and MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30)."
  },
  {
    "objectID": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "th_data_sources.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.5 Accessing ARD image collections in cloud providers",
    "text": "10.5 Accessing ARD image collections in cloud providers\n\nWe now present the use of the sits R package to obtain information on ARD image collection from cloud providers, using the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information which many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured in a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections in cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be assessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "th_data_sources.html#amazon-web-services",
    "href": "th_data_sources.html#amazon-web-services",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.6 Amazon Web Services",
    "text": "10.6 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports collection SENTINEL-2-L2A which is open data. The bands in 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n\n\n\n\nFigure 10.4: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#microsoft-planetary-computer",
    "href": "th_data_sources.html#microsoft-planetary-computer",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.7 Microsoft Planetary Computer",
    "text": "10.7 Microsoft Planetary Computer\nThe sits package supports access to open data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30 meter resolution) and MOD13Q1-6.1(version 6.1 of the MODIS MOD13Q1 product). Access to the non-open data collection is available for users that have registration in MPC.\n\n10.7.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC = sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\n\n\n\nFigure 10.5: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n10.7.2 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from Landsat-4/5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasilia in Brazil.\n\n\nR\nPython\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi = {\"lon_min\" : -43.5526, \"lat_min\" : -2.9644, \n         \"lon_max\" : -42.5124, \"lat_max\" : -2.1671}\n# Select the cube\ns2_L8_cube_MPC = sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = [\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"],\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\n\n\n\nFigure 10.6: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n10.7.3 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject for variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line-of-sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be re-projected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso Brazil that is an area of the Amazon forest that has been deforested. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear included in relation to the Earth’s longitude.\n\n\nR\nPython\n\n\n\n\ncube_s1_grd &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_grd =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = [\"VV\"],\n  orbit = \"descending\",\n  tiles = [\"21LUJ\",\"21LVJ\"],\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 10.7: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain corrected) images as described in the next session.\n\n10.7.4 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric distortions and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of the surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and re-project the SAR image from the slant range (radar line-of-sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\n\nR\nPython\n\n\n\n\ncube_s1_rtc &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_rtc =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = [\"VV\", \"VH\"],\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 10.8: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes mountains. Users are invited to compare this images with the one from the SENTINEL-1-GRD collection and see the significant geometrical distortions of the GRD image compared with the RTC one.\n\n10.7.5 Copernicus DEM 30 meter images in MPC\nThe Copernicus digital elevation model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, designed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30 meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both case, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\n\nR\nPython\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\n\n\n\n\nFigure 10.9: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "th_data_sources.html#brazil-data-cube",
    "href": "th_data_sources.html#brazil-data-cube",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.8 Brazil Data Cube",
    "text": "10.8 Brazil Data Cube\nThe Brazil Data Cube (BDC) is built by Brazil’s National Institute for Space Research (INPE), to provide regular EO data cubes from CBERS, LANDSAT, SENTINEL-2, and TERRA/MODIS satellites for environmental applications. The collections available in the BDC are: LANDSAT-OLI-16D (Landsat-8 OLI, 30 m resolution, 16-day intervals), SENTINEL-2-16D (Sentinel-2A and 2B MSI images at 10 m resolution, 16-day intervals), CBERS-WFI-16D (CBERS 4 WFI, 64 m resolution, 16-day intervals), CBERS-WFI-8D(CBERS 4 and 4A WFI images, 64m resolution, 8-day intervals), and MOD13Q1-6.1 (MODIS MOD13SQ1 product, collection 6, 250 m resolution, 16-day intervals). For more details, use sits_list_collections(source = \"BDC\").\nThe BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The large grid has tiles of 4224.4 \\(\\times4\\) 224.4 km2 and is used for CBERS-4 AWFI collections at 64 m resolution; each CBERS-4 AWFI tile contains images of 6600 \\(\\times\\) 6600 pixels. The medium grid is used for Landsat-8 OLI collections at 30 m resolution; tiles have an extension of 211.2 \\(\\times\\) 211.2 km2, and each image has 7040 \\(\\times\\) 7040 pixels. The small grid covers 105.6 \\(\\times\\) 105.6 km2 and is used for Sentinel-2 MSI collections at 10 m resolutions; each image has 10560 \\(\\times\\) 10560 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [1].\n\n\n\n\nFigure 10.10: Hierarchical BDC tiling system showing (a) large BDC grid overlayed on Brazilian biomes, (b) one learge tile from the grid used for CBERS-4 AWFI data, (c) four medium tiles from the grid used for LANDSAT data; and (d) sixteen small tiles from the grid used for SENTINEL-2 data. Tiles in (b), (c), and (d) are nested.\n\n\n\nTo access the BDC, users must provide their credentials using environment variables, as shown below. Obtaining a BDC access key is free. Users must register at the BDC site to obtain a key. Please include your BDC access key in your “.Rprofile”.\n\nSys.setenv(BDC_ACCESS_KEY = \"&lt;your_bdc_access_key&gt;\")\n\nIn the example below, the data cube is defined as one tile (“005004”) of CBERS-WFI-16D collection, which holds CBERS AWFI images at 16 days resolution.\n\n\nR\nPython\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = c(\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile = sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = [\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"],\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n\n\n\n\nFigure 10.11: CBERS-4 WFI image in a Cerrado area in Brazil."
  },
  {
    "objectID": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "href": "th_data_sources.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.9 Copernicus Data Space Ecosystem (CDSE)",
    "text": "10.9 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CSDE S3 credentials site. The site will request you to add a new credential. You will receive two keys: an an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n      AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n      AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nAfter including these lines in your .Rprofile, restart R for the changes to take effect. By following these steps, users will have access to the Copernicus Data Space Ecosystem.\n\n10.9.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena river delta in Siberia in summertime.\n\n\nR\nPython\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = [\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = \"52XDF\"\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 10.12: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n10.9.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE are the radiometric terrain corrected (RTC) Sentinel-1 images. in CDSE, this product is referred to as normalized terrain backscater (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ0) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set-up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift valley in Ethiopia.\n\n\nR\nPython\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = [\"VV\", \"VH\"],\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = \"37NCH\"\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 10.13: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "th_data_sources.html#digital-earth-africa",
    "href": "th_data_sources.html#digital-earth-africa",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.10 Digital Earth Africa",
    "text": "10.10 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level 2A (SENTINEL-2-L2A), organised as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardised NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California in Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30 meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in grid system WRS-2.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done wither using tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tiles (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\n\nR\nPython\n\n\n\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n    lon_min = 46.1, lat_min = -15.6,\n    lon_max = 46.6, lat_max = -16.1\n  ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\ndea_s2_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = {\n    \"lon_min\" : 46.1, \"lat_min\" : -15.6,\n    \"lon_max\" : 46.6, \"lat_max\" : -16.1\n  },\n    bands = [\"B02\", \"B04\", \"B08\"],\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\n\n\n\nFigure 10.14: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data, covering the Serengeti plain in Tanzania.\n\n\nR\nPython\n\n\n\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\ndea_l9_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = {\n        \"lon_min\" : 33.0, \"lat_min\" : -3.60, \n        \"lon_max\" : 33.6, \"lat_max\" : -3.00\n    },\n    bands = [\"B04\", \"B05\", \"B06\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 10.15: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for year 2020, for an area near the border between Congo and Rwanda.\n\n\nR\nPython\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = {\n       \"lon_min\" : 28.69, \"lat_min\" : -2.35, \n       \"lon_max\" : 29.35, \"lat_max\" : -1.56\n    },\n    bands = [\"HH\", \"HV\"],\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\n\n\n\nFigure 10.16: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "th_data_sources.html#digital-earth-australia",
    "href": "th_data_sources.html#digital-earth-australia",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.11 Digital Earth Australia",
    "text": "10.11 Digital Earth Australia\nDigital Earth Australia (DEAUSTRALIA) is an initiative by Geoscience Australia that uses satellite data to monitor and analyze environmental changes and resources across the Australian continent. It provides many datasets that offer detailed information on phenomena such as droughts, agriculture, water availability, floods, coastal erosion, and urban development. The DEAUSTRALIA image collections in sits are:\n\nGA_LS5T_ARD_3: ARD images from Landsat-5 satellite, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, and “CLOUD”.\nGA_LS7E_ARD_3: ARD images from Landsat-7 satellite, with the same bands as Landsat-5.\nGA_LS8C_ARD_3: ARD images from Landsat-8 satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, “PANCHROMATIC”, and “CLOUD”.\nGA_LS9C_ARD_3: ARD images from Landsat-9 satellite, with the same bands as Landsat-8.\nGA_S2AM_ARD_3: ARD images from Sentinel-2A satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “RED-EDGE-1”, “RED-EDGE-2”, “RED-EDGE-3”, “NIR-1”, “NIR-2”, “SWIR-2”, “SWIR-3”, and “CLOUD”.\nGA_S2BM_ARD_3: ARD images from Sentinel-2B satellite, with the same bands as Sentinel-2A.\nGA_LS5T_GM_CYEAR_3: Landsat-5 geomedian images, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR1”, “SWIR2”, “EDEV”, “SDEV”, “BCDEV”.\nGA_LS7E_GM_CYEAR_3: Landsat-7 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS8CLS9C_GM_CYEAR_3: Landsat-8/9 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS_FC_3: Landsat fractional land cover, with bands “BS”, “PV”, “NPV”.\nGA_S2LS_INTERTIDAL_CYEAR_3: Landsat/Sentinel intertidal data, with bands “ELEVATION”, “ELEVATION-UNCERTAINTY”, “EXPOSURE”, “TA-HAT”, “TA-HOT”, “TA-LOT”, “TA-LAT” “TA-OFFSET-HIGH”, “TA-OFFSET-LOW”, “TA-SPREAD”, “QA-NDWI-CORR”and “QA-NDWI-FREQ”.\n\nThe following code retrieves an image from Sentinel-2A.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_55KGR &lt;- sits_mgrs_to_roi(\"55KGR\")\n# retrieve the world cover map for the chosen roi\ns2_56KKV &lt;- sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n# retrieve the world cover map for the chosen tile\ns2_56KKV = sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"],\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n\n\n\n\nFigure 10.17: Sentinel-2A image from the DEAUSTRALIA collection showing MGRS tile 56KKV."
  },
  {
    "objectID": "th_data_sources.html#harmonized-landsat-sentinel",
    "href": "th_data_sources.html#harmonized-landsat-sentinel",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.12 Harmonized Landsat-Sentinel",
    "text": "10.12 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS tiling system is identical as the one used for Sentinel-2 (MGRS). The tiles dimension is 109.8 km and there is an overlap of 4,900 m on each side.\nTo access NASA HLS, users need to registed at NASA EarthData, and save their login and password in a ~/.netrc plain text file in Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with the earthdatalogin::edl_netrc. This function creates a properly configured .netrc file in the user’s home directory and an environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections which are available in Microsoft Planetary Computer, which are a copy of the NASA collections and are faster to access.\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \nusername = \"&lt;your user name&gt;\", \npassword = \"&lt;your password&gt;\" \n) \n\nAccess to images in NASA HLS is done by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n# define a region of interest\nroi = { \"lon_min\" : -45.6422, \"lat_min\" : -24.0335,\n        \"lon_max\" : -45.0840, \"lat_max\" : -23.6178 }\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"],\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\n\n\n\nFigure 10.18: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "th_data_sources.html#eo-products-from-terrascope",
    "href": "th_data_sources.html#eo-products-from-terrascope",
    "title": "\n10  EO Big Data Sources\n",
    "section": "\n10.13 EO products from TERRASCOPE",
    "text": "10.13 EO products from TERRASCOPE\nTerrascope is online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of Earth observation data and processing services that are accessible free of charge. Currently, sits supports the World Cover 2021 maps, produced by VITO with support form the European Commission and ESA. The following code shows how to access the World Cover 2021 convering tile “22LBL”. The first step is to use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_r\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_py\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\n\n\n\nFigure 10.19: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "th_data_sources.html#references",
    "href": "th_data_sources.html#references",
    "title": "\n10  EO Big Data Sources\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nK. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033."
  },
  {
    "objectID": "th_design_frames.html#outline",
    "href": "th_design_frames.html#outline",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.1 Outline",
    "text": "11.1 Outline\nThis chapter deals with the requirements of field surveys for statistical quality and for compatibility with EO data. Earth Observation (EO) are widely used in agricultural statistics production. However, the accuracy of EO-based land use classification is limited because of the limitations of using in situ census or survey data as training sets for EO applications. In this work, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments."
  },
  {
    "objectID": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "href": "th_design_frames.html#matching-in-situ-survey-data-to-remote-sensing-analysis-needs",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.2 Matching in situ survey data to remote sensing analysis needs",
    "text": "11.2 Matching in situ survey data to remote sensing analysis needs\nData collections done by NSOs can potentially become the main source of training data for EO applications for agricultural statistics. However, while national surveys often adopt GPS technology, satellite imagery remains largely unused by NSOs. To change this status quo, the Global Strategy to Improve Agricultural and Rural Statistics compiled various use cases in its Handbook on Remote Sensing for Agricultural Statistics, highlighting the missing links between EO-driven surveys and most common NSO surveys [1]. Experience shows that sampling design, response design, and quality control of NSO surveys must follow well-documented requirements to obtain statistically sound results when using EO data.\nEO-related quality assurance of the in situ datasets developed in the FAO EOStat and ESA Sent4Stat projects includes two main components: (a) evaluation of survey design and (b) in situ data assessment using EO data. Quality assessment measures the suitability of a statistical survey (i.e. sampling and response design) to leverage satellite imagery in support of agriculture statistics. Many NSOs create their in situ protocols with a focus on aggregation at higher administrative tiers, often overlooking their potential application in EO contexts. Table 1 presents eleven criteria for NSOs to enable the combined use of in situ data for traditional surveys and EO applications.\n\nAssessment framework to qualify the compatibility of an in situ survey design to leverage EO satellite data for agriculture statistics\n\n\n\n\n\n\nCriteria related to the sampling design\n\n\n\n\n\nObservation timing allows identification of crop type in the field (unlike some household surveys, the survey must take place when the crop is visible on the field)\n\n\n\n\n\nMinimum number of samples for marginal crops (including intercrop types) to provide balanced datasets in terms of crop type sample distribution\n\n\n\n\n\nLocal homogeneity of each sample unit to match the corresponding satellite observation footprint\n\n\n\nCriteria related to the response design\n\n\n\n\n\nGeoreferenced ground observation at field or point level to link with satellite geospatial dataset (household geographic coordinates being insufficient)\n\n\n\n\n\nSample unit size at least matching the considered satellite observation footprint (not only the spatial resolution)\n\n\n\n\n\nContextual observation to document sample quality and qualify its representativity\n\n\n\n\n\nRich labelling of each sample beyond crop type to indicate specific growing conditions (e.g., weeds abundance, limited crop cover, water lodging, tree density)\n\n\n\n\n\nHigh precision of crop type nomenclature, including information about infrastructure and agriculture practices (e.g., irrigation, greenhouses, crop under canopy, agroforestry, species dominance for intercropping)"
  },
  {
    "objectID": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "href": "th_design_frames.html#eo-based-quality-control-of-in-situ-surveys",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.3 EO-based quality control of in situ surveys",
    "text": "11.3 EO-based quality control of in situ surveys\nAn integral aspect of the statistical survey process is its quality control procedure. Nationwide surveys require heavy logistics involving hundreds of enumerators dispersed across the country. In many countries, digital encoding devices have integrated GPS receivers and communication support, making near real-time quality checks feasible. In Senegal, the Direction de l’Analyse, de la Prévision et des Statistiques Agricoles (DAPSA) employs near real-time quality control to oversee national data collection, enabling the field campaign to incorporate repetition requests and corrections as needed.\nAchieving the quality required for EO utilization imposes demands on the training of enumerators and presents more challenges for controllers. Besides being useful for aggregated surveys, in situ data must pass EO-based quality control checks. Such protocol is even more critical when combining datasets from distinct surveys requiring strict harmonisation. Experiences of FAO-EOSTAT and Sen2Stat with list frame and area frame statistical survey datasets led us to establish an EO-based quality control protocol. This protocol relies on existing maps and satellite time series processing as independent data quality control sources. Table 2 outlines the criteria applied for EO-based data quality control."
  },
  {
    "objectID": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "href": "th_design_frames.html#technical-suitability-of-in-situ-data",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.4 Technical suitability of in situ data",
    "text": "11.4 Technical suitability of in situ data\nThe first three criteria of the EO-based quality control concern the technical suitability of in situ data for EO applications. Typically, surveyors record GPS coordinates for household localization, crop observation placement, or field area measurement. Requirements for geospatial analysis include ensuring the topological soundness of spatial features, which involves verifying polygon closure, identifying duplicate points, and resolving polygon overlaps (Figure 11.1).\n\n\n\n\nFigure 11.1: Quality control of geospatial features and their coordinates. Examples acquired during the FAO EOStat project in Senegal from left to right: polygon recorded as points sequence instead of a closed polygon, polygon overlap detected and solved, and benchmarking of various protocols and devices (tablet with integrated GPS versus Garmin receiver) to record field boundaries."
  },
  {
    "objectID": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "href": "th_design_frames.html#measuring-spatial-precision-of-field-plot-boundaries",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.5 Measuring spatial precision of field plot boundaries",
    "text": "11.5 Measuring spatial precision of field plot boundaries\nThe last three criteria of the EO-based quality control rely on satellite imagery analysis. The spatial precision of field plot boundaries requires visual image interpretation of high spatial resolution imagery that aligns with the survey timeframe. This type of control usually involves overlaying the plotted polygons onto frequent data, such as monthly cloud-free surface reflectance base maps. Figure 11.2 shows a good-quality sample that aligns well with a cultivated field, whereas the second sample area covers multiple fields and some trees. The ideal situation is to use high-resolution images to visually check samples and plot boundaries and then classify the areas with lower spatial resolutions. A possible situation is to use 4.8-meter Planet monthly reflectance maps for sample quality control and 10-meter Sentinel-2 images for classification.\nFrom an EO perspective, assessing sample quality requires time series from satellites such as Sentinel-2 for the growing season. Open-source platforms such as Sen4Stat and Sen2Agri toolbox [2] allow processing of all Sentinel-2 satellite images acquired along the season. One quantitative indicator of sample purity is the NDVI standard deviation (Figure 11.2, right plot) computed from the values of cloud-free satellite observations.\n\n\n\n\nFigure 11.2: Cloud-free Planet monthly base map images (left) and very high-resolution imagery (middle) overlaid with point observation expected to be representative of a circle area (radius of 20 m), as reported by the 2020 wheat rust survey in Ethiopia (Blasch et al., 2022). Plots highlight expected and unexpected NDVI profiles and the associated standard deviation for a homogeneous wheat field derived from the Sentinel-2 time series during the surveyed season.."
  },
  {
    "objectID": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "href": "th_design_frames.html#using-ndvi-temporal-profiles-to-assess-crop-phenology",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.6 Using NDVI temporal profiles to assess crop phenology",
    "text": "11.6 Using NDVI temporal profiles to assess crop phenology\nThe last EO-based requirement is the most difficult to address. We assume that crops of the same type, grown in the same agro-climatic zone, have similar planting and growing cycles. Analysing NDVI profiles across all crop samples strengthens crop label confidence. Atypical growth patterns (e.g., varied planting/cycle length or lack of growth) indicate potential mislabeling or mislocated samples. These samples need more scrutiny before they can be deemed usable.\nConsider Ethiopia’s diverse crop cycles in Figure 11.3. Temporal profiles for barley, fava beans, and teff show outliers indicating marginal sample quality. The different NDVI profiles for maize samples reveal that a significant portion underwent a double cropping cycle within the observation period. Since the first crop cycle delayed planting, the sample population needs to be divided appropriately using clustering methods. The complexity of the wheat cropping cycle is heightened by sowing dates and varietal differences affecting cycle length. Thus, EO quality control aims to reject unsuitable samples for model calibration and output validation. Subsequent EO-derived results, like crop type maps, area estimates, and yield forecasts, critically depended on this quality control process.\n\n\n\n\nFigure 11.3: NDVI temporal profiles interpolated from cloud-free Sentinel-2 multispectral images acquired along the observation period. Each colour curve corresponds to a sample for a given crop, while the black curve is the average NDVI value of all samples for this crop. Teff is blue, wheat is red, barley is light green, peas are pink, fava beans are orange, and maise is dark green. The CIMMYT provided these samples in the framework of the ESA Sen4Rust partnership."
  },
  {
    "objectID": "th_design_frames.html#summary",
    "href": "th_design_frames.html#summary",
    "title": "\n11  Remote Sensing in the Design of Sampling Frames\n",
    "section": "\n11.7 Summary",
    "text": "11.7 Summary\nIn this chapter, we provide recommendations for National Statistical Offices (NSO) to design in situ data collection campaigns that benefit both conventional statistics and EO-based assessments. Following these guidelines will increase the accuracy of EO-based land use classification.\nReferences{-}\n\n\n\n\n[1] \nJ. Delincé et al., Handbook on remote sensing for agricultural statistics. FAO, 2017.\n\n\n[2] \nP. Defourny et al., “Near real-time agriculture monitoring at national scale at parcel resolution: Performance assessment of the Sen2-Agri automated system in various cropping systems around the world,” Remote Sensing of Environment, vol. 221, pp. 551–568, 2019, doi: 10.1016/j.rse.2018.11.007."
  },
  {
    "objectID": "th_parcel_extraction.html#outline",
    "href": "th_parcel_extraction.html#outline",
    "title": "12  Automatic Extraction of Parcels",
    "section": "12.1 Outline",
    "text": "12.1 Outline\nDescribes AI methods for automatic parcel detection. The link to the forthcoming Census of Agriculture 2026 in Brazil should be treated, explaining the functionality of the boundary data set to the efficiency gains expected during the operations of the census."
  },
  {
    "objectID": "crop_type_mapping.html#outline",
    "href": "crop_type_mapping.html#outline",
    "title": "Use Cases in Crop Type Mapping",
    "section": "Outline",
    "text": "Outline\nDescribe the use cases."
  },
  {
    "objectID": "ct_poland.html#outline",
    "href": "ct_poland.html#outline",
    "title": "13  Crop monitoring with SAR images in Poland",
    "section": "13.1 Outline",
    "text": "13.1 Outline\nCountry-wide crop classification using Sentinel-1 radar imagery. Reference: https://www.sciencedirect.com/science/article/pii/S0303243422000095"
  },
  {
    "objectID": "ct_mexico.html#outline",
    "href": "ct_mexico.html#outline",
    "title": "14  Crop classification in Mexico",
    "section": "14.1 Outline",
    "text": "14.1 Outline\nIn this chapter we will share a crop classification exercise designed to teach, step by step, a simple classification process for didactic purposes, that could be replicated by any user. For this exercise we are using harmonized Landsat and Sentinel 2 images and Sentinel 1 radar images, as well as Machine learning and Deep learning algorithms, such as TPOT libraries and Auto Sklearn. Process algorithms are programmed in Python and Conda in a Sandbox environment."
  },
  {
    "objectID": "ct_senegal.html#overview",
    "href": "ct_senegal.html#overview",
    "title": "15  Multi-seasonal crop mapping in Senegal",
    "section": "15.1 Overview",
    "text": "15.1 Overview"
  },
  {
    "objectID": "ct_zimbabwe.html#section",
    "href": "ct_zimbabwe.html#section",
    "title": "16  Crop classification in Zimbabwe",
    "section": "16.1 ",
    "text": "16.1"
  },
  {
    "objectID": "ct_chile.html#overview",
    "href": "ct_chile.html#overview",
    "title": "17  Crop classification and land use mapping in Chile",
    "section": "17.1 Overview",
    "text": "17.1 Overview\nThis section outlines a pilot methodology for land use and crop classification in Chile, using the Maule Region as a case study. It combines Sentinel-2 time-series imagery, digital elevation models (DEM) and machine learning to produce scalable LULC maps. Ground truth data from agricultural censuses and other sources are used to enhance classification accuracy. The approach aims to support the integration of remote sensing into official agricultural statistics.\nThe study highlights key challenges, including data gaps, cloud cover, and crop heterogeneity. However, it also shows how time-series analysis helps capture crop phenology for better classification. This pilot sets the foundation for a standardized national mapping framework, enabling more timely and spatially detailed agricultural data for decision-making."
  },
  {
    "objectID": "ct_digital_earth_africa.html#outline",
    "href": "ct_digital_earth_africa.html#outline",
    "title": "18  Crop classification using Digital Earth Africa",
    "section": "18.1 Outline",
    "text": "18.1 Outline\nDigital Earth Africa offers continental-scale satellite-derived data products and provides guidance on their application in analytical workflows. This chapter demonstrates the use of high quality, cloud-free, geomedian composite images with median absolute deviations (GeoMADs) for crop type mapping. Digital Earth Africa’s GeoMADs are applied to a classification framework in the satellite image time series (SITS) package in the R language.\nThe chapter demonstrates how high-quality, ‘off-the-shelf’ satellite image composites can overcome common challenges in classification workflows, such as the ‘curse of dimensionality’ and computational demand. It also introduces readers and users to Digital Earth Africa products and services, and modes of access, especially through the SITS R package."
  },
  {
    "objectID": "cy_finland.html#section",
    "href": "cy_finland.html#section",
    "title": "19  Early-season crop yield mapping in Finland",
    "section": "19.1 ",
    "text": "19.1 \n```"
  },
  {
    "objectID": "cy_indonesia.html#outline",
    "href": "cy_indonesia.html#outline",
    "title": "20  Rice Paddy Phenology in Indonesia",
    "section": "20.1 Outline",
    "text": "20.1 Outline\nCombination of multi-year area sampling frame with Sentinel-1 imagery to predict rice phenology and estimate harvest areas.\nRice data plays a vital role in shaping national food security policies, especially in a country like Indonesia, where rice is both a staple and a strategic commodity. At present, the paddy harvested area is estimated through a monthly ground survey known as the Area Sampling Frames (ASF), conducted by Statistics Indonesia. While this method has long served as the official approach, the high cost of data collection and recurring challenges in field implementation underscore the urgent need for more efficient and scalable alternatives. Satellite Imagery Time Series (SITS) data, particularly from the historical archives of Sentinel-1, emerges as a promising solution. By leveraging the power of the XGBoost algorithm — a state-of-the-art machine learning model — it becomes possible to detect phenological stages of paddy growth remotely and with remarkable precision.\nBuilding on this, the study in 2024 introduces an alternative approach implemented in selected 10 provinces across Indonesia based on the harvest and productivity data. The study outlines a carefully designed workflow that enhances model accuracy through a combination of clustering techniques, missing data imputation, noise reduction, and region-specific model calibration. The results show that most cluster regions achieve high accuracy in classifying paddy phenological stages, while the estimated harvested area patterns align closely with the official statistics. These outcomes not only confirm the reliability of the proposed method but also highlight its strong potential to complement or even streamline the existing survey-based system in the future."
  },
  {
    "objectID": "cy_poland.html#overview",
    "href": "cy_poland.html#overview",
    "title": "21  Yield Forecasting in Poland",
    "section": "21.1 Overview",
    "text": "21.1 Overview\nIntegration of Sentinel-3 and MODIS Vegetation Indices with ERA-5 Agro-Meteorological Indicators for Operational Crop Yield Forecasting."
  },
  {
    "objectID": "cy_colombia.html#section",
    "href": "cy_colombia.html#section",
    "title": "22  Rice Phenology in Colombia",
    "section": "22.1 ",
    "text": "22.1"
  },
  {
    "objectID": "cy_china.html#section",
    "href": "cy_china.html#section",
    "title": "23  Crop type classification and crop yield estimation in China",
    "section": "23.1 ",
    "text": "23.1"
  },
  {
    "objectID": "ad_geoglam.html#overview",
    "href": "ad_geoglam.html#overview",
    "title": "24  Extraction of crop statistics from crop type and crop yield maps",
    "section": "24.1 Overview",
    "text": "24.1 Overview\nDescribe concrete experiences carried out by FAO-EOSTAT, Sen4Stat, and NASA Harvest. Ukraine - Crop area and Yield - Production Nasa Harvest Stratifier, sampling, regression estimator"
  },
  {
    "objectID": "ad_world_cereal.html#section",
    "href": "ad_world_cereal.html#section",
    "title": "25  WorldCereal - A Global Effort for Crop Mapping",
    "section": "25.1 ",
    "text": "25.1"
  },
  {
    "objectID": "ad_uav_applications.html#section",
    "href": "ad_uav_applications.html#section",
    "title": "26  UAV use in Agricultural Statistics",
    "section": "26.1 ",
    "text": "26.1"
  },
  {
    "objectID": "ad_disaster_response.html#section",
    "href": "ad_disaster_response.html#section",
    "title": "27  Remote Sensing for Agricultural Disaster Response",
    "section": "27.1 ",
    "text": "27.1"
  },
  {
    "objectID": "ad_governance.html#introduction",
    "href": "ad_governance.html#introduction",
    "title": "28  Data Governance for Agricultural Statistics",
    "section": "28.1 Introduction",
    "text": "28.1 Introduction"
  }
]